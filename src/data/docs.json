[{
    "id": "oai:dspace.mit.edu:1721.1/90651",
    "title": "Effects of Earth encounters on the physical properties of near-earth objects",
    "abstract": "The effects of Earth encounters on the physical properties of near-Earth objects (NEOs) have been shown to be significant factors in their evolution. Previous studies have examined the effects of these encounters on reflectance spectra based on observational measurements, and effects such as spin state and shape changes have been studied for specific asteroids and through simulation. In this project, an automated light-curve fitting routine was developed to support data reduction in an ongoing NEO survey. Additionally, data from previous NEO surveys were used to support simulation results by showing differences between encounter and non-encounter populations' rotational frequency distributions. These results demonstrate that Earth encounters have an effect on asteroid rotation by increasing the overall frequency as well as causing a wider distribution of frequencies when compared to non-encounter populations of NEOs. These data were, however, unable to show any effect on asteroid shape brought on by planetary encounters. A frequency comparison between NEOs that likely had Earth encounters to main-belt-equivalent asteroids did not show the same encounter effect, though the 'equivalent' asteroid populations were likely affected by a size/spin-rate bias.",
    "advisors": ["Richard P. Binzel", "Nicholas A. Moskovitz"],
    "text": "Effects of Earth encounters on the physical properties of near-earth objects The effects of Earth encounters on the physical properties of near-Earth objects (NEOs) have been shown to be significant factors in their evolution. Previous studies have examined the effects of these encounters on reflectance spectra based on observational measurements, and effects such as spin state and shape changes have been studied for specific asteroids and through simulation. In this project, an automated light-curve fitting routine was developed to support data reduction in an ongoing NEO survey. Additionally, data from previous NEO surveys were used to support simulation results by showing differences between encounter and non-encounter populations' rotational frequency distributions. These results demonstrate that Earth encounters have an effect on asteroid rotation by increasing the overall frequency as well as causing a wider distribution of frequencies when compared to non-encounter populations of NEOs. These data were, however, unable to show any effect on asteroid shape brought on by planetary encounters. A frequency comparison between NEOs that likely had Earth encounters to main-belt-equivalent asteroids did not show the same encounter effect, though the 'equivalent' asteroid populations were likely affected by a size/spin-rate bias."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46257",
    "title": "Design and development of advanced load sensors for the International Space Station",
    "abstract": "In preparation for the construction of the International Space Station (ISS) a risk mitigation experiment was conducted to quantify the crew-induced disturbances to the microgravity environment on board a spacecraft during a long duration space flight. Achieving a microgravity environment for scientific experiments is one of the primary objectives of the ISS. While numerous measurements have been made to characterize the overall acceleratory environment on the Space Shuttle and on Mir, the contribution of astronaut motion to the disturbances was little understood. During the first phase of the ISS Program, the stay of U.S. astronauts on the Russian Orbital Complex Mir, the Enhanced Dynamic Load Sensors (EDLS) Spaceflight Experiment measured from May 1996 to May 1997 the forces and moments that astronaut exerted on the space station. Using four instrumented crew restraining and mobility devices, a handhold, two foot loops, and a touchpad, 133 hours of data was recorded during nominal crew activities and scientific experiments. The thesis gives a historical overview of the research that has been conducted to quantify the crew spacecraft interaction. A description of the EDLS experiment set-up and timeline as well as the custom-designed experiment hardware and software is provided. Due to an on-orbit failure of the original data acquisition system, a replacement computer was used to continue the experiment. The post-flight efforts to calibrate the replacement hardware, catalog the data files, and the tests to determine the condition of the sensors are presented. A cross-platform EDLS-specific software package was developed to aid in the analysis of the spaceflight data. The requirements, underlying signal processing equations, and the implementation in MATLAB are discussed. A preliminary design of advanced sensors for the ISS is developed in the thesis. While, retaining the proven strain-gage based method of sensing forces and moments, the restraining portion of the sensors was redesigned to aid astronauts better and can be easily exchanged for a different functionality. While having a volume of only 5800 cubic centimeters, the sensor electronics unit (SEU) incorporates most of the features of the original computer eight times its size. The SEU features an advanced embedded computer system and a Java-based operating system. Feedback on the loads applied can be provided in near real-time to the crew to aid the astronauts in maintaining a quiescent environment on the station during critical microgravity experiments.",
    "advisors": ["Dava J. Newman"],
    "text": "Design and development of advanced load sensors for the International Space Station In preparation for the construction of the International Space Station (ISS) a risk mitigation experiment was conducted to quantify the crew-induced disturbances to the microgravity environment on board a spacecraft during a long duration space flight. Achieving a microgravity environment for scientific experiments is one of the primary objectives of the ISS. While numerous measurements have been made to characterize the overall acceleratory environment on the Space Shuttle and on Mir, the contribution of astronaut motion to the disturbances was little understood. During the first phase of the ISS Program, the stay of U.S. astronauts on the Russian Orbital Complex Mir, the Enhanced Dynamic Load Sensors (EDLS) Spaceflight Experiment measured from May 1996 to May 1997 the forces and moments that astronaut exerted on the space station. Using four instrumented crew restraining and mobility devices, a handhold, two foot loops, and a touchpad, 133 hours of data was recorded during nominal crew activities and scientific experiments. The thesis gives a historical overview of the research that has been conducted to quantify the crew spacecraft interaction. A description of the EDLS experiment set-up and timeline as well as the custom-designed experiment hardware and software is provided. Due to an on-orbit failure of the original data acquisition system, a replacement computer was used to continue the experiment. The post-flight efforts to calibrate the replacement hardware, catalog the data files, and the tests to determine the condition of the sensors are presented. A cross-platform EDLS-specific software package was developed to aid in the analysis of the spaceflight data. The requirements, underlying signal processing equations, and the implementation in MATLAB are discussed. A preliminary design of advanced sensors for the ISS is developed in the thesis. While, retaining the proven strain-gage based method of sensing forces and moments, the restraining portion of the sensors was redesigned to aid astronauts better and can be easily exchanged for a different functionality. While having a volume of only 5800 cubic centimeters, the sensor electronics unit (SEU) incorporates most of the features of the original computer eight times its size. The SEU features an advanced embedded computer system and a Java-based operating system. Feedback on the loads applied can be provided in near real-time to the crew to aid the astronauts in maintaining a quiescent environment on the station during critical microgravity experiments."
}, {
    "id": "oai:dspace.mit.edu:1721.1/90726",
    "title": "Algorithms for autonomous urban navigation with formal specifications",
    "abstract": "This thesis addresses problems in planning and control of autonomous agents. The central theme of this work is that integration of \"low-level control synthesis\" and \"high-level decision making\" is essential to devise robust algorithms with provable guarantees on performance. We pursue two main directions here. The first part considers planning and control algorithms that satisfy temporal specifications expressed using formal languages. We focus on task specifications that become feasible only if some of the specifications are violated and compute a control law that minimizes the level of unsafety of the system while guaranteeing that it still satisfies the task specification. Examples in this domain are motivated from an autonomous car navigating an urban landscape while following road safety rules such as \"always travel in the left lane\" and \"do not change lanes frequently\" or an electric vehicle in a mobility-on-demand scenario. The second part of the thesis focuses on multi-agent control synthesis, where agents are modeled as dynamical systems and they interact with each other while sharing the same road infrastructure - all the while respecting the same road driving rules expressed as LTL specifications. We discuss algorithms that identify well-defined notions in the game theory literature such as Stackelberg equilibria and non-cooperative Nash equilibria under various information structures. This work builds upon ideas from three different fields, viz., sampling-based motion-planning algorithms to construct efficient concretizations of general, continuous time dynamical systems, model checking for formal specifications that helps guarantee the safety of a system under all scenarios, and game theory to model the interaction between different agents trying to perform possibly conflicting tasks.",
    "advisors": ["Emilio Frazzoli"],
    "text": "Algorithms for autonomous urban navigation with formal specifications This thesis addresses problems in planning and control of autonomous agents. The central theme of this work is that integration of \"low-level control synthesis\" and \"high-level decision making\" is essential to devise robust algorithms with provable guarantees on performance. We pursue two main directions here. The first part considers planning and control algorithms that satisfy temporal specifications expressed using formal languages. We focus on task specifications that become feasible only if some of the specifications are violated and compute a control law that minimizes the level of unsafety of the system while guaranteeing that it still satisfies the task specification. Examples in this domain are motivated from an autonomous car navigating an urban landscape while following road safety rules such as \"always travel in the left lane\" and \"do not change lanes frequently\" or an electric vehicle in a mobility-on-demand scenario. The second part of the thesis focuses on multi-agent control synthesis, where agents are modeled as dynamical systems and they interact with each other while sharing the same road infrastructure - all the while respecting the same road driving rules expressed as LTL specifications. We discuss algorithms that identify well-defined notions in the game theory literature such as Stackelberg equilibria and non-cooperative Nash equilibria under various information structures. This work builds upon ideas from three different fields, viz., sampling-based motion-planning algorithms to construct efficient concretizations of general, continuous time dynamical systems, model checking for formal specifications that helps guarantee the safety of a system under all scenarios, and game theory to model the interaction between different agents trying to perform possibly conflicting tasks."
}, {
    "id": "oai:dspace.mit.edu:1721.1/85697",
    "title": "Development of a body force model for centrifugal compressors",
    "abstract": "This project is focused on modeling the internal ow in centrifugal compressors for the purpose of assessing the onset of rotating stall and surge. The current methods to determine centrifugal compressor stability limits are based on empirical data and often, experiments. Unsteady full wheel simulations have become feasible due to the increase in computation power but the prediction of the stability limit still remains a challenge. The presented methodology is based on the idea of body forces and a blade passage model suitable for centrifugal compressors is derived. Previous work has shown that blade passage models are capable of capturing the response to inlet ow distortions and the onset of instability in axial compressors. In this thesis, a blade passage model is developed for centrifugal compressors with the goal of capturing the three-dimensional through-ow computed by steady RANS simulations. The model consists of three main elements, a normal force model, a viscous parallel force model, and a blade metal blockage model. The work demonstrates the model's capabilities on a radial impeller with prismatic blades where the total-to-static pressure rise coefficient and stage loading coefficient are in agreement with RANS calculations within 6.75% and 5%, respectively. While the model denition is shown to be consistent with other blade passage models for axial compressors, its application to a transonic axial compressor rotor and a high-speed centrifugal compressor stage revealed numerical convergence problems. It is thought that the model derivation and denition are sound and that these issues are due to implementation errors. The methodology and related modeling process are investigated step by step for three-dimensional blade geometries and, where applicable, verified with direct numerical calculation. The model limitations and potential implementation error are discussed at length so as to guide future work required to complete the demonstration of this blade passage model for axial and centrifugal compressors with three-dimensional blade shapes.",
    "advisors": ["Zoltán S. Spakovszky"],
    "text": "Development of a body force model for centrifugal compressors This project is focused on modeling the internal ow in centrifugal compressors for the purpose of assessing the onset of rotating stall and surge. The current methods to determine centrifugal compressor stability limits are based on empirical data and often, experiments. Unsteady full wheel simulations have become feasible due to the increase in computation power but the prediction of the stability limit still remains a challenge. The presented methodology is based on the idea of body forces and a blade passage model suitable for centrifugal compressors is derived. Previous work has shown that blade passage models are capable of capturing the response to inlet ow distortions and the onset of instability in axial compressors. In this thesis, a blade passage model is developed for centrifugal compressors with the goal of capturing the three-dimensional through-ow computed by steady RANS simulations. The model consists of three main elements, a normal force model, a viscous parallel force model, and a blade metal blockage model. The work demonstrates the model's capabilities on a radial impeller with prismatic blades where the total-to-static pressure rise coefficient and stage loading coefficient are in agreement with RANS calculations within 6.75% and 5%, respectively. While the model denition is shown to be consistent with other blade passage models for axial compressors, its application to a transonic axial compressor rotor and a high-speed centrifugal compressor stage revealed numerical convergence problems. It is thought that the model derivation and denition are sound and that these issues are due to implementation errors. The methodology and related modeling process are investigated step by step for three-dimensional blade geometries and, where applicable, verified with direct numerical calculation. The model limitations and potential implementation error are discussed at length so as to guide future work required to complete the demonstration of this blade passage model for axial and centrifugal compressors with three-dimensional blade shapes."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54657",
    "title": "Metrics for enterprise transformation",
    "abstract": "The objective of this thesis is to depict the role of metrics in the evolving journey of enterprise transformation. To this end, three propositions are explored: (i) metrics and measurement systems drive transformation, (ii) employee engagement is a proxy to gauge transformation progress; and (iii) metric considerations enable enterprise transformation when systematically executed as part of a transformation roadmap. To explore this problem, the aerospace measurement community was consulted to help grasp a better understanding of the context in which transformation is currently defined and measured. Once the problem space was defined, the environment of doing research with the enterprise as the unit of analysis was described with the intent of exploring the role of metrics and transformation. In particular, the performance measurement literature helped identify tools and methods used to select metrics to enable decision making at the enterprise level. After this review, two case studies were performed, considering: (1) the implementation of a bottom-up measurement system to drive transformation and (2) the effect of a top-down corporate measurement system on the enterprise. The first case study revealed insights regarding the benefits and challenges of implementing measurement systems and highlighted the use of employee engagement as a proxy to measure enterprise transformation. In the second case study, contemporary measurement issues were discussed and mapped to an Eight Views of the Enterprise analysis to identify critical enterprise interactions.",
    "advisors": ["Ricardo Valerdi"],
    "text": "Metrics for enterprise transformation The objective of this thesis is to depict the role of metrics in the evolving journey of enterprise transformation. To this end, three propositions are explored: (i) metrics and measurement systems drive transformation, (ii) employee engagement is a proxy to gauge transformation progress; and (iii) metric considerations enable enterprise transformation when systematically executed as part of a transformation roadmap. To explore this problem, the aerospace measurement community was consulted to help grasp a better understanding of the context in which transformation is currently defined and measured. Once the problem space was defined, the environment of doing research with the enterprise as the unit of analysis was described with the intent of exploring the role of metrics and transformation. In particular, the performance measurement literature helped identify tools and methods used to select metrics to enable decision making at the enterprise level. After this review, two case studies were performed, considering: (1) the implementation of a bottom-up measurement system to drive transformation and (2) the effect of a top-down corporate measurement system on the enterprise. The first case study revealed insights regarding the benefits and challenges of implementing measurement systems and highlighted the use of employee engagement as a proxy to measure enterprise transformation. In the second case study, contemporary measurement issues were discussed and mapped to an Eight Views of the Enterprise analysis to identify critical enterprise interactions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/17816",
    "title": "Integration of system-level optimization with concurrent engineering using parametric subsystem modeling",
    "abstract": "The introduction of concurrent design practices to the aerospace industry has greatly increased the efficiency and productivity of engineers during design sessions. Teams that are well-versed in such practices such as JPL's Team X are able to thoroughly examine a trade space and develop a family of reliable point designs for a given mission in a matter of weeks compared to the months or years sometimes needed for traditional design. Simultaneously, advances in computing power have given rise to a host of potent numerical optimization methods capable of solving complex multidisciplinary optimization problems containing hundreds of variables, constraints, and governing equations. Unfortunately, such methods are tedious to set up and require significant amounts of time and processor power to execute, thus making them unsuitable for rapid concurrent engineering use. In some ways concurrent engineering and automated system-level optimization are often viewed as being mutually incompatible. It is therefore desirable to devise a system to allow concurrent engineering teams to take advantage of these powerful techniques without hindering the teams' performance. This paper proposes such an integration by using parametric approximations of the subsystem models. These approximations are then linked to a system-level optimizer that is capable of reaching a solution more quickly than normally possible due to the reduced complexity of the approximations. The integration structure is described in detail and applied to a standard problem in aerospace engineering. Further, a comparison is made between this application and traditional concurrent engineering through an experimental trial with two groups each using a different method to",
    "advisors": ["Olivier L. de Weck"],
    "text": "Integration of system-level optimization with concurrent engineering using parametric subsystem modeling The introduction of concurrent design practices to the aerospace industry has greatly increased the efficiency and productivity of engineers during design sessions. Teams that are well-versed in such practices such as JPL's Team X are able to thoroughly examine a trade space and develop a family of reliable point designs for a given mission in a matter of weeks compared to the months or years sometimes needed for traditional design. Simultaneously, advances in computing power have given rise to a host of potent numerical optimization methods capable of solving complex multidisciplinary optimization problems containing hundreds of variables, constraints, and governing equations. Unfortunately, such methods are tedious to set up and require significant amounts of time and processor power to execute, thus making them unsuitable for rapid concurrent engineering use. In some ways concurrent engineering and automated system-level optimization are often viewed as being mutually incompatible. It is therefore desirable to devise a system to allow concurrent engineering teams to take advantage of these powerful techniques without hindering the teams' performance. This paper proposes such an integration by using parametric approximations of the subsystem models. These approximations are then linked to a system-level optimizer that is capable of reaching a solution more quickly than normally possible due to the reduced complexity of the approximations. The integration structure is described in detail and applied to a standard problem in aerospace engineering. Further, a comparison is made between this application and traditional concurrent engineering through an experimental trial with two groups each using a different method to"
}, {
    "id": "oai:dspace.mit.edu:1721.1/35290",
    "title": "Real-time path-planning using mixed-integer linear programming and global cost-to-go maps",
    "abstract": "With the advance in the fields of computer science, control and optimization, it is now possible to build aerial vehicles which do not need pilots. An important capability for such autonomous vehicles is to be able to generate their own path to navigate in a constrained environment and accomplish mission objectives, such as reaching waypoints in minimal time. To account for dynamic changes in the environment, perturbations, modeling errors and modifications in the mission scenario, the trajectory needs to be continuously re-optimized online based on the latest available updates. However, to allow for high update rates, the trajectory optimization problem needs to be simple enough to be solved quickly. Optimizing for a continuous trajectory of a dynamically-constrained vehicle in the presence of obstacles is an infinite-dimension nonlinear optimal control problem. Such a problem is intractable in real-time and simplifications need to be made. In this thesis, the author presents the mechanisms used to design a path-planner with real-time and long-range capabilities. The approach relies on converting the optimal control problem into a parameter optimization one whose horizon can be reduced by using a global cost-to-go function to provide an approximate cost for the tail of the trajectory.",
    "advisors": ["Eric Feron"],
    "text": "Real-time path-planning using mixed-integer linear programming and global cost-to-go maps With the advance in the fields of computer science, control and optimization, it is now possible to build aerial vehicles which do not need pilots. An important capability for such autonomous vehicles is to be able to generate their own path to navigate in a constrained environment and accomplish mission objectives, such as reaching waypoints in minimal time. To account for dynamic changes in the environment, perturbations, modeling errors and modifications in the mission scenario, the trajectory needs to be continuously re-optimized online based on the latest available updates. However, to allow for high update rates, the trajectory optimization problem needs to be simple enough to be solved quickly. Optimizing for a continuous trajectory of a dynamically-constrained vehicle in the presence of obstacles is an infinite-dimension nonlinear optimal control problem. Such a problem is intractable in real-time and simplifications need to be made. In this thesis, the author presents the mechanisms used to design a path-planner with real-time and long-range capabilities. The approach relies on converting the optimal control problem into a parameter optimization one whose horizon can be reduced by using a global cost-to-go function to provide an approximate cost for the tail of the trajectory."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32443",
    "title": "The second skin approach : skin strain field analysis and mechanical counter pressure prototyping for advanced spacesuit design",
    "abstract": "The primary aim of this thesis is to advance the theory of advanced locomotion mechanical counter pressure (MCP) spacesuits by studying the changes in the human body shape during joint motion. Two experiments take advantage of three-dimensional laser scan technology to measure the shape changes of the human body. The first experiment is an analysis of the surface area and volume of the thigh, knee, calf, and entire leg during knee flexion. The second experiment is an analysis of the full-field strain on the skin surface of the leg during knee flexion. A repeatable and quantitative technique for mapping the leg skin strain field is developed. The results of the algorithm indicate the magnitude of strain over the entire surface of the leg, as well as the direction of minimum leg skin stretching during knee flexion. For 88% of the leg surface, knee flexion causes skin strain between -0.3 and 0.3 (less than 30% contraction or extension). However, just below the patella, longitudinal strain is as high as 0.7, and at the knee hollow, it is as low as -0.6. Circumferential strain values are as high as 1.0 and 0.5 just below the patella and over the calf muscle, respectively, and along the anterior surface of the lower leg, they are as low as -0.7. The leg area, volume, and skin strain results lead to quantitative design requirements for highly mobile second skin spacesuits, and they inspire two prototype MCP leg sleeves: a hybrid urethane-foam bladder garment and a skintight nylon fiber lines of non-extension garment. These two prototypes are constructed and tested for mobility and skin surface pressure. Pressurization of the hybrid foam prototype inhibits leg mobility.",
    "advisors": ["Dava J. Newman"],
    "text": "The second skin approach : skin strain field analysis and mechanical counter pressure prototyping for advanced spacesuit design The primary aim of this thesis is to advance the theory of advanced locomotion mechanical counter pressure (MCP) spacesuits by studying the changes in the human body shape during joint motion. Two experiments take advantage of three-dimensional laser scan technology to measure the shape changes of the human body. The first experiment is an analysis of the surface area and volume of the thigh, knee, calf, and entire leg during knee flexion. The second experiment is an analysis of the full-field strain on the skin surface of the leg during knee flexion. A repeatable and quantitative technique for mapping the leg skin strain field is developed. The results of the algorithm indicate the magnitude of strain over the entire surface of the leg, as well as the direction of minimum leg skin stretching during knee flexion. For 88% of the leg surface, knee flexion causes skin strain between -0.3 and 0.3 (less than 30% contraction or extension). However, just below the patella, longitudinal strain is as high as 0.7, and at the knee hollow, it is as low as -0.6. Circumferential strain values are as high as 1.0 and 0.5 just below the patella and over the calf muscle, respectively, and along the anterior surface of the lower leg, they are as low as -0.7. The leg area, volume, and skin strain results lead to quantitative design requirements for highly mobile second skin spacesuits, and they inspire two prototype MCP leg sleeves: a hybrid urethane-foam bladder garment and a skintight nylon fiber lines of non-extension garment. These two prototypes are constructed and tested for mobility and skin surface pressure. Pressurization of the hybrid foam prototype inhibits leg mobility."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32461",
    "title": "Assessing the technical, economic and policy-centered feasibility of a proposed satellite communication system for the developing world",
    "abstract": "Satellite communication systems remain one of the most under utilized development mediums in less industrialized countries. This research proposes to establish a low cost satellite communications system tailored specifically for the developing world (+/- 30⁰ latitude). The technical, economic and policy related frontiers of the problem are integrated within a MATLAB based satellite communication constellation simulation which is used to assess the feasibility of the proposed satellite system. The analysis demonstrates that with technical advances that would allow higher capacity systems at lower costs and a renewed policy framework in line with the present state of the satellite system industry, it could be feasible to establish a low earth orbit satellite communications system for the developing world. The inputs to the satellite simulation are the proposed system's desired design variables and other relevant parameters. The outputs are system performance, capacity and cost. The Pareto optimal solution trade space is generated by the simulation model using a full-factorial run that probes the entire design space. The application of choice is short messaging services (SMS), chosen for its ability to provide proven connectivity at moderate costs. The capacity and cost of the most ideal Pareto architecture is contrasted against demand in the defined developing world region. The simulation also accounts for the necessary policy considerations and assesses the feasibility of the proposed system amidst the existing industry policy and regulatory framework. Additionally, data regarding the current economic standing of the region and how this forms an underlying basis for the digital divide is presented and assessed.",
    "advisors": ["Olivier L. de Weck"],
    "text": "Assessing the technical, economic and policy-centered feasibility of a proposed satellite communication system for the developing world Satellite communication systems remain one of the most under utilized development mediums in less industrialized countries. This research proposes to establish a low cost satellite communications system tailored specifically for the developing world (+/- 30⁰ latitude). The technical, economic and policy related frontiers of the problem are integrated within a MATLAB based satellite communication constellation simulation which is used to assess the feasibility of the proposed satellite system. The analysis demonstrates that with technical advances that would allow higher capacity systems at lower costs and a renewed policy framework in line with the present state of the satellite system industry, it could be feasible to establish a low earth orbit satellite communications system for the developing world. The inputs to the satellite simulation are the proposed system's desired design variables and other relevant parameters. The outputs are system performance, capacity and cost. The Pareto optimal solution trade space is generated by the simulation model using a full-factorial run that probes the entire design space. The application of choice is short messaging services (SMS), chosen for its ability to provide proven connectivity at moderate costs. The capacity and cost of the most ideal Pareto architecture is contrasted against demand in the defined developing world region. The simulation also accounts for the necessary policy considerations and assesses the feasibility of the proposed system amidst the existing industry policy and regulatory framework. Additionally, data regarding the current economic standing of the region and how this forms an underlying basis for the digital divide is presented and assessed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/43834",
    "title": "Lean Transformations in Supply Chain, the autocatalytic nature of lean principles, and tactics for implementing lean tools",
    "abstract": "Expanding Lean principles beyond the manufacturing floor, ultimately to entail a comprehensive Lean Enterprise, has gained increasing attention among corporations. This thesis entails a detailed case study of initiating a Lean Transformation in the Supply Chain department of a technology center for engineering, integration and final assembly of directional drilling equipment. This technology center is part of Schlumberger, a global corporation and industry leader in directional drilling and other oilfield technologies and services. Initiating the Lean Transformation in this Supply Chain department is detailed and used as the central theme throughout the thesis. The rapid, successive applications and results of conventional Lean principles are evaluated. Due to the near relative proximity of the several initiatives undertaken, in both time and within the organization, this case is used to evaluate the Autocatalytic Nature of Lean Principles within the Supply Chain department. Concurrently, the dynamics involved with the interactions of personnel within the department are evaluated. As a Lean Transformation is so heavily dependent upon the personnel participating in the change, tactics for initiating a Lean Transformation are treated in reference to the several initiatives of this case study. These three components, Lean principles, their autocatalytic interaction, and relevant human interactions, are all combined to comprehensively address the most influential aspects of affecting a Lean Transformation in a Supply Chain department.",
    "advisors": ["Deborah Nightingale", "Roy Welsch"],
    "text": "Lean Transformations in Supply Chain, the autocatalytic nature of lean principles, and tactics for implementing lean tools Expanding Lean principles beyond the manufacturing floor, ultimately to entail a comprehensive Lean Enterprise, has gained increasing attention among corporations. This thesis entails a detailed case study of initiating a Lean Transformation in the Supply Chain department of a technology center for engineering, integration and final assembly of directional drilling equipment. This technology center is part of Schlumberger, a global corporation and industry leader in directional drilling and other oilfield technologies and services. Initiating the Lean Transformation in this Supply Chain department is detailed and used as the central theme throughout the thesis. The rapid, successive applications and results of conventional Lean principles are evaluated. Due to the near relative proximity of the several initiatives undertaken, in both time and within the organization, this case is used to evaluate the Autocatalytic Nature of Lean Principles within the Supply Chain department. Concurrently, the dynamics involved with the interactions of personnel within the department are evaluated. As a Lean Transformation is so heavily dependent upon the personnel participating in the change, tactics for initiating a Lean Transformation are treated in reference to the several initiatives of this case study. These three components, Lean principles, their autocatalytic interaction, and relevant human interactions, are all combined to comprehensively address the most influential aspects of affecting a Lean Transformation in a Supply Chain department."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54605",
    "title": "Fast interceptor of a dynamic object",
    "abstract": "This thesis presents a path planning and control strategy that enables an unmanned non-holonomic vehicle to intercept a fast moving object. The path planning is performed under model uncertainty, with respect to the vehicle's maneuverability, as well as uncertainty in the estimation of the object's future trajectory and position. This problem involves the tracking of the dynamic object in a cluttered environment and the accurate estimation of its future position in the presence of noisy measurements. The ground vehicle (interceptor) is required to intercept the dynamic object at a predicted (catch) location in a finite amount of time. This time restriction presents quite a challenge given the inherent limitation in the vehicle's steering and maneuverability. The solution strategy is divided into three sub-problems: 1) prediction, 2) path planning and 3) control. The prediction of the parameters that describe the dynamic's object in space is accomplished via Kalman Filtering which, in conjunction with an impact predictor, provide the waypoints needed to construct a reference path that will place the interceptor on a collision course with the dynamic object (target.) A pure pursuit algorithm was used to steer the interceptor along a reference trajectory, which was designed to make the vehicle engage the dynamic object on a near tail-on aspect. In the endgame, the pure pursuit algorithm was modified to ensure arrival to the catch point while a position controller was added to ensure timely arrival to the predicted catch location. The problem statement was then augmented to include obstacle avoidance.",
    "advisors": ["Jonathan P. How"],
    "text": "Fast interceptor of a dynamic object This thesis presents a path planning and control strategy that enables an unmanned non-holonomic vehicle to intercept a fast moving object. The path planning is performed under model uncertainty, with respect to the vehicle's maneuverability, as well as uncertainty in the estimation of the object's future trajectory and position. This problem involves the tracking of the dynamic object in a cluttered environment and the accurate estimation of its future position in the presence of noisy measurements. The ground vehicle (interceptor) is required to intercept the dynamic object at a predicted (catch) location in a finite amount of time. This time restriction presents quite a challenge given the inherent limitation in the vehicle's steering and maneuverability. The solution strategy is divided into three sub-problems: 1) prediction, 2) path planning and 3) control. The prediction of the parameters that describe the dynamic's object in space is accomplished via Kalman Filtering which, in conjunction with an impact predictor, provide the waypoints needed to construct a reference path that will place the interceptor on a collision course with the dynamic object (target.) A pure pursuit algorithm was used to steer the interceptor along a reference trajectory, which was designed to make the vehicle engage the dynamic object on a near tail-on aspect. In the endgame, the pure pursuit algorithm was modified to ensure arrival to the catch point while a position controller was added to ensure timely arrival to the predicted catch location. The problem statement was then augmented to include obstacle avoidance."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8097",
    "title": "Metrics and methods of improving airline schedule reliability",
    "abstract": "Airline scheduling is a daunting task. Much time and resources are spent by airlines developing a schedule that meets expectations of profitability and competitiveness. Most of the time, however, the reliability aspect has a minor, if any, role in such a process. In reality disruption of the schedule occurs due to unforeseen events such as weather conditions, traffic congestion, and mechanical problems. The outcomes of these events are cancellations and delays. The impact that these disruptions have on airline operations is not only the increased cost for system maintenance and recovery, but also the loss of profitability and the perception of poor and unreliable service for the flying customer. In this thesis we present an analysis of the schedule design process, highlight the drawbacks of the current proceedings and outline of new and more flexible framework for schedule design. We define a reliability measure, the Option Value, and a way of comparing flights based on the reliability they are providing, via the Option Disruption Value. The idea of reliability is based on the concept of flight performance: a flight is more reliable if it is able to match or outperform the on-time performance of the flights that leaves its origin station and arrives at its final destination at or near its arrival and departure times. Based on these two measurements, we quantify the robustness and coverage of a sample schedule. Alternative passenger ratings are defined based on the concept of alternative itineraries (Coverage) and alternative independent itineraries (Robustness) that connect two locations. These are the Flight Options and the Flight Protection Options, respectively. Fifteen methods to modify flight schedule are proposed. One method, Reduce/increase Flight Slack Time (R/IFTS) was evaluated. Results indicate that R/IFTS was effective in increasing reliability in 70% of the flight considered, but that other methods need to be employed if reliability is to be increased further.",
    "advisors": ["John-Paul Clarke"],
    "text": "Metrics and methods of improving airline schedule reliability Airline scheduling is a daunting task. Much time and resources are spent by airlines developing a schedule that meets expectations of profitability and competitiveness. Most of the time, however, the reliability aspect has a minor, if any, role in such a process. In reality disruption of the schedule occurs due to unforeseen events such as weather conditions, traffic congestion, and mechanical problems. The outcomes of these events are cancellations and delays. The impact that these disruptions have on airline operations is not only the increased cost for system maintenance and recovery, but also the loss of profitability and the perception of poor and unreliable service for the flying customer. In this thesis we present an analysis of the schedule design process, highlight the drawbacks of the current proceedings and outline of new and more flexible framework for schedule design. We define a reliability measure, the Option Value, and a way of comparing flights based on the reliability they are providing, via the Option Disruption Value. The idea of reliability is based on the concept of flight performance: a flight is more reliable if it is able to match or outperform the on-time performance of the flights that leaves its origin station and arrives at its final destination at or near its arrival and departure times. Based on these two measurements, we quantify the robustness and coverage of a sample schedule. Alternative passenger ratings are defined based on the concept of alternative itineraries (Coverage) and alternative independent itineraries (Robustness) that connect two locations. These are the Flight Options and the Flight Protection Options, respectively. Fifteen methods to modify flight schedule are proposed. One method, Reduce/increase Flight Slack Time (R/IFTS) was evaluated. Results indicate that R/IFTS was effective in increasing reliability in 70% of the flight considered, but that other methods need to be employed if reliability is to be increased further."
}, {
    "id": "oai:dspace.mit.edu:1721.1/76167",
    "title": "Design and analysis of lunar lander control system architectures",
    "abstract": "Although a great deal of separate work exists on the development of spacecraft actuators and control algorithm design, less work exists which examines the connections between the selection of specific actuator types and placements, how this affects control algorithm design, and how these combined factors affect the overall vehicle performance of a lunar lander. This thesis attempts to address these issues by combining a functionality-oriented approach to actuator type/placement with a controls-oriented approach to algorithm design and performance analysis. Three example control system architectures are examined for a generic autonomous 350kg lunar lander during the terminal descent flight phase. Results indicate that stability and control can be achieved using a wide variety of actuator types/placements and algorithms given that a set of 'common sense' subsystem functionality and robustness metrics are met; however, algorithm development was often heavily influenced/restricted by actuator system capabilities. It is therefore recommended that future designers of lunar lander vehicles consider the impact of their control system architectures from both a functionality-oriented and a controls-oriented approach to gain a more complete understanding of the effects of their choices on overall performance.",
    "advisors": ["Jeffrey A. Hoffman", "Brett J. Streetman"],
    "text": "Design and analysis of lunar lander control system architectures Although a great deal of separate work exists on the development of spacecraft actuators and control algorithm design, less work exists which examines the connections between the selection of specific actuator types and placements, how this affects control algorithm design, and how these combined factors affect the overall vehicle performance of a lunar lander. This thesis attempts to address these issues by combining a functionality-oriented approach to actuator type/placement with a controls-oriented approach to algorithm design and performance analysis. Three example control system architectures are examined for a generic autonomous 350kg lunar lander during the terminal descent flight phase. Results indicate that stability and control can be achieved using a wide variety of actuator types/placements and algorithms given that a set of 'common sense' subsystem functionality and robustness metrics are met; however, algorithm development was often heavily influenced/restricted by actuator system capabilities. It is therefore recommended that future designers of lunar lander vehicles consider the impact of their control system architectures from both a functionality-oriented and a controls-oriented approach to gain a more complete understanding of the effects of their choices on overall performance."
}, {
    "id": "oai:dspace.mit.edu:1721.1/108937",
    "title": "Modeling performance and noise of advanced operational procedures for current and future aircraft",
    "abstract": "Increasing concerns regarding aircraft noise has encouraged the push to reduce noise via operational adjustments. The objective here is thus to expand analysis capabilities to enable modeling of the impact on aircraft noise due to advanced operational approach procedures, such as delayed deceleration approaches and thrust cutback scheduling on takeoff, for both current and future aircraft designs. Current industry standard noise models rely on flight test data interpolation and do not fully capture noise impacts from airframe configuration or advanced operational techniques. This is critical for noise assessment because airframe noise becomes a significant factor relative to the low thrust levels characteristic of advanced operational approaches. This method also limits the ability to assess new aircraft designs. Therefore, a new method combining aircraft sizing and performance tools with NASA's Aircraft NOise Prediction Program (ANOPP) has been developed to capture those noise impacts. ANOPP is used because of its capability of computing noise received at ground observers due to both engines and airframe of aircraft flying any flight procedure. Inputs into ANOPP are the aircraft geometry, the flight procedure, and the engine performance during the flight procedure. The Transport Aircraft System OPTimization (TASOPT) model is used to compute the engine performance inputs into ANOPP via first principles, physics-based methods. A separate tool was developed to compute the specifics of the flight procedure (max glide slope obtainable for a particular velocity and configuration, required thrust levels, etc.) based on drag polar supplied either by the Base of Aircraft Data (BADA 4) for current aircraft or by TASOPT for new aircraft. Benefits of this modeling framework include the flexibility in the aircraft and procedure analyzed and the ability to predict the noise of future aircraft configurations without relying on existing data. Both the noise impacts of a sample advanced operational flight procedure and in a future aircraft fleet have been assessed with this model. Next steps include further use of this model to evaluate the noise benefits or detriments of advanced operational approaches.",
    "advisors": ["John Hansman"],
    "text": "Modeling performance and noise of advanced operational procedures for current and future aircraft Increasing concerns regarding aircraft noise has encouraged the push to reduce noise via operational adjustments. The objective here is thus to expand analysis capabilities to enable modeling of the impact on aircraft noise due to advanced operational approach procedures, such as delayed deceleration approaches and thrust cutback scheduling on takeoff, for both current and future aircraft designs. Current industry standard noise models rely on flight test data interpolation and do not fully capture noise impacts from airframe configuration or advanced operational techniques. This is critical for noise assessment because airframe noise becomes a significant factor relative to the low thrust levels characteristic of advanced operational approaches. This method also limits the ability to assess new aircraft designs. Therefore, a new method combining aircraft sizing and performance tools with NASA's Aircraft NOise Prediction Program (ANOPP) has been developed to capture those noise impacts. ANOPP is used because of its capability of computing noise received at ground observers due to both engines and airframe of aircraft flying any flight procedure. Inputs into ANOPP are the aircraft geometry, the flight procedure, and the engine performance during the flight procedure. The Transport Aircraft System OPTimization (TASOPT) model is used to compute the engine performance inputs into ANOPP via first principles, physics-based methods. A separate tool was developed to compute the specifics of the flight procedure (max glide slope obtainable for a particular velocity and configuration, required thrust levels, etc.) based on drag polar supplied either by the Base of Aircraft Data (BADA 4) for current aircraft or by TASOPT for new aircraft. Benefits of this modeling framework include the flexibility in the aircraft and procedure analyzed and the ability to predict the noise of future aircraft configurations without relying on existing data. Both the noise impacts of a sample advanced operational flight procedure and in a future aircraft fleet have been assessed with this model. Next steps include further use of this model to evaluate the noise benefits or detriments of advanced operational approaches."
}, {
    "id": "oai:dspace.mit.edu:1721.1/17751",
    "title": "Using tactical flight level resource allocation to alleviate congested en-route airspace",
    "abstract": "A motivation exists to formulate and implement new tools and methodologies to address the problem of congestion in the National Airspace System (NAS). This thesis presents a novel methodology for allocating aircraft among En Route flight levels as a means to mitigate air traffic congestion and stakeholder operating costs. The core of the methodology is a decision-aiding tool comprised of a Mixed-Integer Linear Program (MILP) that is solved using a an A* Search-based Branch & Bound framework. Two metrics, measuring cumulative delay reduction and fuel burn savings, are used to benchmark the performance of the methodology. A combination of these two metrics is also explored as a means to minimize overall airline operating costs. A subsection of the Northeast Corridor is modeled and forms part of the analytic structure used to quantify the potential benefits of the proposed methodology. Simulations are generated from these models in order to gain an understanding of the benefits as they relate to varying NAS conditions. The following scenarios were modeled: 1) A baseline single jetway corridor, 2) Reduced Vertical Separation Minimum (RVSM), 3) Miles in Trail (MIT) restrictions on corridor traffic, and 4) the merging of Terminal Area air traffic with En route air traffic. Thus, this research also provides a preliminary, quantitative measure of the delay reduction, fuel burn savings and operating cost savings possible under each scenario, within a NAS corridor setting. Results indicate that 8.5 minutes of delay reduction per flight can be achieved when minimizing air traffic delay. Similarly, 16.47 kg/min of fuel burn savings per flight can be achieved when minimizing air traffic fuel burn. Instituting RVSM procedures result in an additional 45% of delay",
    "advisors": ["John-Paul Barrington Clarke"],
    "text": "Using tactical flight level resource allocation to alleviate congested en-route airspace A motivation exists to formulate and implement new tools and methodologies to address the problem of congestion in the National Airspace System (NAS). This thesis presents a novel methodology for allocating aircraft among En Route flight levels as a means to mitigate air traffic congestion and stakeholder operating costs. The core of the methodology is a decision-aiding tool comprised of a Mixed-Integer Linear Program (MILP) that is solved using a an A* Search-based Branch & Bound framework. Two metrics, measuring cumulative delay reduction and fuel burn savings, are used to benchmark the performance of the methodology. A combination of these two metrics is also explored as a means to minimize overall airline operating costs. A subsection of the Northeast Corridor is modeled and forms part of the analytic structure used to quantify the potential benefits of the proposed methodology. Simulations are generated from these models in order to gain an understanding of the benefits as they relate to varying NAS conditions. The following scenarios were modeled: 1) A baseline single jetway corridor, 2) Reduced Vertical Separation Minimum (RVSM), 3) Miles in Trail (MIT) restrictions on corridor traffic, and 4) the merging of Terminal Area air traffic with En route air traffic. Thus, this research also provides a preliminary, quantitative measure of the delay reduction, fuel burn savings and operating cost savings possible under each scenario, within a NAS corridor setting. Results indicate that 8.5 minutes of delay reduction per flight can be achieved when minimizing air traffic delay. Similarly, 16.47 kg/min of fuel burn savings per flight can be achieved when minimizing air traffic fuel burn. Instituting RVSM procedures result in an additional 45% of delay"
}, {
    "id": "oai:dspace.mit.edu:1721.1/66051",
    "title": "A holistic approach to finished goods inventory in a global supply chain : analysis and trade-offs",
    "abstract": "Since Michael Dell returned as CEO in 2007 the company has undergone several changes such as utilizing third party manufacturers, reentry into retail, and a new focus on solution based offerings. Although historically Dell has been a build to order business, it is now expanding into build to plan (such as retail) and build-to-stock (BTS) fulfillment channels. This study focuses on Dell's recent entry into the BTS space and the use of finished goods inventory analysis to understand policy tradeoffs. Finished goods inventory decisions often have implications across multiple groups in a corporation. Decisions such as how many locations in which to hold inventory, where to hold inventory, how to fulfill that inventory, and at what service level cannot be made independently as they often influence each other and can be customer and product dependent. Additionally, external factors such as fuel costs, taxes, and market rates can change frequently, which can alter optimal strategies. A means of quickly evaluating alternative strategies to understand tradeoffs is needed. This study creates a model of inventory associated costs from the point of manufacturing to delivery to the customer for the US computer notebook market and seeks to account for the impacts across multiple organizations. Key inventory levels are explored and inventory theory is utilized. From this study a flexible model has been created that estimates a cost per unit for a given inventory policy as well as a methodology that will be used globally. Key decision makers have also gained greater intuition on the tradeoffs associated with these integrated decisions and have a tool that helps quantify the impacts of changes such as improved forecast accuracy, increased ocean shipment, and higher service levels. In this example, fundamental inventory theory and basic modeling techniques have been utilized to provide a tool that can evaluate complicated tradeoffs and the financial implications of inventory policies. This stresses the importance of knowledge of inventory fundamentals such as risk pooling, type one and type two service levels, and risk management by managers setting policy.",
    "advisors": ["Stephen Graves", "David Simchi-Levi"],
    "text": "A holistic approach to finished goods inventory in a global supply chain : analysis and trade-offs Since Michael Dell returned as CEO in 2007 the company has undergone several changes such as utilizing third party manufacturers, reentry into retail, and a new focus on solution based offerings. Although historically Dell has been a build to order business, it is now expanding into build to plan (such as retail) and build-to-stock (BTS) fulfillment channels. This study focuses on Dell's recent entry into the BTS space and the use of finished goods inventory analysis to understand policy tradeoffs. Finished goods inventory decisions often have implications across multiple groups in a corporation. Decisions such as how many locations in which to hold inventory, where to hold inventory, how to fulfill that inventory, and at what service level cannot be made independently as they often influence each other and can be customer and product dependent. Additionally, external factors such as fuel costs, taxes, and market rates can change frequently, which can alter optimal strategies. A means of quickly evaluating alternative strategies to understand tradeoffs is needed. This study creates a model of inventory associated costs from the point of manufacturing to delivery to the customer for the US computer notebook market and seeks to account for the impacts across multiple organizations. Key inventory levels are explored and inventory theory is utilized. From this study a flexible model has been created that estimates a cost per unit for a given inventory policy as well as a methodology that will be used globally. Key decision makers have also gained greater intuition on the tradeoffs associated with these integrated decisions and have a tool that helps quantify the impacts of changes such as improved forecast accuracy, increased ocean shipment, and higher service levels. In this example, fundamental inventory theory and basic modeling techniques have been utilized to provide a tool that can evaluate complicated tradeoffs and the financial implications of inventory policies. This stresses the importance of knowledge of inventory fundamentals such as risk pooling, type one and type two service levels, and risk management by managers setting policy."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33211",
    "title": "Atmospheric signal delay affecting GPS measurements made by space vehicles during launch, orbit and reentry",
    "abstract": "In this thesis, I present neutral atmosphere, ionosphere and total delays experienced by GPS signals traveling to space vehicles during launch, orbit and reentry. I calculate these delays for receivers at 0 km to 1700 km altitude by ray-tracing through the Global Reference Atmosphere Model (1999) and the International Reference Ionosphere (2001). These delays are potentially much larger than those experienced by signals traveling to GPS receivers near the surface of the Earth, but are primarily experienced at negative elevation angles, and are therefore most relevant for space vehicles with limited visibility of GPS satellites and during launch and reentry. I compare these signal delays to the delays predicted by three onboard delay models: the Altshuler and NATO neutral atmosphere delay models, and the Klobuchar ionosphere delay model. I find that these models are inadequate when the space vehicle is in orbit. The NATO model will suffice during the final period of reentry, where it predicts the neutral atmosphere delay to within 1 m of the ray-traced value, but it will not suffice when a satellite is rising or setting. I propose a method to extend the NATO model for receivers at higher altitudes. The Klobuchar model will suffice for most satellites during reentry, but will potentially predict ionosphere delays with errors up to 30 m, and will not suffice when a satellite is rising or setting.",
    "advisors": ["Anthony J. Bogner", "Thomas A. Herring"],
    "text": "Atmospheric signal delay affecting GPS measurements made by space vehicles during launch, orbit and reentry In this thesis, I present neutral atmosphere, ionosphere and total delays experienced by GPS signals traveling to space vehicles during launch, orbit and reentry. I calculate these delays for receivers at 0 km to 1700 km altitude by ray-tracing through the Global Reference Atmosphere Model (1999) and the International Reference Ionosphere (2001). These delays are potentially much larger than those experienced by signals traveling to GPS receivers near the surface of the Earth, but are primarily experienced at negative elevation angles, and are therefore most relevant for space vehicles with limited visibility of GPS satellites and during launch and reentry. I compare these signal delays to the delays predicted by three onboard delay models: the Altshuler and NATO neutral atmosphere delay models, and the Klobuchar ionosphere delay model. I find that these models are inadequate when the space vehicle is in orbit. The NATO model will suffice during the final period of reentry, where it predicts the neutral atmosphere delay to within 1 m of the ray-traced value, but it will not suffice when a satellite is rising or setting. I propose a method to extend the NATO model for receivers at higher altitudes. The Klobuchar model will suffice for most satellites during reentry, but will potentially predict ionosphere delays with errors up to 30 m, and will not suffice when a satellite is rising or setting."
}, {
    "id": "oai:dspace.mit.edu:1721.1/90808",
    "title": "Environmental and economic assessment of alternative transportation fuels",
    "abstract": "Alternative fuels have the potential to mitigate transportation's impact on the environment and enhance energy security. In this work, we investigate two alternative fuels: liquefied natural gas (LNG) as an aviation fuel, and middle distillate fuel derived from woody biomass for use in aviation or road transport. The use of LNG as a supplemental aircraft fuel is considered in the context of the Lockheed Martin C- 1 30H and C-130J transport aircraft. We estimate the cost of retrofitting these aircraft to use LNG and the savings from reduced fuel expenses. We evaluate the societal impacts of LNG within a cost-benefit framework, taking into account resource consumption, human health impacts related to air quality, and climate damage. We find that aircraft operators can save up to 14% on fuel expenses (retrofit costs included) by employing LNG retrofits, with a 95% confidence interval of 2-23%. Society can also benefit by 12% (3-20%) from LNG use as a result of improved surface air quality, lower resource consumption, and climate neutrality relative to conventional fuel. These results are highly dependent on fuel prices, the quantity and cost of the LNG retrofits, and the frequency and length of missions. Woody biomass harvested from old-growth forests produces a large carbon debt when used as a feedstock for transportation fuel. Managed forests are an attractive alternative for inexpensive biomass production with the potential to reduce this carbon debt. We study the effect of forest management practices on the carbon debt payback time resulting from harvesting woody biomass from managed forests for middle distillate fuel production. We also calculate the breakeven time in terms of radiative forcing, temperature change, and economic damages. We find that biofuels produced over a period of 30 years have higher CO 2 emissions than fossil fuels for 59 years, higher radiative forcing for 42 years, higher temperature change for 48 years, and higher cumulative discounted (1-2%) economic damages for more than 100 years. These damages never break even at discount rates above 2%. Payback times can be reduced by increasing the age at which biomass is harvested. When biofuel production is sustained indefinitely, greater climate benefits are achieved over the next 100 years by instead producing long-lived wood products like lumber.",
    "advisors": ["Steven R.H. Barrett"],
    "text": "Environmental and economic assessment of alternative transportation fuels Alternative fuels have the potential to mitigate transportation's impact on the environment and enhance energy security. In this work, we investigate two alternative fuels: liquefied natural gas (LNG) as an aviation fuel, and middle distillate fuel derived from woody biomass for use in aviation or road transport. The use of LNG as a supplemental aircraft fuel is considered in the context of the Lockheed Martin C- 1 30H and C-130J transport aircraft. We estimate the cost of retrofitting these aircraft to use LNG and the savings from reduced fuel expenses. We evaluate the societal impacts of LNG within a cost-benefit framework, taking into account resource consumption, human health impacts related to air quality, and climate damage. We find that aircraft operators can save up to 14% on fuel expenses (retrofit costs included) by employing LNG retrofits, with a 95% confidence interval of 2-23%. Society can also benefit by 12% (3-20%) from LNG use as a result of improved surface air quality, lower resource consumption, and climate neutrality relative to conventional fuel. These results are highly dependent on fuel prices, the quantity and cost of the LNG retrofits, and the frequency and length of missions. Woody biomass harvested from old-growth forests produces a large carbon debt when used as a feedstock for transportation fuel. Managed forests are an attractive alternative for inexpensive biomass production with the potential to reduce this carbon debt. We study the effect of forest management practices on the carbon debt payback time resulting from harvesting woody biomass from managed forests for middle distillate fuel production. We also calculate the breakeven time in terms of radiative forcing, temperature change, and economic damages. We find that biofuels produced over a period of 30 years have higher CO 2 emissions than fossil fuels for 59 years, higher radiative forcing for 42 years, higher temperature change for 48 years, and higher cumulative discounted (1-2%) economic damages for more than 100 years. These damages never break even at discount rates above 2%. Payback times can be reduced by increasing the age at which biomass is harvested. When biofuel production is sustained indefinitely, greater climate benefits are achieved over the next 100 years by instead producing long-lived wood products like lumber."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32440",
    "title": "Integrated navigation architecture analysis for Moon and Mars exploration",
    "abstract": "The new solar system exploration objectives announced in January 2004 have the goal of sending humans back to the Moon by the year 2020 in preparation for human exploration of Mars. Advanced, but cost effective, surface navigation and communication capabilities are required to support these new exploration objectives. In response to this need, a set of three Navigation/Communication architectures have been designed: Minimalist, Simple, and Performance, as well as several augmentation options. The design and refinement of these architectures was performed using numerous models and tools developed for this work. A unique feature of the analysis in this thesis was that the architectures considered combine different navigation assets (onboard, on-surface and on-orbit). The three main Navigation/Communication architectures were then evaluated and compared using several metrics, such as navigation coverage, accuracy and operability, communication metrics, and mass. Based on this analysis we recommend the initial deployment of the Simple architecture for surface exploration of the Moon and Mars with a gradual accretion of assets and possibly transition to the Performance architecture. A specific combination of onboard and vision-based sensors is recommended as the fundamental navigation equipment. In addition to this navigation study, a control-based analysis of formation flying dynamic models around the libration point L₂ of the Sun-Earth system is also presented. The objective of this research was to assess the quality of different dynamical models of the relative motion of two spacecraft in orbit around Sun-Earth L₂.",
    "advisors": ["Jonathan P. How"],
    "text": "Integrated navigation architecture analysis for Moon and Mars exploration The new solar system exploration objectives announced in January 2004 have the goal of sending humans back to the Moon by the year 2020 in preparation for human exploration of Mars. Advanced, but cost effective, surface navigation and communication capabilities are required to support these new exploration objectives. In response to this need, a set of three Navigation/Communication architectures have been designed: Minimalist, Simple, and Performance, as well as several augmentation options. The design and refinement of these architectures was performed using numerous models and tools developed for this work. A unique feature of the analysis in this thesis was that the architectures considered combine different navigation assets (onboard, on-surface and on-orbit). The three main Navigation/Communication architectures were then evaluated and compared using several metrics, such as navigation coverage, accuracy and operability, communication metrics, and mass. Based on this analysis we recommend the initial deployment of the Simple architecture for surface exploration of the Moon and Mars with a gradual accretion of assets and possibly transition to the Performance architecture. A specific combination of onboard and vision-based sensors is recommended as the fundamental navigation equipment. In addition to this navigation study, a control-based analysis of formation flying dynamic models around the libration point L₂ of the Sun-Earth system is also presented. The objective of this research was to assess the quality of different dynamical models of the relative motion of two spacecraft in orbit around Sun-Earth L₂."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8096",
    "title": "Aerodynamic design of an aspirated counter-rotating compressor",
    "abstract": "A primary goal in compressor design for jet engines is the reduction of size and weight. This can be achieved by increasing the work output per stage, thereby reducing the required number of stages. In this thesis, the aerodynamic design of a high speed compressor that produces a pressure ratio of 9.1:1 in only two stages (rather than the typical six or seven) is presented. This is accomplished by employing blade aspiration in conjunction with rotor counter-rotation. Aspiration has been shown to make feasible significantly increased blade loading and counter-rotation provides a means of taking full advantage of this potential throughout a multistage compressor. The aspirated counter-rotating compressor was designed using a one-dimensional stage analysis program coupled with an axisymmetric throughflow code and a quasi-three- dimensional cascade code for blade design. The design of each stage focused on maximizing pressure ratio within diffusion factor and relative inlet Mach number (i.e. shock loss) constraints. The exit angle of the first stator was optimized to maximize the pressure ratio of the counter-rotating (second) rotor. The blade design code MISES allowed for each feature of the blade sections, including aspiration, to be precisely designed for the predicted conditions. To improve the process of designing blades with MISES, extensive analysis of previously designed high-speed aspirated blades was performed to identify the relationships between various blade features.",
    "advisors": ["Jack L. Kerrebrock"],
    "text": "Aerodynamic design of an aspirated counter-rotating compressor A primary goal in compressor design for jet engines is the reduction of size and weight. This can be achieved by increasing the work output per stage, thereby reducing the required number of stages. In this thesis, the aerodynamic design of a high speed compressor that produces a pressure ratio of 9.1:1 in only two stages (rather than the typical six or seven) is presented. This is accomplished by employing blade aspiration in conjunction with rotor counter-rotation. Aspiration has been shown to make feasible significantly increased blade loading and counter-rotation provides a means of taking full advantage of this potential throughout a multistage compressor. The aspirated counter-rotating compressor was designed using a one-dimensional stage analysis program coupled with an axisymmetric throughflow code and a quasi-three- dimensional cascade code for blade design. The design of each stage focused on maximizing pressure ratio within diffusion factor and relative inlet Mach number (i.e. shock loss) constraints. The exit angle of the first stator was optimized to maximize the pressure ratio of the counter-rotating (second) rotor. The blade design code MISES allowed for each feature of the blade sections, including aspiration, to be precisely designed for the predicted conditions. To improve the process of designing blades with MISES, extensive analysis of previously designed high-speed aspirated blades was performed to identify the relationships between various blade features."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101440",
    "title": "Model adaptivity for goal-oriented inference",
    "abstract": "In scientific and engineering contexts, physical systems are represented by mathematical models, characterized by a set of parameters. The inverse problem arises when the parameters are unknown and one tries to infer these parameters based on observations. Solving the inverse problem can require many model simulations, which may be expensive for complex models; multiple models of varying fidelity and complexity may be available to describe the physical system. However, inferring the parameters may only be an intermediate step, and what is ultimately desired may be a low-dimensional Quantity of Interest (QoI); we refer to this as the goal-oriented inverse problem. We present a novel algorithm for solving the goal-oriented inverse problem, which allows one to manage the fidelity of modeling choices while solving the inverse problem. We formulate a hierarchy of models, and assume that the QoI obtained by inferring the parameters with the highest-fidelity model is the most accurate QoI. We derive an estimate for the error in the QoI from inferring the parameters using a lower-fidelity model instead of the highest-fidelity model. This estimate can be localized to individual elements of a discretized domain, and this element-wise decomposition can then be used to adaptively form mixed-fidelity models. These mixed-fidelity models can be used to infer the parameters, while controlling the error in the QoI. We demonstrate the method with two pairs of steady-state models in 2D. In one pair, the models differ in the physics included; in the other pair, the models differ in the space to which the parameters belong. In both cases, we are able to obtain a QoI estimate with a small relative error without having to solve the inverse problem with the high-fidelity model. We also demonstrate a case where solving the inverse problem with the high-fidelity model requires a more complex algorithm, but where our method gives a mixed-fidelity model with which we can infer parameters using a simple Newton solver, while achieving a low error in the QoI.",
    "advisors": ["Karen Willcox"],
    "text": "Model adaptivity for goal-oriented inference In scientific and engineering contexts, physical systems are represented by mathematical models, characterized by a set of parameters. The inverse problem arises when the parameters are unknown and one tries to infer these parameters based on observations. Solving the inverse problem can require many model simulations, which may be expensive for complex models; multiple models of varying fidelity and complexity may be available to describe the physical system. However, inferring the parameters may only be an intermediate step, and what is ultimately desired may be a low-dimensional Quantity of Interest (QoI); we refer to this as the goal-oriented inverse problem. We present a novel algorithm for solving the goal-oriented inverse problem, which allows one to manage the fidelity of modeling choices while solving the inverse problem. We formulate a hierarchy of models, and assume that the QoI obtained by inferring the parameters with the highest-fidelity model is the most accurate QoI. We derive an estimate for the error in the QoI from inferring the parameters using a lower-fidelity model instead of the highest-fidelity model. This estimate can be localized to individual elements of a discretized domain, and this element-wise decomposition can then be used to adaptively form mixed-fidelity models. These mixed-fidelity models can be used to infer the parameters, while controlling the error in the QoI. We demonstrate the method with two pairs of steady-state models in 2D. In one pair, the models differ in the physics included; in the other pair, the models differ in the space to which the parameters belong. In both cases, we are able to obtain a QoI estimate with a small relative error without having to solve the inverse problem with the high-fidelity model. We also demonstrate a case where solving the inverse problem with the high-fidelity model requires a more complex algorithm, but where our method gives a mixed-fidelity model with which we can infer parameters using a simple Newton solver, while achieving a low error in the QoI."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46798",
    "title": "Influence of spatial abilities on primary and secondary space telerobotics operator performance",
    "abstract": "Teleoperated manipulators have been invaluable tools during space missions. Arm operators work in pairs, with the primary operator controlling the arm and the secondary operator assisting by monitoring arm clearance and helping to avoid singularities. Individual ability to manipulate the arm and integrate camera views is believed to correlate with 3 subcomponents of spatial intelligence: spatial visualization (SV), mental rotation (MR) and perspective taking (PT). In particular, PT (the ability to imagine an object from another viewpoint) is thought to be important for integrating camera views. Two experiments were performed; one on primary operator performance, and one on secondary operator performance. In Experiment 1, 19 naive subjects were trained to manipulate a 6 degree of freedom (DOF) simulated arm using a pair of hand-controllers. Over 18 trials, the disparity between the arm's control frame and the cameras was varied between low (< 90 degrees) and high (> 90 degrees) conditions. We used the Cube Comparisons (CC) test to assess SV, the Vandenberg Mental Rotations Test (MRT) to assess MR, and the Purdue Spatial Visualization of Views Test (PSVT) and a Perspective Taking Ability (PTA) test to assess PT. Subjects with high PSVT scores moved the arm more directly to the target and were better at maintaining the required clearance between the arm and obstacles, even without a direct camera view. The subjects' performance degraded under the high disparity condition. In Experiment 2, 11 naive and 9 returning subjects were trained to manipulate the same simulated arm during 6 trials and then acted as a secondary operator observing an additional 32 trials.",
    "advisors": ["Charles M. Oman"],
    "text": "Influence of spatial abilities on primary and secondary space telerobotics operator performance Teleoperated manipulators have been invaluable tools during space missions. Arm operators work in pairs, with the primary operator controlling the arm and the secondary operator assisting by monitoring arm clearance and helping to avoid singularities. Individual ability to manipulate the arm and integrate camera views is believed to correlate with 3 subcomponents of spatial intelligence: spatial visualization (SV), mental rotation (MR) and perspective taking (PT). In particular, PT (the ability to imagine an object from another viewpoint) is thought to be important for integrating camera views. Two experiments were performed; one on primary operator performance, and one on secondary operator performance. In Experiment 1, 19 naive subjects were trained to manipulate a 6 degree of freedom (DOF) simulated arm using a pair of hand-controllers. Over 18 trials, the disparity between the arm's control frame and the cameras was varied between low (< 90 degrees) and high (> 90 degrees) conditions. We used the Cube Comparisons (CC) test to assess SV, the Vandenberg Mental Rotations Test (MRT) to assess MR, and the Purdue Spatial Visualization of Views Test (PSVT) and a Perspective Taking Ability (PTA) test to assess PT. Subjects with high PSVT scores moved the arm more directly to the target and were better at maintaining the required clearance between the arm and obstacles, even without a direct camera view. The subjects' performance degraded under the high disparity condition. In Experiment 2, 11 naive and 9 returning subjects were trained to manipulate the same simulated arm during 6 trials and then acted as a secondary operator observing an additional 32 trials."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45217",
    "title": "Minimizing high spatial frequency residual in active space telescope mirrors",
    "abstract": "The trend in future space telescopes is towards large apertures and lightweight, rib-stiffened, and actively controlled deformable mirrors. These mirror architectures permit the development of segmented and deployed primary mirrors that lead to tremendous advancement in space telescope performance. Rib-stiffened and discretely actuated deformable mirrors have been shown to effectively mitigate common low order disturbances, but they are inevitably plagued by the \"correction limit,\" or the extent to which the actuators can correct for a given shape disturbance. Improving the correctability of deformable mirrors requires understanding the origins of the correction limit, and optimizing the mirror design accordingly. This thesis details efforts to evaluate the mirror correction limit and the three predominant high spatial frequency mirror surface residual components: actuation-induced dimpling, manufacturing-induced print-through, and disturbance-induced uncorrectable error. The methods for simulating each effect are discussed, and an objective function is developed to quantify the effects of these residual components to gage the performance of each mirror design. A gradient descent algorithm is combined with the parametric capability of the Modular Optical Space Telescope (MOST) modeling tool to allow rapid trade space navigation and optimization of the mirror design across variations in mirror areal density, f-number, structural mass fractions, and rib aspect ratio. These optimization routines yield more advanced design heuristics that improve upon the simplified design techniques that are typical in industry. By forming the heuristics in terms of minimum machinable rib thickness, these new design relationships produce mirrors that satisfy manufacturing constraints and minimize uncorrectable high spatial frequency error.",
    "advisors": ["David W. Miller"],
    "text": "Minimizing high spatial frequency residual in active space telescope mirrors The trend in future space telescopes is towards large apertures and lightweight, rib-stiffened, and actively controlled deformable mirrors. These mirror architectures permit the development of segmented and deployed primary mirrors that lead to tremendous advancement in space telescope performance. Rib-stiffened and discretely actuated deformable mirrors have been shown to effectively mitigate common low order disturbances, but they are inevitably plagued by the \"correction limit,\" or the extent to which the actuators can correct for a given shape disturbance. Improving the correctability of deformable mirrors requires understanding the origins of the correction limit, and optimizing the mirror design accordingly. This thesis details efforts to evaluate the mirror correction limit and the three predominant high spatial frequency mirror surface residual components: actuation-induced dimpling, manufacturing-induced print-through, and disturbance-induced uncorrectable error. The methods for simulating each effect are discussed, and an objective function is developed to quantify the effects of these residual components to gage the performance of each mirror design. A gradient descent algorithm is combined with the parametric capability of the Modular Optical Space Telescope (MOST) modeling tool to allow rapid trade space navigation and optimization of the mirror design across variations in mirror areal density, f-number, structural mass fractions, and rib aspect ratio. These optimization routines yield more advanced design heuristics that improve upon the simplified design techniques that are typical in industry. By forming the heuristics in terms of minimum machinable rib thickness, these new design relationships produce mirrors that satisfy manufacturing constraints and minimize uncorrectable high spatial frequency error."
}, {
    "id": "oai:dspace.mit.edu:1721.1/71283",
    "title": "Performance characterization and optimization of a diverging cusped field thruster with a calibrated counter-weighted millinewton thrust stand",
    "abstract": "The previously developed Diverging Cusped Field Thruster (DCFT) has undergone further investigations and performance characterization. The DCFT is a magnetically conned plasma thruster that uses cusped magnetic fields to confine electron ow and reduce losses to the walls. The magnetic confinement of the plasma away from the walls also reduces wall erosion to increase thruster lifetime. Additionally, modifications to the original DCFT have increased robustness and decreased mass to become more desirable for space flight. Research on reducing the plasma plume divergence of the thruster by altering the magnetic field has also been performed. The DCFT has exhibited competitive thrust and eciency performance when compared to typical Hall thrusters of similar size. Specifically, the anode eciency reached a maximum of 39.3% providing 11.8 mN of thrust with a specific impulse of 1436 s. The xenon mass ow rate to the anode was 8.5 standard cubic centimeters per minute, and the power consumption was 210 W. Two distinct modes, as well as a \"mixed\" mode, were observed during performance testing and had signicant, though not completely predictable, effects on thruster performance. The modes differ in plasma diffusivity and anode current. Facility effects, such as chamber back pressure and cathode coupling, on performance were also briefly y researched. In order to characterize the performance of the DCFT, the Milli-Newton Thrust Stand (MiNTS) was developed. The MiNTS is a non-conventional torsional-style thrust stand capable of measuring thrust in the range of 3 to 20 mN with an accuracy of up to 0.2 mN. Calibration of the stand is necessary to map the output of the MiNTS to the force felt by it. A calibration stand was designed to apply a known force to the MiNTS using weights. The MiNTS is controlled by a Labview Virtual Instrument that can measure and counteract the force of the DCFT. Drift forces due to external connections to the MiNTS and thermal transfer from the DCFT are also studied, and processes for negating the drift forces are provided.",
    "advisors": ["Paulo C. Lozano"],
    "text": "Performance characterization and optimization of a diverging cusped field thruster with a calibrated counter-weighted millinewton thrust stand The previously developed Diverging Cusped Field Thruster (DCFT) has undergone further investigations and performance characterization. The DCFT is a magnetically conned plasma thruster that uses cusped magnetic fields to confine electron ow and reduce losses to the walls. The magnetic confinement of the plasma away from the walls also reduces wall erosion to increase thruster lifetime. Additionally, modifications to the original DCFT have increased robustness and decreased mass to become more desirable for space flight. Research on reducing the plasma plume divergence of the thruster by altering the magnetic field has also been performed. The DCFT has exhibited competitive thrust and eciency performance when compared to typical Hall thrusters of similar size. Specifically, the anode eciency reached a maximum of 39.3% providing 11.8 mN of thrust with a specific impulse of 1436 s. The xenon mass ow rate to the anode was 8.5 standard cubic centimeters per minute, and the power consumption was 210 W. Two distinct modes, as well as a \"mixed\" mode, were observed during performance testing and had signicant, though not completely predictable, effects on thruster performance. The modes differ in plasma diffusivity and anode current. Facility effects, such as chamber back pressure and cathode coupling, on performance were also briefly y researched. In order to characterize the performance of the DCFT, the Milli-Newton Thrust Stand (MiNTS) was developed. The MiNTS is a non-conventional torsional-style thrust stand capable of measuring thrust in the range of 3 to 20 mN with an accuracy of up to 0.2 mN. Calibration of the stand is necessary to map the output of the MiNTS to the force felt by it. A calibration stand was designed to apply a known force to the MiNTS using weights. The MiNTS is controlled by a Labview Virtual Instrument that can measure and counteract the force of the DCFT. Drift forces due to external connections to the MiNTS and thermal transfer from the DCFT are also studied, and processes for negating the drift forces are provided."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46796",
    "title": "A carbon nanotube bearing and Stodola rotor",
    "abstract": "A nano-scale rotor supported on a cantilevered multi-wall carbon nanotube (MWNT) shaft (Stodola configuration) is proposed. The nanotube is also expected to function as the bearing, since individual walls of a MWNT are not strongly bonded and can slide on each other with low friction. While MWNT based rotors have been previously constructed, they have so far been limited to horizontally oriented nanotubes. The rotor uses a vertically aligned tube, which allows superior control of the rotor geometry, enabling improved rotor balancing and axisymmetric features such as electrodes or blades. The rotor is proposed as a test stand for measuring inter-wall friction in MWNTs. The low friction in nanotubes has been studied with simulations and experiments, and while it is agreed that relative motion between walls is possible, there is much debate about the qualitative nature of the friction force between walls. Furthermore the reported quantitative values of friction vary by as much as ten orders of magnitude. The proposed rotor might be used to gather new friction data on rotating MWNT bearings at higher speeds that previously attempted. In addition, identical rotors fabricated on nanotubes of varying size, type, and crystalline quality might provide a large dataset that could be used to find correlations between friction behavior and these factors. Applications for the rotor beyond a friction testing apparatus could include pumps to work with existing micro-chemical sensors, gyroscopes, energy storage flywheels, and turbomachinery for power generation. A fabrication process for the proposed rotor was developed, and is being refined. An isolated vertically aligned MWNT is grown by chemical vapor deposition (CVD), from a nickel catalyst dot defined with electron-beam lithography. A silicon dioxide sacrificial layer is applied, followed by a polysilicon layer from which to cut out the rotor.",
    "advisors": ["David J. Carter"],
    "text": "A carbon nanotube bearing and Stodola rotor A nano-scale rotor supported on a cantilevered multi-wall carbon nanotube (MWNT) shaft (Stodola configuration) is proposed. The nanotube is also expected to function as the bearing, since individual walls of a MWNT are not strongly bonded and can slide on each other with low friction. While MWNT based rotors have been previously constructed, they have so far been limited to horizontally oriented nanotubes. The rotor uses a vertically aligned tube, which allows superior control of the rotor geometry, enabling improved rotor balancing and axisymmetric features such as electrodes or blades. The rotor is proposed as a test stand for measuring inter-wall friction in MWNTs. The low friction in nanotubes has been studied with simulations and experiments, and while it is agreed that relative motion between walls is possible, there is much debate about the qualitative nature of the friction force between walls. Furthermore the reported quantitative values of friction vary by as much as ten orders of magnitude. The proposed rotor might be used to gather new friction data on rotating MWNT bearings at higher speeds that previously attempted. In addition, identical rotors fabricated on nanotubes of varying size, type, and crystalline quality might provide a large dataset that could be used to find correlations between friction behavior and these factors. Applications for the rotor beyond a friction testing apparatus could include pumps to work with existing micro-chemical sensors, gyroscopes, energy storage flywheels, and turbomachinery for power generation. A fabrication process for the proposed rotor was developed, and is being refined. An isolated vertically aligned MWNT is grown by chemical vapor deposition (CVD), from a nickel catalyst dot defined with electron-beam lithography. A silicon dioxide sacrificial layer is applied, followed by a polysilicon layer from which to cut out the rotor."
}, {
    "id": "oai:dspace.mit.edu:1721.1/67192",
    "title": "Identification and evolution of quantities of interest for a stochastic process view of complex space system development",
    "abstract": "The objective of stochastic process design is to strategically identify, measure, and reduce sources of uncertainty to guide the development of complex systems. Fundamental to this design approach is the idea that system development is driven by measurable characteristics called quantities of interest. These quantities of interest collectively describe the state of system development and evolve as the system matures. This thesis provides context for the contributions of quantities of interest to a stochastic process view of complex system development using three space hardware development projects. The CASTOR satellite provides the opportunity for retrospective identification of quantities of interest and their evolution through time. As a complement to CASTOR, the preliminary design of the REXIS x-ray spectrometer provides the foundation for applying stochastic process approaches during the early phases of system development. Lastly, a spacecraft panel structural dynamics experiment is presented that illustrates analysis techniques commonly employed in stochastic process analysis.",
    "advisors": ["David W. Miller"],
    "text": "Identification and evolution of quantities of interest for a stochastic process view of complex space system development The objective of stochastic process design is to strategically identify, measure, and reduce sources of uncertainty to guide the development of complex systems. Fundamental to this design approach is the idea that system development is driven by measurable characteristics called quantities of interest. These quantities of interest collectively describe the state of system development and evolve as the system matures. This thesis provides context for the contributions of quantities of interest to a stochastic process view of complex system development using three space hardware development projects. The CASTOR satellite provides the opportunity for retrospective identification of quantities of interest and their evolution through time. As a complement to CASTOR, the preliminary design of the REXIS x-ray spectrometer provides the foundation for applying stochastic process approaches during the early phases of system development. Lastly, a spacecraft panel structural dynamics experiment is presented that illustrates analysis techniques commonly employed in stochastic process analysis."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46570",
    "title": "Development of an aerodynamic/RCS framework for the preliminary design of a hypersonic aircraft",
    "abstract": "The design of hypersonic airbreathing aircraft pushes the envelope of current state-ofthe-art aerospace propulsion and materials technology. Therefore, these aircraft are highly integrated to produce adequate thrust, reduce drag, and limit surface heating. Consequently, every aircraft component (e.g., wings, fuselage, propulsion system) is sensitive to changes in every other component. Including Radar Cross Section (RCS) considerations further complicates matters. During preliminary design, this requires the rapid analysis of different aircraft configurations to investigate component interactions and determine performance trends. This thesis presents a framework and accompanying software for performing such an analysis. The intent is to optimize a hypersonic airbreathing aircraft design in terms of aerodynamic performance and RCS. Computational Fluid Dynamics (CFD) and Computational Electromagnetics (CEM) are the two main framework software components. CFD simulates airflow around the aircraft to analyze its aerodynamic performance. Alternately, CEM simulates the electromagnetic signature of the aircraft to predict its RCS. The framework begins with the generation of a three-dimensional computer aided design aircraft model. Next, a grid generator discretizes this model. The flow simulation is performed on this grid and the aircraft's aerodynamic characteristics are determined. Flow visualization aids this determination. Then, aircraft geometry refinements are made to improve aerodynamic performance. Afterward, CEM is performed on aerodynamically favorable designs at various aspect angles and frequencies. RCS values are determined and used to rank the different configurations. Also, inverse synthetic aperture radar images are generated to locate major scattering centers and aid the design refinement. The design loop continues in this fashion until an acceptable aircraft design is achieved. The NASA X-43A test vehicle was used to validate this preliminary design framework.",
    "advisors": ["Jaime Peraire"],
    "text": "Development of an aerodynamic/RCS framework for the preliminary design of a hypersonic aircraft The design of hypersonic airbreathing aircraft pushes the envelope of current state-ofthe-art aerospace propulsion and materials technology. Therefore, these aircraft are highly integrated to produce adequate thrust, reduce drag, and limit surface heating. Consequently, every aircraft component (e.g., wings, fuselage, propulsion system) is sensitive to changes in every other component. Including Radar Cross Section (RCS) considerations further complicates matters. During preliminary design, this requires the rapid analysis of different aircraft configurations to investigate component interactions and determine performance trends. This thesis presents a framework and accompanying software for performing such an analysis. The intent is to optimize a hypersonic airbreathing aircraft design in terms of aerodynamic performance and RCS. Computational Fluid Dynamics (CFD) and Computational Electromagnetics (CEM) are the two main framework software components. CFD simulates airflow around the aircraft to analyze its aerodynamic performance. Alternately, CEM simulates the electromagnetic signature of the aircraft to predict its RCS. The framework begins with the generation of a three-dimensional computer aided design aircraft model. Next, a grid generator discretizes this model. The flow simulation is performed on this grid and the aircraft's aerodynamic characteristics are determined. Flow visualization aids this determination. Then, aircraft geometry refinements are made to improve aerodynamic performance. Afterward, CEM is performed on aerodynamically favorable designs at various aspect angles and frequencies. RCS values are determined and used to rank the different configurations. Also, inverse synthetic aperture radar images are generated to locate major scattering centers and aid the design refinement. The design loop continues in this fashion until an acceptable aircraft design is achieved. The NASA X-43A test vehicle was used to validate this preliminary design framework."
}, {
    "id": "oai:dspace.mit.edu:1721.1/71458",
    "title": "Decentralized task allocation for dynamic environments",
    "abstract": "This thesis presents an overview of the design process for creating greedy decentralized task allocation algorithms and outlines the main decisions that progressed the algorithm through three different forms. The first form was called the Sequential Greedy Algorithm (SGA). This algorithm, although fast, relied on a large number of iterations to converge, which slowed convergence in decentralized environments. The second form was called the Consensus Based Bundle Algorithm (CBBA). CBBA required significantly fewer iterations than SGA but it is noted that both still rely on global synchronization mechanisms. These synchronization mechanisms end up being difficult to enforce in decentralized environments. The main result of this thesis is the creation of the Asynchronous Consensus Based Bundle Algorithm (ACBBA). ACBBA broke the global synchronous assumptions of CBBA and SGA to allow each agent more autonomy and thus provided more robustness to the task allocation solutions in these decentralized environments.",
    "advisors": ["Jonathan P. How"],
    "text": "Decentralized task allocation for dynamic environments This thesis presents an overview of the design process for creating greedy decentralized task allocation algorithms and outlines the main decisions that progressed the algorithm through three different forms. The first form was called the Sequential Greedy Algorithm (SGA). This algorithm, although fast, relied on a large number of iterations to converge, which slowed convergence in decentralized environments. The second form was called the Consensus Based Bundle Algorithm (CBBA). CBBA required significantly fewer iterations than SGA but it is noted that both still rely on global synchronization mechanisms. These synchronization mechanisms end up being difficult to enforce in decentralized environments. The main result of this thesis is the creation of the Asynchronous Consensus Based Bundle Algorithm (ACBBA). ACBBA broke the global synchronous assumptions of CBBA and SGA to allow each agent more autonomy and thus provided more robustness to the task allocation solutions in these decentralized environments."
}, {
    "id": "oai:dspace.mit.edu:1721.1/57882",
    "title": "An assessment of body force representations for compressor stall simulation",
    "abstract": "This thesis examines an axial compressor body force representation constructed from 3D CFD calculations. The radial distribution of body forces is compared to that of a body force representation based on axisymmetric streamline curvature (SLC) calculations, and shown to be in qualitative agreement except in the vicinity of the blade tip. In terms of stall inception type and stall point, computations based on both representations exhibit agreement with rig test data. A parametric study is undertaken in which the magnitude of the forces in the blade tip region of both representations is reduced so as to obtain reductions in compressor pressure rise similar to those observed experimentally due to increased tip clearance. It is shown that on a back-to-back basis, a given change to the end wall forces produces similar effects on the computed stall point, whether the underlying body force representation derives from 3D CFD or SLC. Based on this result one route to capturing effects of tip clearance on stall prediction can be the development of a tip clearance body force model for use in conjunction with SLC calculations.",
    "advisors": ["Edward M. Greitzer", "Choon S. Tan"],
    "text": "An assessment of body force representations for compressor stall simulation This thesis examines an axial compressor body force representation constructed from 3D CFD calculations. The radial distribution of body forces is compared to that of a body force representation based on axisymmetric streamline curvature (SLC) calculations, and shown to be in qualitative agreement except in the vicinity of the blade tip. In terms of stall inception type and stall point, computations based on both representations exhibit agreement with rig test data. A parametric study is undertaken in which the magnitude of the forces in the blade tip region of both representations is reduced so as to obtain reductions in compressor pressure rise similar to those observed experimentally due to increased tip clearance. It is shown that on a back-to-back basis, a given change to the end wall forces produces similar effects on the computed stall point, whether the underlying body force representation derives from 3D CFD or SLC. Based on this result one route to capturing effects of tip clearance on stall prediction can be the development of a tip clearance body force model for use in conjunction with SLC calculations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28909",
    "title": "Evaluation of a digital communication device for railroad worker safety",
    "abstract": "This thesis documents the testing of a prototype of a smartphone to be used by roadway workers and dispatchers that was based in a wireless data link service. The main purpose of using a smartphone in railroad communications is to eliminate errors due to radio and pronunciation deficiencies. Previous studies analyzed the communication environment of the dispatcher in order to address questions based upon data link becoming a means for sending and receiving information in railroad operations. These studies have examined what kind of information is appropriate for each medium (voice and visual), and by what criteria a dispatcher will select which communication medium. Building on these studies, this work presents a comparison between a radio and data link mediums for a long communication of a characteristic type: assignment of a form D. This thesis reports on the on site testing of the data link system which proved to be useful and efficient in certain aspects of railroad applications. The new system was faster and more effective than the radio communication when used to convey long messages such as filling out Form Ds. The radio communication was faster than the datalink for confirmation communications that only require yes/no answers. One reason for this difference appears to be the users' unfamiliarity with the device. The time to convey short messages could also be reduced after the users become more proficient with the new system. The document also includes an analysis of the regulatory challenges that the new system would bring. A list or recommendations for the new regulations are presented at the end of the report.",
    "advisors": ["Thomas B. Sheridan"],
    "text": "Evaluation of a digital communication device for railroad worker safety This thesis documents the testing of a prototype of a smartphone to be used by roadway workers and dispatchers that was based in a wireless data link service. The main purpose of using a smartphone in railroad communications is to eliminate errors due to radio and pronunciation deficiencies. Previous studies analyzed the communication environment of the dispatcher in order to address questions based upon data link becoming a means for sending and receiving information in railroad operations. These studies have examined what kind of information is appropriate for each medium (voice and visual), and by what criteria a dispatcher will select which communication medium. Building on these studies, this work presents a comparison between a radio and data link mediums for a long communication of a characteristic type: assignment of a form D. This thesis reports on the on site testing of the data link system which proved to be useful and efficient in certain aspects of railroad applications. The new system was faster and more effective than the radio communication when used to convey long messages such as filling out Form Ds. The radio communication was faster than the datalink for confirmation communications that only require yes/no answers. One reason for this difference appears to be the users' unfamiliarity with the device. The time to convey short messages could also be reduced after the users become more proficient with the new system. The document also includes an analysis of the regulatory challenges that the new system would bring. A list or recommendations for the new regulations are presented at the end of the report."
}, {
    "id": "oai:dspace.mit.edu:1721.1/59681",
    "title": "Feasibility study of long-life micro fuel cell power supply for sensor networks for space and terrestrial applications",
    "abstract": "Sensor networks used for activities like border security, search and rescue, planetary exploration, commonly operate in harsh environments for long durations, where human supervision is minimal. A major challenge confronting such devices is providing adequate and reliable power supply required for long durations. This research considers the feasibility of a miniature Proton Exchange Membrane (PEM) fuel cell system coupled with battery to supply power for long life missions. The focus of this research is to prove the feasibility of long-life, self-contained power-supplies using miniature fuel cells for low-power distributed sensor networks. In this research, the performance of fuel cell power-supplies weighing not more than a few hundred grams is studied. The performance of the PEM fuel cell is modeled, analyzed and validated using experimental results. The feasibility of the fuel cell power systems are studied for two reference missions - one on the lunar surface and the other in the desert regions of Negev, Israel. This research analyzes the use of passive methods to achieve thermal, air and water management for PEM fuel cells supplying power to these field sensors. The results of this study suggest that the proposed fuel cell power system is capable of providing power to sensor modules in challenging field conditions with operational lives extending from many months to years. The scope of this concept can be extended to power devices such as micro-robots and small unmanned aerial vehicles operating in extreme environmental conditions for sustained periods of time.",
    "advisors": ["Steven Dubowsky"],
    "text": "Feasibility study of long-life micro fuel cell power supply for sensor networks for space and terrestrial applications Sensor networks used for activities like border security, search and rescue, planetary exploration, commonly operate in harsh environments for long durations, where human supervision is minimal. A major challenge confronting such devices is providing adequate and reliable power supply required for long durations. This research considers the feasibility of a miniature Proton Exchange Membrane (PEM) fuel cell system coupled with battery to supply power for long life missions. The focus of this research is to prove the feasibility of long-life, self-contained power-supplies using miniature fuel cells for low-power distributed sensor networks. In this research, the performance of fuel cell power-supplies weighing not more than a few hundred grams is studied. The performance of the PEM fuel cell is modeled, analyzed and validated using experimental results. The feasibility of the fuel cell power systems are studied for two reference missions - one on the lunar surface and the other in the desert regions of Negev, Israel. This research analyzes the use of passive methods to achieve thermal, air and water management for PEM fuel cells supplying power to these field sensors. The results of this study suggest that the proposed fuel cell power system is capable of providing power to sensor modules in challenging field conditions with operational lives extending from many months to years. The scope of this concept can be extended to power devices such as micro-robots and small unmanned aerial vehicles operating in extreme environmental conditions for sustained periods of time."
}, {
    "id": "oai:dspace.mit.edu:1721.1/85801",
    "title": "Centrifugal compressor return channel shape optimization using adjoint method",
    "abstract": "This thesis describes the construction of an automated gradient-based optimization process using the adjoint method and its application to centrifugal compressor return channel loss reduction. A proper objective function definition and a generalized geometry parametrization and manipulation algorithm were developed, and the appropriate adjoint equations and boundary conditions were derived for internal flow of an axisymmetric incompressible laminar flow. The adjoint-based gradient calculation was then validated against finite-difference calculations and embedded in a quasi- Newton optimization algorithm. An optimal design was proposed, which achieved an approximately 5% performance improvement compared to the baseline design in an incompressible laminar flow. The geometry was assessed in a compressible turbulent flow at the actual Mach number and Reynolds number and found to yield a 11% performance improvement for an axisymmetric channel with a previously optimized geometry.",
    "advisors": ["Qiqi Wang", "Edward M. Greitzer"],
    "text": "Centrifugal compressor return channel shape optimization using adjoint method This thesis describes the construction of an automated gradient-based optimization process using the adjoint method and its application to centrifugal compressor return channel loss reduction. A proper objective function definition and a generalized geometry parametrization and manipulation algorithm were developed, and the appropriate adjoint equations and boundary conditions were derived for internal flow of an axisymmetric incompressible laminar flow. The adjoint-based gradient calculation was then validated against finite-difference calculations and embedded in a quasi- Newton optimization algorithm. An optimal design was proposed, which achieved an approximately 5% performance improvement compared to the baseline design in an incompressible laminar flow. The geometry was assessed in a compressible turbulent flow at the actual Mach number and Reynolds number and found to yield a 11% performance improvement for an axisymmetric channel with a previously optimized geometry."
}, {
    "id": "oai:dspace.mit.edu:1721.1/47794",
    "title": "Trajectory optimization for target localization using small unmanned aerial vehicles",
    "abstract": "Small unmanned aerial vehicles (UAVs), equipped with navigation systems and video capability, are currently being deployed for intelligence, reconnaissance and surveillance missions. One particular mission of interest involves computing location estimates for targets detected by onboard sensors. Combining UAV state estimates with information gathered by the imaging sensors leads to bearing measurements of the target that can be used to determine the target's location. This 3-D bearings-only estimation problem is nonlinear and traditional filtering methods produce biased and uncertain estimates, occasionally leading to filter instabilities. Careful selection of the measurement locations greatly enhances filter performance, motivating the development of UAV trajectories that minimize target location estimation error and improve filter convergence. The objective of this work is to develop guidance algorithms that enable the UAV to fly trajectories that increase the amount of information provided by the measurements and improve overall estimation observability, resulting in proper target tracking and an accurate target location estimate. The performance of the target estimation is dependent upon the positions from which measurements are taken relative to the target and to previous measurements. Past research has provided methods to quantify the information content of a set of measurements using the Fisher Information Matrix (FIM). Forming objective functions based on the FIM and using numerical optimization methods produce UAV trajectories that locally maximize the information content for a given number of measurements. In this project, trajectory optimization leads to the development of UAV flight paths that provide the highest amount of information about the target, while considering sensor restrictions, vehicle dynamics and operation constraints.",
    "advisors": ["Emilio Frazzoli", "Richard M. Kolacinski"],
    "text": "Trajectory optimization for target localization using small unmanned aerial vehicles Small unmanned aerial vehicles (UAVs), equipped with navigation systems and video capability, are currently being deployed for intelligence, reconnaissance and surveillance missions. One particular mission of interest involves computing location estimates for targets detected by onboard sensors. Combining UAV state estimates with information gathered by the imaging sensors leads to bearing measurements of the target that can be used to determine the target's location. This 3-D bearings-only estimation problem is nonlinear and traditional filtering methods produce biased and uncertain estimates, occasionally leading to filter instabilities. Careful selection of the measurement locations greatly enhances filter performance, motivating the development of UAV trajectories that minimize target location estimation error and improve filter convergence. The objective of this work is to develop guidance algorithms that enable the UAV to fly trajectories that increase the amount of information provided by the measurements and improve overall estimation observability, resulting in proper target tracking and an accurate target location estimate. The performance of the target estimation is dependent upon the positions from which measurements are taken relative to the target and to previous measurements. Past research has provided methods to quantify the information content of a set of measurements using the Fisher Information Matrix (FIM). Forming objective functions based on the FIM and using numerical optimization methods produce UAV trajectories that locally maximize the information content for a given number of measurements. In this project, trajectory optimization leads to the development of UAV flight paths that provide the highest amount of information about the target, while considering sensor restrictions, vehicle dynamics and operation constraints."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62481",
    "title": "Multilayer network modeling of change propagation for engineering change management",
    "abstract": "Engineering change management is a critical and challenging process within product development. One pervasive source of difficulty for this process is the phenomenon of change propagation, by which a change to one part or element of a design requires additional changes throughout the product. Research efforts to understand and manage change propagation have largely drawn on network analysis. This thesis builds upon past research by introducing a multilayer network model that incorporates three proposed layers, or domains, that contribute to change propagation: namely, the product layer, change layer, and social layer. Each layer contains a distinct network of nodes and intralayer edges, but also connects to the other two layers through inter-layer dependencies. The model facilitates extensive quantitative analysis of change propagation using a repository of single-layer, double-layer, and triple-layer tools and metrics. A case study of a large technical program, which managed over 41,000 change requests in eight years, is employed to demonstrate the practical utility of the model. Most significantly, the case study explores the program's social layer and discovers a real-world correspondence between an engineer's organizational role and the propagation effects of his or her work, as measured by the newly proposed Engineer Change Propagation Index (Engineer-CPI). The case study also reveals that parent-child propagation often spanned more than one, but never more than three, system interfaces, thus confirming the possibility of indirect propagation. Finally, the study finds that propagation always stopped after five, and rarely more than four, generations of descendants. In all, the multilayer network model's holistic approach has significant policy implications for engineering change management in industry.",
    "advisors": ["Olivier L. de Weck"],
    "text": "Multilayer network modeling of change propagation for engineering change management Engineering change management is a critical and challenging process within product development. One pervasive source of difficulty for this process is the phenomenon of change propagation, by which a change to one part or element of a design requires additional changes throughout the product. Research efforts to understand and manage change propagation have largely drawn on network analysis. This thesis builds upon past research by introducing a multilayer network model that incorporates three proposed layers, or domains, that contribute to change propagation: namely, the product layer, change layer, and social layer. Each layer contains a distinct network of nodes and intralayer edges, but also connects to the other two layers through inter-layer dependencies. The model facilitates extensive quantitative analysis of change propagation using a repository of single-layer, double-layer, and triple-layer tools and metrics. A case study of a large technical program, which managed over 41,000 change requests in eight years, is employed to demonstrate the practical utility of the model. Most significantly, the case study explores the program's social layer and discovers a real-world correspondence between an engineer's organizational role and the propagation effects of his or her work, as measured by the newly proposed Engineer Change Propagation Index (Engineer-CPI). The case study also reveals that parent-child propagation often spanned more than one, but never more than three, system interfaces, thus confirming the possibility of indirect propagation. Finally, the study finds that propagation always stopped after five, and rarely more than four, generations of descendants. In all, the multilayer network model's holistic approach has significant policy implications for engineering change management in industry."
}, {
    "id": "oai:dspace.mit.edu:1721.1/67185",
    "title": "Equitable resource allocation using all-pay auctions",
    "abstract": "Auction theory is a very interesting topic which studies mechanisms of various formats for buying and selling goods. There are all kinds of mechanisms covered in the ambit of auction theory, ranging from sealed bid auctions like first-price or second price auctions, to open bid auctions like the English or Dutch auctions. One interesting kind of auction is the all-pay auction, in which, as the name suggests, all the players pay their bids. Analysis of all-pay auctions with virtual money has so far been neglected in the auction theory literature. However, it can be used to design a mechanism with a wide variety of applications including equitable resource allocation, and fairer systems for legislation and corporate management. In this thesis, we shall present this novel mechanism, analyze it in a game-theoretic framework and consider the variants of the problem including the perfect and imperfect information cases. Furthermore, we shall outline how to find the equilibrium bidding strategies for this mechanism which lead to equitable distribution of resources. One possible application of this is in equitable resource allocation, such as in a battlefield where different agents have competing needs for limited resources to complete their missions. Finally, we shall draw the connection between this mechanism, and the process of voting in the legislature or the company board rooms. Currently, a party needs just more than 50% of seats in the legislature to control 100% of the bills. Similarly, a shareholder needs just 51% of shares to completely control the company. We shall show how it is possible to modify these voting systems, using the proposed mechanism to enable even the minority players to have an equitable say in the decisions.",
    "advisors": ["Emilio Frazzoli"],
    "text": "Equitable resource allocation using all-pay auctions Auction theory is a very interesting topic which studies mechanisms of various formats for buying and selling goods. There are all kinds of mechanisms covered in the ambit of auction theory, ranging from sealed bid auctions like first-price or second price auctions, to open bid auctions like the English or Dutch auctions. One interesting kind of auction is the all-pay auction, in which, as the name suggests, all the players pay their bids. Analysis of all-pay auctions with virtual money has so far been neglected in the auction theory literature. However, it can be used to design a mechanism with a wide variety of applications including equitable resource allocation, and fairer systems for legislation and corporate management. In this thesis, we shall present this novel mechanism, analyze it in a game-theoretic framework and consider the variants of the problem including the perfect and imperfect information cases. Furthermore, we shall outline how to find the equilibrium bidding strategies for this mechanism which lead to equitable distribution of resources. One possible application of this is in equitable resource allocation, such as in a battlefield where different agents have competing needs for limited resources to complete their missions. Finally, we shall draw the connection between this mechanism, and the process of voting in the legislature or the company board rooms. Currently, a party needs just more than 50% of seats in the legislature to control 100% of the bills. Similarly, a shareholder needs just 51% of shares to completely control the company. We shall show how it is possible to modify these voting systems, using the proposed mechanism to enable even the minority players to have an equitable say in the decisions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/105614",
    "title": "Investigation of customized refresher training for telerobotic operations in long-duration spaceflight",
    "abstract": "As humanity prepares to break the bonds of Earth's orbit and send explorers deeper into the solar system, mission duration will drastically increase; forcing crewmembers to retain skills and knowledge from prior training on Earth for unprecedented lengths of time. Since performance generally diminishes when a skill is unused, the development of efficient and effective refresher training is essential. The effectiveness of training presumably can be increased by taking into account the learning style of the student and customizing training or retraining material. To understand the effect of customized retraining material on skill reacquisition, we compared space telerobotics performance post training and six months later using two refresher training regimens: written refresher material and personally customized refresher videos. Videos were created by the subjects after training was completed. We used a simulator of the ISS Robotic Arm as a complex task, which requires intricate bi-manual control as well as adherence to complex procedures. We compared performance change between the two retraining styles using subjective instructor evaluation as well as quantitative performance metrics. We assessed each subject's Felder-Silverman Index of Learning Style (ILS), and developed an analogous rubric to assess the teaching style of the refresher videos. We found weak correlations between learning and teaching metrics in 2/4 dimensions. We also found metrics of a subject's spatial abilities (MRT and PTA, with p-values <0.005) predicted performance and retention in procedurally complex tasks. Spatial ability had more effect on the control group's retention than those who received customized retraining. Results of this study will be used to inform NASA on the appropriate refresher materials for long-duration spaceflight crews.",
    "advisors": ["Charles M. Oman"],
    "text": "Investigation of customized refresher training for telerobotic operations in long-duration spaceflight As humanity prepares to break the bonds of Earth's orbit and send explorers deeper into the solar system, mission duration will drastically increase; forcing crewmembers to retain skills and knowledge from prior training on Earth for unprecedented lengths of time. Since performance generally diminishes when a skill is unused, the development of efficient and effective refresher training is essential. The effectiveness of training presumably can be increased by taking into account the learning style of the student and customizing training or retraining material. To understand the effect of customized retraining material on skill reacquisition, we compared space telerobotics performance post training and six months later using two refresher training regimens: written refresher material and personally customized refresher videos. Videos were created by the subjects after training was completed. We used a simulator of the ISS Robotic Arm as a complex task, which requires intricate bi-manual control as well as adherence to complex procedures. We compared performance change between the two retraining styles using subjective instructor evaluation as well as quantitative performance metrics. We assessed each subject's Felder-Silverman Index of Learning Style (ILS), and developed an analogous rubric to assess the teaching style of the refresher videos. We found weak correlations between learning and teaching metrics in 2/4 dimensions. We also found metrics of a subject's spatial abilities (MRT and PTA, with p-values <0.005) predicted performance and retention in procedurally complex tasks. Spatial ability had more effect on the control group's retention than those who received customized retraining. Results of this study will be used to inform NASA on the appropriate refresher materials for long-duration spaceflight crews."
}, {
    "id": "oai:dspace.mit.edu:1721.1/119295",
    "title": "Multi-fidelity data fusion for the design of multidisciplinary systems under uncertainty",
    "abstract": "This thesis presents a multi-fidelity methodology to enable the incorporation of high-fidelity data into a conceptual design process. The methodology is based upon a fidelity weighted combination of Gaussian Process surrogate models that takes into account both the quality of the Gaussian Process approximation and the confidence of the designer in the disciplinary model being approximated. The methodology is demonstrated on the stability and control analysis of a Blended-Wing-Body aircraft's center of gravity limits. The results show that low-fidelity data is enhanced by the presence of high-fidelity data in key areas of the design space. At the same time, the presence of even sparse high-fidelity data is key to reducing the variance in the stability and control analysis, thereby improving the quality of the predictions of the center of gravity limits.",
    "advisors": ["Karen E. Willcox"],
    "text": "Multi-fidelity data fusion for the design of multidisciplinary systems under uncertainty This thesis presents a multi-fidelity methodology to enable the incorporation of high-fidelity data into a conceptual design process. The methodology is based upon a fidelity weighted combination of Gaussian Process surrogate models that takes into account both the quality of the Gaussian Process approximation and the confidence of the designer in the disciplinary model being approximated. The methodology is demonstrated on the stability and control analysis of a Blended-Wing-Body aircraft's center of gravity limits. The results show that low-fidelity data is enhanced by the presence of high-fidelity data in key areas of the design space. At the same time, the presence of even sparse high-fidelity data is key to reducing the variance in the stability and control analysis, thereby improving the quality of the predictions of the center of gravity limits."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112412",
    "title": "Materials for small-scale space propulsion systems",
    "abstract": "This thesis explores a variety of materials and methods for creating emitter arrays for the ion electrospray propulsion system (iEPS), a compact, efficient, and scalable space propulsion system for use in a wide range of space missions. The increasing utilization of small, cheap, easy-to-launch satellites known as CubeSats has spurred demand for a propulsion system which exists at the nexus of high power efficiency, low mass, surface area, and volume, and high specific impulse. iEPS has demonstrated a unique potential to satisfy all of these stringent design requirements in a way no presently existing propulsion system can. The first part of this work explores utilizing microelectromechanical systems (MEMS) processing to increase the thrust density of iEPS. Silicon molds were designed and manufactured with differing emission site size and spacing. Additionally, a variety of materials were tested with the aim of forming a porous network within the molds prior to selective removal of the mold. A molded array is successfully fired as a result of these research efforts. The second part of this work explores creation of porous substrates for use with an existing laser ablation method of creating emitter arrays. The first iEPS thrusters tested in space used porous borosilicate glass emitter chips, which demonstrated shortcomings in terms of material uniformity, pore size, and ionic liquid fuel containment. This work explores materials and methods for improving all of these and demonstrates the successful firing of an array made by sintering a silicon dioxide nano-bead powder.",
    "advisors": ["Paulo C. Lozano"],
    "text": "Materials for small-scale space propulsion systems This thesis explores a variety of materials and methods for creating emitter arrays for the ion electrospray propulsion system (iEPS), a compact, efficient, and scalable space propulsion system for use in a wide range of space missions. The increasing utilization of small, cheap, easy-to-launch satellites known as CubeSats has spurred demand for a propulsion system which exists at the nexus of high power efficiency, low mass, surface area, and volume, and high specific impulse. iEPS has demonstrated a unique potential to satisfy all of these stringent design requirements in a way no presently existing propulsion system can. The first part of this work explores utilizing microelectromechanical systems (MEMS) processing to increase the thrust density of iEPS. Silicon molds were designed and manufactured with differing emission site size and spacing. Additionally, a variety of materials were tested with the aim of forming a porous network within the molds prior to selective removal of the mold. A molded array is successfully fired as a result of these research efforts. The second part of this work explores creation of porous substrates for use with an existing laser ablation method of creating emitter arrays. The first iEPS thrusters tested in space used porous borosilicate glass emitter chips, which demonstrated shortcomings in terms of material uniformity, pore size, and ionic liquid fuel containment. This work explores materials and methods for improving all of these and demonstrates the successful firing of an array made by sintering a silicon dioxide nano-bead powder."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98807",
    "title": "A hybridized discontinuous Galerkin formulation for modeling electrohydrodynamic thrusters",
    "abstract": "Electrohydrodynamic (EHD) thrusters utilize ion neutral collisions in air to produce a propulsive force. The ions are generated at an emitting electrode in an asymmetric capacitor by a corona discharge. This thesis presents a Hybridized Discontinuous Galerkin (HDG) formulation for solving the EHD thruster governing equations with the exception of fluid flow equations. The problem is two-way coupled and non-linear. A smoothed charge injection model from the literature for the corona discharge is included in the HDG scheme. The formulation is validated against a model problem which has an analytical solution and parallel wire single stage and dual stage thruster performance data from the literature. The model problem consists of concentric cylinders with charge density and potential specified on the inner and outer cylinders. The inner cylinder is offset to test the charge injection boundary condition in an asymmetric solution. The single stage thruster consists of two parallel wires of different diameters separated by a 1 cm gap. The dual stage thruster consists of three inline parallel wires of different diameters separated by 1 cm and 3 cm. The HDG solution for the model problem is found to produce normalized errors on the order of 10-3 for the potential and charge density solutions. The charge density applied to the inner emitter electrode is increased over several solution iterations to resolve high charge density gradients. The charge density boundary condition applied to the offset case represented the expected qualities of a corona discharge. The smoothed boundary condition is shown to be tunable to allow for a trade-off between accuracy and numerical stability. The single stage thruster model replicated experimental thrust results within 14% error using homogeneous charge injection and the smoothed charge injection model requires a less stable setting to achieve similar accuracy. The dual stage model shows the necessity of a mixed outflow boundary condition to avoid non-unique solutions.",
    "advisors": ["Steven R. H. Barrett"],
    "text": "A hybridized discontinuous Galerkin formulation for modeling electrohydrodynamic thrusters Electrohydrodynamic (EHD) thrusters utilize ion neutral collisions in air to produce a propulsive force. The ions are generated at an emitting electrode in an asymmetric capacitor by a corona discharge. This thesis presents a Hybridized Discontinuous Galerkin (HDG) formulation for solving the EHD thruster governing equations with the exception of fluid flow equations. The problem is two-way coupled and non-linear. A smoothed charge injection model from the literature for the corona discharge is included in the HDG scheme. The formulation is validated against a model problem which has an analytical solution and parallel wire single stage and dual stage thruster performance data from the literature. The model problem consists of concentric cylinders with charge density and potential specified on the inner and outer cylinders. The inner cylinder is offset to test the charge injection boundary condition in an asymmetric solution. The single stage thruster consists of two parallel wires of different diameters separated by a 1 cm gap. The dual stage thruster consists of three inline parallel wires of different diameters separated by 1 cm and 3 cm. The HDG solution for the model problem is found to produce normalized errors on the order of 10-3 for the potential and charge density solutions. The charge density applied to the inner emitter electrode is increased over several solution iterations to resolve high charge density gradients. The charge density boundary condition applied to the offset case represented the expected qualities of a corona discharge. The smoothed boundary condition is shown to be tunable to allow for a trade-off between accuracy and numerical stability. The single stage thruster model replicated experimental thrust results within 14% error using homogeneous charge injection and the smoothed charge injection model requires a less stable setting to achieve similar accuracy. The dual stage model shows the necessity of a mixed outflow boundary condition to avoid non-unique solutions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/103449",
    "title": "Analysis and uncertainty of airport pushback rate control policies",
    "abstract": "This thesis analyzes the effects of two algorithms that control the departure of aircraft at congested airports, with an emphasis on the uncertainty of the underlying processes. These algorithms, N-control and dynamic programming, belong to a broader class of control policies called Pushback Rate Control (PRC) policies that calculate a pushback rate for departing aircraft based on the state of the airport surface congestion. During times of congestion, these algorithms limit the amount of aircraft on the airport surface while maintaining departure throughput. This reduces the taxi-out time of aircraft, resulting in reduced fuel burn and emissions. This thesis introduces the policies and simulates their performance at LaGuardia Airport while varying two policy parameters, the length of the prediction interval and the number of prediction intervals, under several types of uncertainty, including the departure schedule and arrival rate. As will be shown, each policy results in significant taxi-out time reductions, saving airlines at least 60,000 minutes of taxiing over a 2-month period with the traditional 15-minute time window simulations. However, when accounting for the uncertainty in the algorithm inputs or the variation of policy parameters, the performance of both PRC policies degrades. By accounting for the variation of policy parameters and the different sources of uncertainty that affect airport surface management, the main contribution of this thesis provides a realistic analysis of PRC policies.",
    "advisors": ["Hamsa Balakrishnan"],
    "text": "Analysis and uncertainty of airport pushback rate control policies This thesis analyzes the effects of two algorithms that control the departure of aircraft at congested airports, with an emphasis on the uncertainty of the underlying processes. These algorithms, N-control and dynamic programming, belong to a broader class of control policies called Pushback Rate Control (PRC) policies that calculate a pushback rate for departing aircraft based on the state of the airport surface congestion. During times of congestion, these algorithms limit the amount of aircraft on the airport surface while maintaining departure throughput. This reduces the taxi-out time of aircraft, resulting in reduced fuel burn and emissions. This thesis introduces the policies and simulates their performance at LaGuardia Airport while varying two policy parameters, the length of the prediction interval and the number of prediction intervals, under several types of uncertainty, including the departure schedule and arrival rate. As will be shown, each policy results in significant taxi-out time reductions, saving airlines at least 60,000 minutes of taxiing over a 2-month period with the traditional 15-minute time window simulations. However, when accounting for the uncertainty in the algorithm inputs or the variation of policy parameters, the performance of both PRC policies degrades. By accounting for the variation of policy parameters and the different sources of uncertainty that affect airport surface management, the main contribution of this thesis provides a realistic analysis of PRC policies."
}, {
    "id": "oai:dspace.mit.edu:1721.1/59678",
    "title": "Accelerated Bayesian experimental design for chemical kinetic models",
    "abstract": "The optimal selection of experimental conditions is essential in maximizing the value of data for inference and prediction, particularly in situations where experiments are time-consuming and expensive to conduct. A general Bayesian framework for optimal experimental design with nonlinear simulation-based models is proposed. The formulation accounts for uncertainty in model parameters, observables, and experimental conditions. Straightforward Monte Carlo evaluation of the objective function - which reflects expected information gain (Kullback-Leibler divergence) from prior to posterior - is intractable when the likelihood is computationally intensive. Instead, polynomial chaos expansions are introduced to capture the dependence of observables on model parameters and on design conditions. Under suitable regularity conditions, these expansions converge exponentially fast. Since both the parameter space and the design space can be high-dimensional, dimension-adaptive sparse quadrature is used to construct the polynomial expansions. Stochastic optimization methods will be used in the future to maximize the expected utility. While this approach is broadly applicable, it is demonstrated on a chemical kinetic system with strong nonlinearities. In particular, the Arrhenius rate parameters in a combustion reaction mechanism are estimated from observations of autoignition. Results show multiple order-of-magnitude speedups in both experimental design and parameter inference.",
    "advisors": ["Youssef M. Marzouk"],
    "text": "Accelerated Bayesian experimental design for chemical kinetic models The optimal selection of experimental conditions is essential in maximizing the value of data for inference and prediction, particularly in situations where experiments are time-consuming and expensive to conduct. A general Bayesian framework for optimal experimental design with nonlinear simulation-based models is proposed. The formulation accounts for uncertainty in model parameters, observables, and experimental conditions. Straightforward Monte Carlo evaluation of the objective function - which reflects expected information gain (Kullback-Leibler divergence) from prior to posterior - is intractable when the likelihood is computationally intensive. Instead, polynomial chaos expansions are introduced to capture the dependence of observables on model parameters and on design conditions. Under suitable regularity conditions, these expansions converge exponentially fast. Since both the parameter space and the design space can be high-dimensional, dimension-adaptive sparse quadrature is used to construct the polynomial expansions. Stochastic optimization methods will be used in the future to maximize the expected utility. While this approach is broadly applicable, it is demonstrated on a chemical kinetic system with strong nonlinearities. In particular, the Arrhenius rate parameters in a combustion reaction mechanism are estimated from observations of autoignition. Results show multiple order-of-magnitude speedups in both experimental design and parameter inference."
}, {
    "id": "oai:dspace.mit.edu:1721.1/30280",
    "title": "Planning and scheduling proximity operations for autonomous orbital rendezvous",
    "abstract": "This thesis develops a mixed integer programming formulation to solve the proximity operations scheduling problem for autonomous orbital rendezvous. The algorithm of this thesis allows the operator to specify planned modes, which encode the chase satellite's operations. The scheduler optimally places these modes in the midst of the environmental conditions that fall out of the chase satellite's orbit parameters. The algorithm manages resources, i. e. battery state of charge, and observes temporal constraints. Experiments show that the scheduler responds to changes in a variety of situations. It accommodates changes to the constraints in the modes. Relaxing or tightening the restrictions on the resources illuminates the algorithm's responsiveness to practical resource demands. Changes to the definition of optimality via a cost function indicate that the scheduler reacts to a diverse set of parameters.",
    "advisors": ["Lance A. Page", "John J. Deyst"],
    "text": "Planning and scheduling proximity operations for autonomous orbital rendezvous This thesis develops a mixed integer programming formulation to solve the proximity operations scheduling problem for autonomous orbital rendezvous. The algorithm of this thesis allows the operator to specify planned modes, which encode the chase satellite's operations. The scheduler optimally places these modes in the midst of the environmental conditions that fall out of the chase satellite's orbit parameters. The algorithm manages resources, i. e. battery state of charge, and observes temporal constraints. Experiments show that the scheduler responds to changes in a variety of situations. It accommodates changes to the constraints in the modes. Relaxing or tightening the restrictions on the resources illuminates the algorithm's responsiveness to practical resource demands. Changes to the definition of optimality via a cost function indicate that the scheduler reacts to a diverse set of parameters."
}, {
    "id": "oai:dspace.mit.edu:1721.1/67199",
    "title": "Approaches to representing aircraft fuel efficiency performance for the purpose of a commercial aircraft certification standard",
    "abstract": "Increasing concern over the potential harmful effects of green house gas emissions from various sources has motivated the consideration of an aircraft certification standard as one way to reduce aircraft C02 emissions and mitigate aviation impacts on the climate. In order to develop a commercial aircraft certification standard, a fuel efficiency performance metric and the condition at which it is evaluated must be determined. The fuel efficiency metric form of interest to this research is fuel/range, where fuel and range can either be evaluated over the course of a reference mission or at a single, instantaneous point. A mission-based metric encompasses all phases of flight and is robust to changes in technology; however, definition of the reference mission requires many assumptions and is cumbersome for both manufacturers and regulators. An instantaneous metric based on fundamental aircraft parameters measures the fuel efficiency performance of the aircraft at a single point, greatly reducing the complexity of the standard and certification process; however, a single point might not be robust to future changes in aircraft technology. In this thesis, typical aircraft operations are assessed in order to develop evaluation assumptions for a mission-based metric, Block Fuel divided by Range (BF/R), and an instantaneous metric, incremental fuel burn per incremental distance (inverse Specific Air Range (1/SAR)). Operating patterns and fuel burn maps are used to demonstrate the importance of mission range on fleet fuel burn, and thus the importance of a properly defined range evaluation condition for BF/R. An evaluation condition of 40% of the range at Maximum Structural Payload (MSP) limited by Maximum Takeoff Weight (MTOW) is determined to be representative for the mission-based metric. A potential evaluation condition for 1/SAR is determined to be optimal speed and altitude for a representative mid-cruise weight defined by half of the difference between MTOW and Maximum Zero Fuel Weight (MZFW). To demonstrate suitability as a potential surrogate for BF/R, correlation of 1/SAR with BF/R is shown for the current fleet, and a case study of potential future aircraft technologies is presented to show the correlation of improvements in the 1/SAR metric with improvements in BF/R.",
    "advisors": ["R. John Hansman"],
    "text": "Approaches to representing aircraft fuel efficiency performance for the purpose of a commercial aircraft certification standard Increasing concern over the potential harmful effects of green house gas emissions from various sources has motivated the consideration of an aircraft certification standard as one way to reduce aircraft C02 emissions and mitigate aviation impacts on the climate. In order to develop a commercial aircraft certification standard, a fuel efficiency performance metric and the condition at which it is evaluated must be determined. The fuel efficiency metric form of interest to this research is fuel/range, where fuel and range can either be evaluated over the course of a reference mission or at a single, instantaneous point. A mission-based metric encompasses all phases of flight and is robust to changes in technology; however, definition of the reference mission requires many assumptions and is cumbersome for both manufacturers and regulators. An instantaneous metric based on fundamental aircraft parameters measures the fuel efficiency performance of the aircraft at a single point, greatly reducing the complexity of the standard and certification process; however, a single point might not be robust to future changes in aircraft technology. In this thesis, typical aircraft operations are assessed in order to develop evaluation assumptions for a mission-based metric, Block Fuel divided by Range (BF/R), and an instantaneous metric, incremental fuel burn per incremental distance (inverse Specific Air Range (1/SAR)). Operating patterns and fuel burn maps are used to demonstrate the importance of mission range on fleet fuel burn, and thus the importance of a properly defined range evaluation condition for BF/R. An evaluation condition of 40% of the range at Maximum Structural Payload (MSP) limited by Maximum Takeoff Weight (MTOW) is determined to be representative for the mission-based metric. A potential evaluation condition for 1/SAR is determined to be optimal speed and altitude for a representative mid-cruise weight defined by half of the difference between MTOW and Maximum Zero Fuel Weight (MZFW). To demonstrate suitability as a potential surrogate for BF/R, correlation of 1/SAR with BF/R is shown for the current fleet, and a case study of potential future aircraft technologies is presented to show the correlation of improvements in the 1/SAR metric with improvements in BF/R."
}, {
    "id": "oai:dspace.mit.edu:1721.1/17539",
    "title": "Aerodynamic study of a small, ducted VTOL aerial vehicle",
    "abstract": "The Perching Unmanned Aerial Vehicle (PUAV) is a 9-inch diameter ducted vertical takeoff and landing reconnaissance vehicle with the capability of fast-forward cruise flight. Currently in the development stage, the program is envisaged to yield a man-portable craft that a foot soldier can use to provide over-the-hill observation. Several prototypes have been constructed and tested, with mixed results. Concerns regarding duct aerodynamics led to the proposal for further aerodynamic study to investigate effects of inlet lip radius and surface area, diffuser area ratio, blade tip clearance and rotor position on thrust, power and efficiency. This report covers the theory of rotorcraft and ducted propeller aerodynamics, and outlines the tests performed and results obtained. It also presents specifications of the test vehicle and methods that can be used in future ducted aircraft studies. Large angle diffusers tested showed reduced thrust and efficiency and increased power compared to smaller diffusers, contrary to theory. Reverse flow within the core appears to disrupt uniform exit flow and yields a conically divergent turbulent wake. Results of this study will be used in the redesign of a duct core fairing, which will act to control the airflow and reduce the tendency for reverse flow at the center where blade thrust is absent. Future studies will also consider twisted, cambered and tapered rotor blades in an effort to better address spanwise thrust distribution and optimized airflow. The test apparatus and methods developed for this report, in addition to results of initial testing, will be instrumental to further development of small ducted UAVs. Findings and methods are not limited to exact duplicates of PUAV-like aircraft, but can be used in a wide range of applications including lift and thrust-producing ducts.",
    "advisors": ["Sean George"],
    "text": "Aerodynamic study of a small, ducted VTOL aerial vehicle The Perching Unmanned Aerial Vehicle (PUAV) is a 9-inch diameter ducted vertical takeoff and landing reconnaissance vehicle with the capability of fast-forward cruise flight. Currently in the development stage, the program is envisaged to yield a man-portable craft that a foot soldier can use to provide over-the-hill observation. Several prototypes have been constructed and tested, with mixed results. Concerns regarding duct aerodynamics led to the proposal for further aerodynamic study to investigate effects of inlet lip radius and surface area, diffuser area ratio, blade tip clearance and rotor position on thrust, power and efficiency. This report covers the theory of rotorcraft and ducted propeller aerodynamics, and outlines the tests performed and results obtained. It also presents specifications of the test vehicle and methods that can be used in future ducted aircraft studies. Large angle diffusers tested showed reduced thrust and efficiency and increased power compared to smaller diffusers, contrary to theory. Reverse flow within the core appears to disrupt uniform exit flow and yields a conically divergent turbulent wake. Results of this study will be used in the redesign of a duct core fairing, which will act to control the airflow and reduce the tendency for reverse flow at the center where blade thrust is absent. Future studies will also consider twisted, cambered and tapered rotor blades in an effort to better address spanwise thrust distribution and optimized airflow. The test apparatus and methods developed for this report, in addition to results of initial testing, will be instrumental to further development of small ducted UAVs. Findings and methods are not limited to exact duplicates of PUAV-like aircraft, but can be used in a wide range of applications including lift and thrust-producing ducts."
}, {
    "id": "oai:dspace.mit.edu:1721.1/50272",
    "title": "Mechanical design for the tactile exploration of constrained internal geometries",
    "abstract": "Rising world oil prices and advanced oil recovery techniques have made it economically attractive to rehabilitate abandoned oil wells. This requires guiding tools through well junctions where divergent branches leave the main wellbore. The unknown locations and shapes of these junctions must be determined. Harsh down-well conditions prevent the use of ranged sensors. However, robotic tactile exploration using a manipulator is well suited to this problem. This tactile characterization must be done quickly because of the high costs of working on oil wells. Consequently, intelligent tactile exploration algorithms that can characterize a shape using sparse data sets must be developed. This thesis explores the design and system architecture of robotic manipulators for down-well tactile exploration. A design approach minimizing sensing is adopted to produce a system that is mechanically robust and suited to the harsh down-well environment. A feasibility study on down-well tactile exploration manipulators is conducted. This study focuses on the mature robotic technology of link and joint manipulators with zero or low kinematic redundancy. This study produces a field system architecture that specifies a unified combination of control, sensing, kinematic solutions for down-well applications. An experimental system is built to demonstrate the proposed field system architecture and test control and intelligent tactile exploration algorithms. Experimental results to date have indicated acceptability of the proposed field system architecture and have demonstrated the ability to characterize geometry with sparse tactile data.",
    "advisors": ["Steven Dubowsky"],
    "text": "Mechanical design for the tactile exploration of constrained internal geometries Rising world oil prices and advanced oil recovery techniques have made it economically attractive to rehabilitate abandoned oil wells. This requires guiding tools through well junctions where divergent branches leave the main wellbore. The unknown locations and shapes of these junctions must be determined. Harsh down-well conditions prevent the use of ranged sensors. However, robotic tactile exploration using a manipulator is well suited to this problem. This tactile characterization must be done quickly because of the high costs of working on oil wells. Consequently, intelligent tactile exploration algorithms that can characterize a shape using sparse data sets must be developed. This thesis explores the design and system architecture of robotic manipulators for down-well tactile exploration. A design approach minimizing sensing is adopted to produce a system that is mechanically robust and suited to the harsh down-well environment. A feasibility study on down-well tactile exploration manipulators is conducted. This study focuses on the mature robotic technology of link and joint manipulators with zero or low kinematic redundancy. This study produces a field system architecture that specifies a unified combination of control, sensing, kinematic solutions for down-well applications. An experimental system is built to demonstrate the proposed field system architecture and test control and intelligent tactile exploration algorithms. Experimental results to date have indicated acceptability of the proposed field system architecture and have demonstrated the ability to characterize geometry with sparse tactile data."
}, {
    "id": "oai:dspace.mit.edu:1721.1/97265",
    "title": "Evaluation of propulsor aerodynamic performance for powered aircraft wind tunnel experiments",
    "abstract": "This thesis describes a methodology to convert electrical power measurements to propulsor mechanical ow power for a 1:11-scale, powered wind tunnel model of an advanced civil aircraft utilizing boundary layer ingestion (BLI); mechanical ow power is a surrogate for aircraft fuel burn. Back-to-back experiments of BLI and non-BLI aircraft configurations to assess the BLI benet directly measured electrical power, and supporting experiments were performed in a 1x1 foot wind tunnel at the MIT Gas Turbine Laboratory to convert these measurements into mechanical flow power. The incoming flow conditions of the powered wind tunnel tests (Reynolds number and inlet distortion) were replicated. This propulsor characterization was found to convert the electrical power measurements to mechanical ow power with experimental uncertainty of roughly 1.6%.",
    "advisors": ["Edward M. Greitzer", "Alejandra Uranga"],
    "text": "Evaluation of propulsor aerodynamic performance for powered aircraft wind tunnel experiments This thesis describes a methodology to convert electrical power measurements to propulsor mechanical ow power for a 1:11-scale, powered wind tunnel model of an advanced civil aircraft utilizing boundary layer ingestion (BLI); mechanical ow power is a surrogate for aircraft fuel burn. Back-to-back experiments of BLI and non-BLI aircraft configurations to assess the BLI benet directly measured electrical power, and supporting experiments were performed in a 1x1 foot wind tunnel at the MIT Gas Turbine Laboratory to convert these measurements into mechanical flow power. The incoming flow conditions of the powered wind tunnel tests (Reynolds number and inlet distortion) were replicated. This propulsor characterization was found to convert the electrical power measurements to mechanical ow power with experimental uncertainty of roughly 1.6%."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62877",
    "title": "Improvement of terminal area capacity in the New York airspace",
    "abstract": "The New York airspace is the most congested in the U.S. air transportation network. Increasing capacity in this area is critical to ensure the balanced growth of traffic across the U.S. This study compares the total measured runway capacity at the New York airports with the achieved throughput of the New York airspace. The comparison is performed for six airspace configurations representing operations under different wind conditions, visibility and relative arrival and departure demand. The comparison shows that in all cases the capacity of the system of airports is lower than the total capacity of the airports considered individually by approximately 20%. This finding suggests that air traffic throughput in the New York area is constrained by shared airspace resources. If these constraints could be removed, these funding suggest that capacity could be increased approximately 20% without any airport infrastructure or procedure changes. An examination of procedures close to the airports is performed to identify fixed constraints. The impact of these constraints is not captured by the empirical analysis because these constraints are always present. This analysis identifies cases where new navigation technologies could be used to reduce the interactions between airports. The greatest potential for improvement is found to be in the lower performing configurations. Therefore procedural changes close to the airports may provide more benefit in reducing the variability of capacity between different configurations, rather than providing large increases in maximum capacity.",
    "advisors": ["R. John Hansman"],
    "text": "Improvement of terminal area capacity in the New York airspace The New York airspace is the most congested in the U.S. air transportation network. Increasing capacity in this area is critical to ensure the balanced growth of traffic across the U.S. This study compares the total measured runway capacity at the New York airports with the achieved throughput of the New York airspace. The comparison is performed for six airspace configurations representing operations under different wind conditions, visibility and relative arrival and departure demand. The comparison shows that in all cases the capacity of the system of airports is lower than the total capacity of the airports considered individually by approximately 20%. This finding suggests that air traffic throughput in the New York area is constrained by shared airspace resources. If these constraints could be removed, these funding suggest that capacity could be increased approximately 20% without any airport infrastructure or procedure changes. An examination of procedures close to the airports is performed to identify fixed constraints. The impact of these constraints is not captured by the empirical analysis because these constraints are always present. This analysis identifies cases where new navigation technologies could be used to reduce the interactions between airports. The greatest potential for improvement is found to be in the lower performing configurations. Therefore procedural changes close to the airports may provide more benefit in reducing the variability of capacity between different configurations, rather than providing large increases in maximum capacity."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45219",
    "title": "Closed-loop control of spacecraft formations with applications on SPHERES",
    "abstract": "Formation flying satellites offer potentially greater science returns and operational capabilities than attainable with a monolithic spacecraft. Successful control of a formation of spacecraft can be divided into two separate stages. The first stage creates a plan that meets a set of mission objectives, and the second stage implements the plan. Plans are specified as a sequence of [delta]V commands executed at specific times during an orbit. This thesis presents an online method for generating fleet-wide plans, using convex optimization techniques, that satisfy multiple objectives. The approach allows for minimum and balanced fuel usage, can position spacecraft in arbitrary configurations, and favors low-maintenance orbits that do not drift apart. Additionally, the architecture is applicable not only to formation-keeping maneuvers, but also to formation reconfigurations. Various simulations demonstrate the importance of accurately implementing plans for formation flying as well as autonomous rendezvous and docking missions. Specifically, the relationships between process error, overall fuel use, and position error are studied. Theory is put into practice with the development of a new low-level, closed-loop thrust controller for the Synchronized Position Hold Engage and Reorient Experimental Satellites (SPHERES). The controller processes measurements from accelerometers and gyroscopes to monitor thruster performance in real-time. Experiments conducted on the International Space Station (ISS) validate the controller and establish a foundation for future enhancements to the underlying algorithm. Finally, data from a series of high-fidelity formation flying simulations is presented that confirms the analysis done elsewhere in the thesis.",
    "advisors": ["Jonathan P. How"],
    "text": "Closed-loop control of spacecraft formations with applications on SPHERES Formation flying satellites offer potentially greater science returns and operational capabilities than attainable with a monolithic spacecraft. Successful control of a formation of spacecraft can be divided into two separate stages. The first stage creates a plan that meets a set of mission objectives, and the second stage implements the plan. Plans are specified as a sequence of [delta]V commands executed at specific times during an orbit. This thesis presents an online method for generating fleet-wide plans, using convex optimization techniques, that satisfy multiple objectives. The approach allows for minimum and balanced fuel usage, can position spacecraft in arbitrary configurations, and favors low-maintenance orbits that do not drift apart. Additionally, the architecture is applicable not only to formation-keeping maneuvers, but also to formation reconfigurations. Various simulations demonstrate the importance of accurately implementing plans for formation flying as well as autonomous rendezvous and docking missions. Specifically, the relationships between process error, overall fuel use, and position error are studied. Theory is put into practice with the development of a new low-level, closed-loop thrust controller for the Synchronized Position Hold Engage and Reorient Experimental Satellites (SPHERES). The controller processes measurements from accelerometers and gyroscopes to monitor thruster performance in real-time. Experiments conducted on the International Space Station (ISS) validate the controller and establish a foundation for future enhancements to the underlying algorithm. Finally, data from a series of high-fidelity formation flying simulations is presented that confirms the analysis done elsewhere in the thesis."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62487",
    "title": "Improved return passages for multistage centrifugal compressors",
    "abstract": "This thesis presents a design concept for return passages in multistage centrifugal compressors. Flow in a baseline return passage is analyzed to identify loss sources that have substantial potential for reduction. For the geometry analyzed, it is found that mismatched incidence of the return vane, separation on the hub surface at the exit of the return bend, and blockage due to separation on the shroud surface near the return bend inlet lead to losses which have such potential. Preliminary designs were developed and computationally assessed to determine effective geometries for eliminating separation on the hub at the return bend exit and for reducing losses. Then, based on assessment of the preliminary designs, other features of the loss generation were addressed. The geometry proposed includes an increased axial extent of the return bend, an increasing radius of curvature through the return bend, and lean of the return vane leading edge, mitigating the loss mechanisms identified in the baseline return passage. The three-dimensional calculations showed a cumulative loss coefficient that was 10% lower than the baseline. The design described was carried out with a fixed inlet condition, and a second return passage was thus developed to determine the potential loss reduction if the inlet geometry were modified. (The design of the impeller required to achieve the modified inlet flow was not considered.) The cumulative loss coefficient was reduced by 27% compared with the baseline, with the implication that an area for exploration is integration of the impeller and return passage flow fields.",
    "advisors": ["Edward M. Greitzer"],
    "text": "Improved return passages for multistage centrifugal compressors This thesis presents a design concept for return passages in multistage centrifugal compressors. Flow in a baseline return passage is analyzed to identify loss sources that have substantial potential for reduction. For the geometry analyzed, it is found that mismatched incidence of the return vane, separation on the hub surface at the exit of the return bend, and blockage due to separation on the shroud surface near the return bend inlet lead to losses which have such potential. Preliminary designs were developed and computationally assessed to determine effective geometries for eliminating separation on the hub at the return bend exit and for reducing losses. Then, based on assessment of the preliminary designs, other features of the loss generation were addressed. The geometry proposed includes an increased axial extent of the return bend, an increasing radius of curvature through the return bend, and lean of the return vane leading edge, mitigating the loss mechanisms identified in the baseline return passage. The three-dimensional calculations showed a cumulative loss coefficient that was 10% lower than the baseline. The design described was carried out with a fixed inlet condition, and a second return passage was thus developed to determine the potential loss reduction if the inlet geometry were modified. (The design of the impeller required to achieve the modified inlet flow was not considered.) The cumulative loss coefficient was reduced by 27% compared with the baseline, with the implication that an area for exploration is integration of the impeller and return passage flow fields."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37949",
    "title": "Engine placement for manned descent at Mars considering single engine failures",
    "abstract": "Previous missions to Mars have landed masses of approximately I metric ton on the surface. Vehicles large enough to support humans on the flight to Mars and land them safely on the surface are closer to 100 metric tons, a two order of magnitude increase. This large mass causes many changes in the EDL of a manned vehicle compared to proven unmanned landers. One critical change is the potential choice of a propulsive descent to replace parachute systems that do not scale to large masses. The placement of these engines on a lander is subject to many concerns such as heat shield packing, cargo handling, and engine out mitigation. Engine out mitigation is of considerable interest because configurations that improve failure mitigation tend to be poorer for the other considerations. This thesis presents the development of a simulation of the descent phase of a manned landing at Mars, an overview of the effects of the various requirements on manned lander engine configuration and the results of a 6 DOF analysis of engine failure scenarios.",
    "advisors": ["David Miller"],
    "text": "Engine placement for manned descent at Mars considering single engine failures Previous missions to Mars have landed masses of approximately I metric ton on the surface. Vehicles large enough to support humans on the flight to Mars and land them safely on the surface are closer to 100 metric tons, a two order of magnitude increase. This large mass causes many changes in the EDL of a manned vehicle compared to proven unmanned landers. One critical change is the potential choice of a propulsive descent to replace parachute systems that do not scale to large masses. The placement of these engines on a lander is subject to many concerns such as heat shield packing, cargo handling, and engine out mitigation. Engine out mitigation is of considerable interest because configurations that improve failure mitigation tend to be poorer for the other considerations. This thesis presents the development of a simulation of the descent phase of a manned landing at Mars, an overview of the effects of the various requirements on manned lander engine configuration and the results of a 6 DOF analysis of engine failure scenarios."
}, {
    "id": "oai:dspace.mit.edu:1721.1/51634",
    "title": "Emission spectroscopy for the study of electric propulsion plasmas",
    "abstract": "Typical electric propulsion devices rely on the acceleration of highly ionized plasmas to produce thrust at specific impulses unattainable with state-of-the-art chemical systems. This thesis examines the use of a miniaturized Helicon plasma source for an open-ended, electrode-less, cathode-less thruster through emission spectroscopy. The use of non-invasive diagnostics allows the measurement of important plasma parameters near the ionization region, where the plasma densities and temperatures are prohibitively high for typical electrostatic probes, while avoiding the inherent perturbations caused by invasive techniques. A spectral study of the Helicon antenna region, yielding axially resolved information on the electron temperature and degree of ionization, is discussed. A similar study in the near-field plume is presented, along with Doppler shift measurements, which clearly demonstrate continued acceleration upstream of the thruster exit. The Doppler shift measurements are validated by extending the study to a Hall effect thruster plasma, well characterized in the literature. Ion flux estimates from the downstream portion of the spectroscopic survey are compared with Faraday probe measurements. Possible mechanisms for thrust are presented along with their implications on Helicon thruster design.",
    "advisors": ["Oleg Batishchev"],
    "text": "Emission spectroscopy for the study of electric propulsion plasmas Typical electric propulsion devices rely on the acceleration of highly ionized plasmas to produce thrust at specific impulses unattainable with state-of-the-art chemical systems. This thesis examines the use of a miniaturized Helicon plasma source for an open-ended, electrode-less, cathode-less thruster through emission spectroscopy. The use of non-invasive diagnostics allows the measurement of important plasma parameters near the ionization region, where the plasma densities and temperatures are prohibitively high for typical electrostatic probes, while avoiding the inherent perturbations caused by invasive techniques. A spectral study of the Helicon antenna region, yielding axially resolved information on the electron temperature and degree of ionization, is discussed. A similar study in the near-field plume is presented, along with Doppler shift measurements, which clearly demonstrate continued acceleration upstream of the thruster exit. The Doppler shift measurements are validated by extending the study to a Hall effect thruster plasma, well characterized in the literature. Ion flux estimates from the downstream portion of the spectroscopic survey are compared with Faraday probe measurements. Possible mechanisms for thrust are presented along with their implications on Helicon thruster design."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34633",
    "title": "Multi-stakeholder quantitative analysis of sustainability for value delivery systems",
    "abstract": "This thesis presents a model to analyze multi-stakeholder decision-making and its application to Space Exploration strategy. The analysis of decision-making for Space Exploration is especially difficult because of the complexity of the value delivery process and the extended time frame to deliver value. In order to analyze the sustainability of Space Exploration, we use the hypothesis that only stakeholder groups that should be considered are those that control resources needed for the survival and growth of the initiative. Consequently, the key to sustainability lays in a tiered multi-attribute decision- making process, where the top layer is populated by the needs of Space Exploration as a Value Creating System (VCS), its second layer is the stakeholders who control the resources that satisfy those needs, and its third layer is the stakeholders' needs. Our model tries to measure the ability of different architectures to increase stakeholder needs satisfaction, thus increasing the likelihood that those stakeholders will provide resources back to the VCS, which is the key to the VCS's survival. The model uses a numerical extension of the Kano model of quality to weight the criticality of the needs. The feedback loop of value to and from the VCS is modeled as a flow of vectorial elements.",
    "advisors": ["Jeffrey A. Hoffman"],
    "text": "Multi-stakeholder quantitative analysis of sustainability for value delivery systems This thesis presents a model to analyze multi-stakeholder decision-making and its application to Space Exploration strategy. The analysis of decision-making for Space Exploration is especially difficult because of the complexity of the value delivery process and the extended time frame to deliver value. In order to analyze the sustainability of Space Exploration, we use the hypothesis that only stakeholder groups that should be considered are those that control resources needed for the survival and growth of the initiative. Consequently, the key to sustainability lays in a tiered multi-attribute decision- making process, where the top layer is populated by the needs of Space Exploration as a Value Creating System (VCS), its second layer is the stakeholders who control the resources that satisfy those needs, and its third layer is the stakeholders' needs. Our model tries to measure the ability of different architectures to increase stakeholder needs satisfaction, thus increasing the likelihood that those stakeholders will provide resources back to the VCS, which is the key to the VCS's survival. The model uses a numerical extension of the Kano model of quality to weight the criticality of the needs. The feedback loop of value to and from the VCS is modeled as a flow of vectorial elements."
}, {
    "id": "oai:dspace.mit.edu:1721.1/82504",
    "title": "Characterization of turbine rim seal flow and its sealing effectiveness",
    "abstract": "In a gas turbine engine, ingestion of hot gas from the flowpath into the gaps between the turbine rotor and stator can lead to elevated metal temperatures and a deterioration of component life. To prevent ingestion, bleed air from the compressor is used to \"purge\" the rim seal cavities. Establishing a quantitative understanding of the wheelspace and rim cavity flow processes driving ingestion is critical to optimizing seal design and minimizing the associated performance penalty. A computational model of the wheelspace that does not limit the spatial or temporal scales of flow processes is formulated. This allows the assessment of the response of the wheelspace to external stimuli set up by the turbine main flow path, and the development of causal links between flow processes and their drivers. Varying the axisymmetric turbine flowpath pressure on a quasi-steady basis when the purge flow supply seal is choked has no impact on ingestion; the pressure field in the wheelspace merely scales with the flowpath pressure, leaving the flow structure unchanged. Introducing circumferential variation in the external pressure field can, however, lead to ingestion with the ratio of disturbance wavelength to the trench depth emerging as a key parameter. Varying rotational speed alone does not drive ingestion as a stagnation point is formed on the outer shroud that is ingestion resistant. It is shown that excitation at frequencies corresponding to the natural modes of the wheelspace system can lead to large responses in pressure and seal flow rate, with the seal reduced frequency appearing as a characterizing parameter. The existence and parametric dependence of these modes is further assessed through a small disturbance flow analysis. A generalized small disturbance flow analysis is formulated that provides a direct enumeration of the key characterizing parameters.",
    "advisors": ["Choon S. Tan"],
    "text": "Characterization of turbine rim seal flow and its sealing effectiveness In a gas turbine engine, ingestion of hot gas from the flowpath into the gaps between the turbine rotor and stator can lead to elevated metal temperatures and a deterioration of component life. To prevent ingestion, bleed air from the compressor is used to \"purge\" the rim seal cavities. Establishing a quantitative understanding of the wheelspace and rim cavity flow processes driving ingestion is critical to optimizing seal design and minimizing the associated performance penalty. A computational model of the wheelspace that does not limit the spatial or temporal scales of flow processes is formulated. This allows the assessment of the response of the wheelspace to external stimuli set up by the turbine main flow path, and the development of causal links between flow processes and their drivers. Varying the axisymmetric turbine flowpath pressure on a quasi-steady basis when the purge flow supply seal is choked has no impact on ingestion; the pressure field in the wheelspace merely scales with the flowpath pressure, leaving the flow structure unchanged. Introducing circumferential variation in the external pressure field can, however, lead to ingestion with the ratio of disturbance wavelength to the trench depth emerging as a key parameter. Varying rotational speed alone does not drive ingestion as a stagnation point is formed on the outer shroud that is ingestion resistant. It is shown that excitation at frequencies corresponding to the natural modes of the wheelspace system can lead to large responses in pressure and seal flow rate, with the seal reduced frequency appearing as a characterizing parameter. The existence and parametric dependence of these modes is further assessed through a small disturbance flow analysis. A generalized small disturbance flow analysis is formulated that provides a direct enumeration of the key characterizing parameters."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39704",
    "title": "Design and implementation of a relative state estimator for docking and formation control of modular autonomous spacecraft",
    "abstract": "Modularity is a promising design concept for space systems. In a modular satellite, the individual subsystems would be broken down into physically distinct modules, which would then dynamically recombine into an aggregate vehicle. This could improve the flexibility and reusability of satellites, and could even enable some mission objectives which are not possible at all with monolithic vehicles. However, modularity requires that some additional new elements be included in the design that are not needed with a monolithic satellite. Two of these are a docking interface to allow modules to attach, and a position measurement system to allow modules to fly accurately in formation and dock with each other. These two additional elements are explored in this thesis. The central focus is on a relative state estimator based on an extended Kalman filter. The estimator is first presented theoretically, then the results of implementation and hardware testing are discussed. This thesis presents two main hardware applications for the estimator, both of which mirror prime space-based applications of modularity itself: docking and formation maintenance/reconfiguration.",
    "advisors": ["David W. Miller"],
    "text": "Design and implementation of a relative state estimator for docking and formation control of modular autonomous spacecraft Modularity is a promising design concept for space systems. In a modular satellite, the individual subsystems would be broken down into physically distinct modules, which would then dynamically recombine into an aggregate vehicle. This could improve the flexibility and reusability of satellites, and could even enable some mission objectives which are not possible at all with monolithic vehicles. However, modularity requires that some additional new elements be included in the design that are not needed with a monolithic satellite. Two of these are a docking interface to allow modules to attach, and a position measurement system to allow modules to fly accurately in formation and dock with each other. These two additional elements are explored in this thesis. The central focus is on a relative state estimator based on an extended Kalman filter. The estimator is first presented theoretically, then the results of implementation and hardware testing are discussed. This thesis presents two main hardware applications for the estimator, both of which mirror prime space-based applications of modularity itself: docking and formation maintenance/reconfiguration."
}, {
    "id": "oai:dspace.mit.edu:1721.1/115595",
    "title": "Conceptual engineering design and optimization methodologies using geometric programming",
    "abstract": "Geometric programs (GPs) and other forms of convex optimization have recently experienced a resurgence due to the advent of polynomial-time solution algorithms and improvements in computing. Observing the need for fast and stable methods for multidisciplinary design optimization (MDO), previous work has shown that geometric programming can be a powerful framework for MDO by leveraging the mathematical guarantees and speed of convex optimization. However, there are barriers to the implementation of optimization in design. In this work, we formalize how the formulation of non-linear design problems as GPs facilitates design process. Using the principles of pressure and boundedness, we demonstrate the intuitive transformation of physics- and data-based engineering relations into GP-compatible constraints by systematically formulating an aircraft design model. We motivate the difference-of-convex GP extension called signomial programs (SPs) in order to extend the scope and fidelity of the model. We detail the features specific to GPkit, an object-oriented GP formulation framework, which facilitate the modern engineering design process. Using both performance and mission modeling paradigms, we demonstrate the ability to model and design increasingly complex systems in GP, and extract maximal engineering intuition using sensitivities and tradespace exploration methods. Though the methods are applied to an aircraft design problem, they are general to models with continuous, explicit constraints, and lower the barriers to implementing optimization in design.",
    "advisors": ["Mark Drela"],
    "text": "Conceptual engineering design and optimization methodologies using geometric programming Geometric programs (GPs) and other forms of convex optimization have recently experienced a resurgence due to the advent of polynomial-time solution algorithms and improvements in computing. Observing the need for fast and stable methods for multidisciplinary design optimization (MDO), previous work has shown that geometric programming can be a powerful framework for MDO by leveraging the mathematical guarantees and speed of convex optimization. However, there are barriers to the implementation of optimization in design. In this work, we formalize how the formulation of non-linear design problems as GPs facilitates design process. Using the principles of pressure and boundedness, we demonstrate the intuitive transformation of physics- and data-based engineering relations into GP-compatible constraints by systematically formulating an aircraft design model. We motivate the difference-of-convex GP extension called signomial programs (SPs) in order to extend the scope and fidelity of the model. We detail the features specific to GPkit, an object-oriented GP formulation framework, which facilitate the modern engineering design process. Using both performance and mission modeling paradigms, we demonstrate the ability to model and design increasingly complex systems in GP, and extract maximal engineering intuition using sensitivities and tradespace exploration methods. Though the methods are applied to an aircraft design problem, they are general to models with continuous, explicit constraints, and lower the barriers to implementing optimization in design."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35584",
    "title": "A physics-based emissions model for aircraft gas turbine combustors",
    "abstract": "In this thesis, a physics-based model of an aircraft gas turbine combustor is developed for predicting NO. and CO emissions. The objective of the model is to predict the emissions of current and potential future gas turbine engines within quantified uncertainty bounds for the purpose of assessing design tradeoffs and interdependencies in a policy-making setting. The approach taken is to capture the physical relationships among operating conditions, combustor design parameters, and pollutant emissions. The model is developed using only high-level combustor design parameters and ideal reactors. The predictive capability of the model is assessed by comparing model estimates of NO, and CO emissions from five different industry combustors to certification data. The model developed in this work correctly captures the physical relationships between engine operating conditions, combustor design parameters, and NO. and CO emissions. The NO. estimates are as good as, or better than, the NO. estimates from an established empirical model; and the CO estimates are within the uncertainty in the certification data at most of the important low power operating conditions.",
    "advisors": ["Karen Willcox", "Ian Waitz"],
    "text": "A physics-based emissions model for aircraft gas turbine combustors In this thesis, a physics-based model of an aircraft gas turbine combustor is developed for predicting NO. and CO emissions. The objective of the model is to predict the emissions of current and potential future gas turbine engines within quantified uncertainty bounds for the purpose of assessing design tradeoffs and interdependencies in a policy-making setting. The approach taken is to capture the physical relationships among operating conditions, combustor design parameters, and pollutant emissions. The model is developed using only high-level combustor design parameters and ideal reactors. The predictive capability of the model is assessed by comparing model estimates of NO, and CO emissions from five different industry combustors to certification data. The model developed in this work correctly captures the physical relationships between engine operating conditions, combustor design parameters, and NO. and CO emissions. The NO. estimates are as good as, or better than, the NO. estimates from an established empirical model; and the CO estimates are within the uncertainty in the certification data at most of the important low power operating conditions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/82503",
    "title": "An evaluation of short innovation contest implementation in the federal context",
    "abstract": "Technologies over the past three decades have democratized the tools of knowledge creation, thus creating increasing communities of innovators outside traditional organizations' boundaries. Cost effective ways of leveraging these innovative crowds can be imperative to creating and maintaining value. One specific way for organizations to tap into this diverse audience is through the use of short innovation contests. The purpose of this thesis is to better understand the application of this strategy as a tool for technology procurement in the context of government. Through a contest, participants approach a well-defined challenge independently, conducting different experiments to find a solution. The combination of these various \"trials\" leads to an increased probability for a government agency to find one particularly good, extreme-value solution. Contests can also allow government agencies to engage individuals who are normally unable or unwilling to compete in traditional government contracts, thus alleviating certain imperfections in the competitive market of government contracting. This open design strategy for procurement is fundamentally different than traditional procurement methods. For this reason, there is inherent uncertainty in the organizational implications contest implementation will have in government agencies which has made program managers hesitant to employ this strategy in their programs. This thesis sheds light on the cost structure, program management implications, and policy considerations for short innovation contests. An empirical analysis is presented for four short innovation contests used for technology procurement in different government agencies. For each case study, the cost of the contest was compared to traditional procurement and key program management considerations were identified. Additionally, recent policy initiatives passed for prize-based contests were analyzed for their applicability to short innovation contests. It was found that three of the four contests procured technology solutions for estimated costs of less than half that of traditional procurement methods. It was also found that recent contest policy initiatives were unsuitable for short innovation contests. This thesis provides recommendations for policies appropriate for short innovation contests and considerations that must be made to expand the use of this strategy as a tool for technology procurement in government agencies.",
    "advisors": ["Eric von Hippel"],
    "text": "An evaluation of short innovation contest implementation in the federal context Technologies over the past three decades have democratized the tools of knowledge creation, thus creating increasing communities of innovators outside traditional organizations' boundaries. Cost effective ways of leveraging these innovative crowds can be imperative to creating and maintaining value. One specific way for organizations to tap into this diverse audience is through the use of short innovation contests. The purpose of this thesis is to better understand the application of this strategy as a tool for technology procurement in the context of government. Through a contest, participants approach a well-defined challenge independently, conducting different experiments to find a solution. The combination of these various \"trials\" leads to an increased probability for a government agency to find one particularly good, extreme-value solution. Contests can also allow government agencies to engage individuals who are normally unable or unwilling to compete in traditional government contracts, thus alleviating certain imperfections in the competitive market of government contracting. This open design strategy for procurement is fundamentally different than traditional procurement methods. For this reason, there is inherent uncertainty in the organizational implications contest implementation will have in government agencies which has made program managers hesitant to employ this strategy in their programs. This thesis sheds light on the cost structure, program management implications, and policy considerations for short innovation contests. An empirical analysis is presented for four short innovation contests used for technology procurement in different government agencies. For each case study, the cost of the contest was compared to traditional procurement and key program management considerations were identified. Additionally, recent policy initiatives passed for prize-based contests were analyzed for their applicability to short innovation contests. It was found that three of the four contests procured technology solutions for estimated costs of less than half that of traditional procurement methods. It was also found that recent contest policy initiatives were unsuitable for short innovation contests. This thesis provides recommendations for policies appropriate for short innovation contests and considerations that must be made to expand the use of this strategy as a tool for technology procurement in government agencies."
}, {
    "id": "oai:dspace.mit.edu:1721.1/40895",
    "title": "Improving commonality implementation in the cockpits of commercial aircraft",
    "abstract": "Product development is a major source of competitive advantage in the commercial aircraft business. Judiciously implementing commonality across a range of products yields important benefits in this area. Thus, measuring the quality of commonality implementation is extremely beneficial for aircraft manufacturers. This thesis analyses the concept of commonality and divides it into three constructs that can help understand all of its aspects: standardization, reusability and modularity. This work then presents a set of metrics measuring each of these aspects, from the point of view of the manufacturer and of the customer. The appropriateness of this set of metrics is then tested in a case study analyzing the efficiency of commonality implementation in the cockpit of two well-known commercial aircraft families: the Airbus A320 family and the Boeing 737 family. This thesis further describes what additional analysis should be performed to validate the set of metrics for broader applications. After documenting the efficiency of the set of metrics, this thesis analyses the current practices of commonality management in commercial aviation. It finally explores some of the limitations of the concept of commonality and sketches solutions to overcome them.",
    "advisors": ["Warren P. Seering"],
    "text": "Improving commonality implementation in the cockpits of commercial aircraft Product development is a major source of competitive advantage in the commercial aircraft business. Judiciously implementing commonality across a range of products yields important benefits in this area. Thus, measuring the quality of commonality implementation is extremely beneficial for aircraft manufacturers. This thesis analyses the concept of commonality and divides it into three constructs that can help understand all of its aspects: standardization, reusability and modularity. This work then presents a set of metrics measuring each of these aspects, from the point of view of the manufacturer and of the customer. The appropriateness of this set of metrics is then tested in a case study analyzing the efficiency of commonality implementation in the cockpit of two well-known commercial aircraft families: the Airbus A320 family and the Boeing 737 family. This thesis further describes what additional analysis should be performed to validate the set of metrics for broader applications. After documenting the efficiency of the set of metrics, this thesis analyses the current practices of commonality management in commercial aviation. It finally explores some of the limitations of the concept of commonality and sketches solutions to overcome them."
}, {
    "id": "oai:dspace.mit.edu:1721.1/76826",
    "title": "Air flow effects in the piston ring pack and their implications on oil transport",
    "abstract": "3 different flow regimes of piston blowby air and their influences on oil transport are studied. It is found that air mainly interacts with oil close to the ring gaps and directly below the ring-liner contacts. Geometric features at the gaps to smoothen airflow and prevent flow detachments can increase blowby mass flow rate and thus drainage oil mass flow rate by up to 60%. Only oil within 1 to 2 gap widths distance from the gaps are transported through the gap by air drag and the engine pressure drop. Downstream of the ring gap, transported oil will either be caught in vortices directly below the ring gaps or pumped into the downstream ring groove due to the creation of a blowby stagnation point. Far away from the gaps, oil is mainly transported in axial direction through the grooves and the piston-liner interface. Low capillary numbers in the order of 10-5 indicate close to no oil transport into circumferential direction from blowby shear. The oil transport radially into the grooves is mainly determined by hydrostatics and capillary effects in the groove flanks whereas air in the second land only has an influence on oil transport by preventing bridging after TDC by creating a stagnation point directly below the rings on the liner.",
    "advisors": ["Tian Tian"],
    "text": "Air flow effects in the piston ring pack and their implications on oil transport 3 different flow regimes of piston blowby air and their influences on oil transport are studied. It is found that air mainly interacts with oil close to the ring gaps and directly below the ring-liner contacts. Geometric features at the gaps to smoothen airflow and prevent flow detachments can increase blowby mass flow rate and thus drainage oil mass flow rate by up to 60%. Only oil within 1 to 2 gap widths distance from the gaps are transported through the gap by air drag and the engine pressure drop. Downstream of the ring gap, transported oil will either be caught in vortices directly below the ring gaps or pumped into the downstream ring groove due to the creation of a blowby stagnation point. Far away from the gaps, oil is mainly transported in axial direction through the grooves and the piston-liner interface. Low capillary numbers in the order of 10-5 indicate close to no oil transport into circumferential direction from blowby shear. The oil transport radially into the grooves is mainly determined by hydrostatics and capillary effects in the groove flanks whereas air in the second land only has an influence on oil transport by preventing bridging after TDC by creating a stagnation point directly below the rings on the liner."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45258",
    "title": "Assuring safety in high-speed magnetically levitated (maglev) systems : the need for a system safety approach",
    "abstract": "Magnetic levitation is a railway technology that enables vehicles to be magnetically suspended above their tracks. Although this technology is still under development, magnetically levitated (maglev) systems have great potential to introduce significant changes in today's transportation networks. This thesis proposes an approach to assuring safety in high-speed maglev systems. It examines characteristic features of the systems, and analyzes the Japanese commuter railway accident in 2005, using Systems Theory Accident Modeling and Processes (STAMP) and System Dynamics models. The characteristic features reveal that the likelihood and potential severity of accidents in maglev systems are higher than those in conventional railway systems because of their high speed, levitation technology, software intensiveness, and other factors. A primary lesson learned from the accident is the importance of risk/hazard analysis that can qualitatively focus on the severity of accidents and human factors. These findings are put together in the form of requirements of risk/hazard analysis and organizational structures. This thesis demonstrates that these requirements, which are not entirely consistent with current actual practices based on international railway standards, conform well to the fundamentals of System Safety, which is an organized and established method to assure safety in complex systems.",
    "advisors": ["Nancy G. Leveson"],
    "text": "Assuring safety in high-speed magnetically levitated (maglev) systems : the need for a system safety approach Magnetic levitation is a railway technology that enables vehicles to be magnetically suspended above their tracks. Although this technology is still under development, magnetically levitated (maglev) systems have great potential to introduce significant changes in today's transportation networks. This thesis proposes an approach to assuring safety in high-speed maglev systems. It examines characteristic features of the systems, and analyzes the Japanese commuter railway accident in 2005, using Systems Theory Accident Modeling and Processes (STAMP) and System Dynamics models. The characteristic features reveal that the likelihood and potential severity of accidents in maglev systems are higher than those in conventional railway systems because of their high speed, levitation technology, software intensiveness, and other factors. A primary lesson learned from the accident is the importance of risk/hazard analysis that can qualitatively focus on the severity of accidents and human factors. These findings are put together in the form of requirements of risk/hazard analysis and organizational structures. This thesis demonstrates that these requirements, which are not entirely consistent with current actual practices based on international railway standards, conform well to the fundamentals of System Safety, which is an organized and established method to assure safety in complex systems."
}, {
    "id": "oai:dspace.mit.edu:1721.1/59683",
    "title": "Cost-benefit analysis of ultra-low sulfur jet fuel",
    "abstract": "The growth of aviation has spurred increased study of its environmental impacts and the possible mitigation thereof. One emissions reduction option is the introduction of an Ultra Low Sulfur (ULS) jet fuel standard for global commercial aviation. A full cost-benefit analysis, including impacts on air quality, climate, operations, and lifecycle costs is necessary to justify such a policy. The cost of a ULS jet fuel policy is well-characterized by the adoption of ULS diesel fuel, similar to jet fuel, for ground transportation in the US and elsewhere. The cost of hydrodesulfurization (HDS), the process used to remove sulfur from fuel, is projected to be between 4 and 7 cents per gallon of jet fuel. With 2006 levels of domestic fuel consumption, this translates to a yearly cost of HDS of $540-$940 million within the US. The climate and air quality benefits are characterized by several earth-atmosphere models, which isolate the perturbation of aviation emissions. Comparisons among models, which employ different modeling methods and assumptions as well as different spatial resolution, provide some cross-validation, as well as characterizing the degree of uncertainty in the state of the science. This thesis focuses in detail on the CMAQ (Community Multi-scale Air Quality) model, used by the Environmental Protection Agency (EPA) to support regulatory impact assessment. Other models, their results, and efforts at inter-model comparison are also discussed. Benefits are monetized through valuing the reduction in premature mortality from reduced concentrations of ground-level particulate matter (PM). The central finding from CMAQ is that with nominal health impact parameters, a global ULS jet fuel policy is predicted to save 110 lives per year in the US when considering full flight emissions, a 14% reduction in aviation-attributable mortality resulting in an estimated monetary benefit of $800 million.",
    "advisors": ["Ian A. Waitz"],
    "text": "Cost-benefit analysis of ultra-low sulfur jet fuel The growth of aviation has spurred increased study of its environmental impacts and the possible mitigation thereof. One emissions reduction option is the introduction of an Ultra Low Sulfur (ULS) jet fuel standard for global commercial aviation. A full cost-benefit analysis, including impacts on air quality, climate, operations, and lifecycle costs is necessary to justify such a policy. The cost of a ULS jet fuel policy is well-characterized by the adoption of ULS diesel fuel, similar to jet fuel, for ground transportation in the US and elsewhere. The cost of hydrodesulfurization (HDS), the process used to remove sulfur from fuel, is projected to be between 4 and 7 cents per gallon of jet fuel. With 2006 levels of domestic fuel consumption, this translates to a yearly cost of HDS of $540-$940 million within the US. The climate and air quality benefits are characterized by several earth-atmosphere models, which isolate the perturbation of aviation emissions. Comparisons among models, which employ different modeling methods and assumptions as well as different spatial resolution, provide some cross-validation, as well as characterizing the degree of uncertainty in the state of the science. This thesis focuses in detail on the CMAQ (Community Multi-scale Air Quality) model, used by the Environmental Protection Agency (EPA) to support regulatory impact assessment. Other models, their results, and efforts at inter-model comparison are also discussed. Benefits are monetized through valuing the reduction in premature mortality from reduced concentrations of ground-level particulate matter (PM). The central finding from CMAQ is that with nominal health impact parameters, a global ULS jet fuel policy is predicted to save 110 lives per year in the US when considering full flight emissions, a 14% reduction in aviation-attributable mortality resulting in an estimated monetary benefit of $800 million."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32463",
    "title": "A microfabricated ElectroQuasiStatic induction turbine-generator",
    "abstract": "An ElectroQuasiStatic (EQS) induction machine has been fabricated and has generated net electric power. A maximum power output of 192 [mu]W at 235 krpm has been measured under driven excitation of the six phases. Self excited operation was also demonstrated. Under self-excitation, no external drive electronics are required and sufficient power was produced to dimly light four LED's on two of the six phases. This is believed to be the first demonstration of both power generation and self-excited operation of an EQS induction machine of any scale reported in the open literature. The generator comprises 5 silicon layers, fusion bonded together, and annealed at 700⁰C. The turbine rotor, 4 mm in diameter, is supported on gas bearings. The thrust bearings are formed by a shallow etch of 1.5 [mu]m to define the thrust bearing gap. Thrust bearing pressurization is through 10 [mu]m diameter nozzles, etched 100 [mu]m deep. The journal bearing is a precision, ... wide, 300 [mu]m deep annular trench around the periphery of the turbine disk. The generator airgap is 3 [mu]m. The inner radius of the generator is 1.011 mm, and the outer radius 1.87mm. The machine has ].31 poles for each of the 6 phases, for a total of 786 stator electrodes. Precise microfabrication and aligned, full-wafer fusion bonding enabled turbine generator devices to be operated at rotational speeds as high as 850 krpm. A detailed state-space model of the EQS machine and its external parasitics is presented. The external stray capacitances, and their unbalance, play a critical role in the performance of the device. A method for estimating the strays experimentally is discussed.",
    "advisors": ["Carol Livermore"],
    "text": "A microfabricated ElectroQuasiStatic induction turbine-generator An ElectroQuasiStatic (EQS) induction machine has been fabricated and has generated net electric power. A maximum power output of 192 [mu]W at 235 krpm has been measured under driven excitation of the six phases. Self excited operation was also demonstrated. Under self-excitation, no external drive electronics are required and sufficient power was produced to dimly light four LED's on two of the six phases. This is believed to be the first demonstration of both power generation and self-excited operation of an EQS induction machine of any scale reported in the open literature. The generator comprises 5 silicon layers, fusion bonded together, and annealed at 700⁰C. The turbine rotor, 4 mm in diameter, is supported on gas bearings. The thrust bearings are formed by a shallow etch of 1.5 [mu]m to define the thrust bearing gap. Thrust bearing pressurization is through 10 [mu]m diameter nozzles, etched 100 [mu]m deep. The journal bearing is a precision, ... wide, 300 [mu]m deep annular trench around the periphery of the turbine disk. The generator airgap is 3 [mu]m. The inner radius of the generator is 1.011 mm, and the outer radius 1.87mm. The machine has ].31 poles for each of the 6 phases, for a total of 786 stator electrodes. Precise microfabrication and aligned, full-wafer fusion bonding enabled turbine generator devices to be operated at rotational speeds as high as 850 krpm. A detailed state-space model of the EQS machine and its external parasitics is presented. The external stray capacitances, and their unbalance, play a critical role in the performance of the device. A method for estimating the strays experimentally is discussed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/40306",
    "title": "Experimental and computational studies of electric thruster plasma radiation emission",
    "abstract": "Electric thrusters are being developed for in-space propulsion needs of spacecraft as their higher specific impulse enables a significant reduction in the required propellant mass and allows longer duration missions. Over the last few decades many different electric propulsion concepts have been proposed and studied. In studying the electric thrusters, in order to improve the thruster performance as well as to understand the underlying physics of thruster's operation, various diagnostics methods were employed. As one unique method, emission spectroscopy provides a non-invasive, fast and economical diagnostic allowing also the ability to access hard to reach locations. In this study, emission spectroscopy is employed as a means to determine the trends in thruster operations as well as diagnosing the plasma parameters. This study presents the spectral measurement results of three different electric thrusters and plasma sources. First, the BHT-200 Hall thruster emission spectra measurements are presented for varying discharge voltage and for various regions of observation.",
    "advisors": ["Manual Martínez-Sánchez", "Oleg Batishchev"],
    "text": "Experimental and computational studies of electric thruster plasma radiation emission Electric thrusters are being developed for in-space propulsion needs of spacecraft as their higher specific impulse enables a significant reduction in the required propellant mass and allows longer duration missions. Over the last few decades many different electric propulsion concepts have been proposed and studied. In studying the electric thrusters, in order to improve the thruster performance as well as to understand the underlying physics of thruster's operation, various diagnostics methods were employed. As one unique method, emission spectroscopy provides a non-invasive, fast and economical diagnostic allowing also the ability to access hard to reach locations. In this study, emission spectroscopy is employed as a means to determine the trends in thruster operations as well as diagnosing the plasma parameters. This study presents the spectral measurement results of three different electric thrusters and plasma sources. First, the BHT-200 Hall thruster emission spectra measurements are presented for varying discharge voltage and for various regions of observation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/85692",
    "title": "Human perception and control of vehicle roll tilt in hyper-gravity",
    "abstract": "Pilots and astronauts experience a range of altered gravity environments in which they must maintain accurate perception and control of vehicle orientation for tasks such as landing and docking. To study sensorimotor function in altered gravity, a hyper-gravity test-bed was produced using a centrifuge. Previous experiments have quantified static tilt perception in hyper-gravity; however, studies of dynamic tilt, such as those experienced by astronauts and pilots, have been entirely qualitative. Current dynamic models of orientation perception cannot reproduce the characteristic perceptions observed in hyper-gravity. The aims of this thesis are to: 1) quantify static and dynamic roll tilt perception in hyper-gravity, 2) study pilot manual control of vehicle roll tilt in hyper-gravity, and 3) modify a dynamic model to predict hyper-gravity orientation perception. A long-radius centrifuge was utilized to create hyper-gravity environments of 1.5 and 2 Earth G's. In one experiment, over a range of roll tilt angles and frequencies, human subjects' (N=8) perceptions of orientation, in the dark, were assayed with a somatosensory task. Static roll tilts were overestimated in hyper-gravity with more overestimation at higher gravity levels and larger roll angles. Dynamic rotations were also overestimated in hyper-gravity, but generally less so than for static tilts. The amount of overestimation during dynamic rotations was dependent upon the angular velocity of the rotation with less overestimation at higher angular velocities. In a second experiment, human subjects (N=12) were tasked with nulling a pseudo-random vehicle roll disturbance using a rotational hand controller. Initial nulling performance was significantly worse in hyper-gravity as compared to the 1 G performance baseline. However, hyper-gravity performance improved with practice, reaching near the 1 G baseline over the time course of several minutes. Finally, pre-exposure to one hyper-gravity level reduced the measured initial performance decrement in a subsequent, different hyper-gravity environment. A modification to a previous dynamic spatial orientation perception model was proposed to allow for the prediction of roll tilt overestimation observed in hyper-gravity. It was hypothesized that the central nervous system treats otolith signals in the utricular plane differently from those out of plane. This was implemented in the model by setting a difference between the linear acceleration feedback gains in and out of the utricular plane. The modified model was simulated and found to accurately predict the static overestimation observed over a wide range of angles and hyper-gravity levels. Furthermore, it simulated the characteristic dependence of dynamic overestimation upon angular velocity with less overestimation at higher angular velocities. The modified model now allows for simulation across a range of altered gravity environments to predict human orientation perception. We conclude that hyper-gravity results in misperception of static and dynamic roll tilt and decrements in pilot manual control performance. Perception and manual control errors due to altered gravity, such as those observed here in hyper-gravity, may impact the safety of future crewed space exploration missions, in terms of accidents or aborts.",
    "advisors": ["Torin K. Clark"],
    "text": "Human perception and control of vehicle roll tilt in hyper-gravity Pilots and astronauts experience a range of altered gravity environments in which they must maintain accurate perception and control of vehicle orientation for tasks such as landing and docking. To study sensorimotor function in altered gravity, a hyper-gravity test-bed was produced using a centrifuge. Previous experiments have quantified static tilt perception in hyper-gravity; however, studies of dynamic tilt, such as those experienced by astronauts and pilots, have been entirely qualitative. Current dynamic models of orientation perception cannot reproduce the characteristic perceptions observed in hyper-gravity. The aims of this thesis are to: 1) quantify static and dynamic roll tilt perception in hyper-gravity, 2) study pilot manual control of vehicle roll tilt in hyper-gravity, and 3) modify a dynamic model to predict hyper-gravity orientation perception. A long-radius centrifuge was utilized to create hyper-gravity environments of 1.5 and 2 Earth G's. In one experiment, over a range of roll tilt angles and frequencies, human subjects' (N=8) perceptions of orientation, in the dark, were assayed with a somatosensory task. Static roll tilts were overestimated in hyper-gravity with more overestimation at higher gravity levels and larger roll angles. Dynamic rotations were also overestimated in hyper-gravity, but generally less so than for static tilts. The amount of overestimation during dynamic rotations was dependent upon the angular velocity of the rotation with less overestimation at higher angular velocities. In a second experiment, human subjects (N=12) were tasked with nulling a pseudo-random vehicle roll disturbance using a rotational hand controller. Initial nulling performance was significantly worse in hyper-gravity as compared to the 1 G performance baseline. However, hyper-gravity performance improved with practice, reaching near the 1 G baseline over the time course of several minutes. Finally, pre-exposure to one hyper-gravity level reduced the measured initial performance decrement in a subsequent, different hyper-gravity environment. A modification to a previous dynamic spatial orientation perception model was proposed to allow for the prediction of roll tilt overestimation observed in hyper-gravity. It was hypothesized that the central nervous system treats otolith signals in the utricular plane differently from those out of plane. This was implemented in the model by setting a difference between the linear acceleration feedback gains in and out of the utricular plane. The modified model was simulated and found to accurately predict the static overestimation observed over a wide range of angles and hyper-gravity levels. Furthermore, it simulated the characteristic dependence of dynamic overestimation upon angular velocity with less overestimation at higher angular velocities. The modified model now allows for simulation across a range of altered gravity environments to predict human orientation perception. We conclude that hyper-gravity results in misperception of static and dynamic roll tilt and decrements in pilot manual control performance. Perception and manual control errors due to altered gravity, such as those observed here in hyper-gravity, may impact the safety of future crewed space exploration missions, in terms of accidents or aborts."
}, {
    "id": "oai:dspace.mit.edu:1721.1/17831",
    "title": "Experimental evaluation and modeling of a turbine blade with potassium evaporative cooling",
    "abstract": "A new method of turbine blade cooling, the Return Flow Cascade, has been developed in which vaporization of a liquid metal such as potassium is used to maintain the blade surface at a nearly uniform temperature. Turbine blades cooled using this technology have lower blade temperature levels compared to that available with conventional air cooling, potentially resulting in higher firing temperatures or a choice of a wider range of materials for the hot gas path. The detailed operation of the Return Flow Cascade is described including fluid mechanic and heat transfer phenomena that occur at high heat flux and gravitational acceleration levels characteristic of modern gas turbine engines. The performance limits of the Return Flow Cascade are identified by the development of a theoretical model that estimates the performance of the system for a range of operating conditions found in the experimental test rig and in an actual gas turbine engine. These limits include vapor choking in the internal blade passages, pool boiling limits in the blade, and surface tension restriction of liquid flow. Experimental results are presented from testing in the Rotating Heat Transfer Rig at the Massachusetts Institute of Technology. An infrared detector capable of high scan rates was used to fully map the temperature distribution of a single, heated, rotating turbine blade. The blade surface temperature maps show that the Return Flow Cascade works as designed by enforcing a nearly constant temperature over the surface of the blade. Cascade initiation limits predicted by the internal vapor choking model are in good agreement with the test data.",
    "advisors": ["Jack L. Kerrebrock"],
    "text": "Experimental evaluation and modeling of a turbine blade with potassium evaporative cooling A new method of turbine blade cooling, the Return Flow Cascade, has been developed in which vaporization of a liquid metal such as potassium is used to maintain the blade surface at a nearly uniform temperature. Turbine blades cooled using this technology have lower blade temperature levels compared to that available with conventional air cooling, potentially resulting in higher firing temperatures or a choice of a wider range of materials for the hot gas path. The detailed operation of the Return Flow Cascade is described including fluid mechanic and heat transfer phenomena that occur at high heat flux and gravitational acceleration levels characteristic of modern gas turbine engines. The performance limits of the Return Flow Cascade are identified by the development of a theoretical model that estimates the performance of the system for a range of operating conditions found in the experimental test rig and in an actual gas turbine engine. These limits include vapor choking in the internal blade passages, pool boiling limits in the blade, and surface tension restriction of liquid flow. Experimental results are presented from testing in the Rotating Heat Transfer Rig at the Massachusetts Institute of Technology. An infrared detector capable of high scan rates was used to fully map the temperature distribution of a single, heated, rotating turbine blade. The blade surface temperature maps show that the Return Flow Cascade works as designed by enforcing a nearly constant temperature over the surface of the blade. Cascade initiation limits predicted by the internal vapor choking model are in good agreement with the test data."
}, {
    "id": "oai:dspace.mit.edu:1721.1/103422",
    "title": "Using STPA to inform developmental product testing",
    "abstract": "Developmental product testing currently evaluates system safety the same way it evaluates system performance: it attempts to isolate individual components' behaviors to evaluate their reliability. However, today's systems are often irreducible because of their complexity, leaving current practices ineffective at identifying safety deficiencies. Evolving to a modern systems-based hazard analysis is important for product development. Products stand to benefit during the testing stage, before initial fielding. In test, designs meet operation for the first time, and use practices and organizational influences both contribute to the safety of the system. By evaluating safety as an emergent property, hazards that emerge because of the testing process itself can be mitigated, and hazards that exist because of the inherent system design and use philosophy can be identified and traced throughout development and fielding. System-Theoretic Process Analysis (STPA), developed by Nancy Leveson at the Massachusetts Institute of Technology, is a modern hazard analysis technique that identifies unsafe scenarios in a system in order to generate requirements to eliminate or control those scenarios. It improves on traditional reductionist approaches that treat accident causation only as a linear chain of events or probabilistic occurrence of simultaneous component failures (including human error). While systems-based and complete, STPA could benefit from additional guidance, particularly in the identification of human contributions to accidents. The present research begins by extending STPA to include more guidance for the controller analysis, including refinements to the process model, fundamental human-engineering considerations, and socio-organizational influences. Next, Leveson's organizational control structure example is updated to include a test stage that serves as an intermediary between design and field use. Model inclusion criteria are updated, and Explicit-Influence Maps are introduced as a tool to understand the organization and aid in hazard analysis. Finally, this research investigates the U.S. Air Force developmental testing enterprise and applies STPA to a product test. Results are compared to that of the test-safety planning and reporting techniques traditionally in use, and utility is assessed with a research survey administered to developmental test professionals. Keywords: STAMP, STPA, system safety, hazard analysis, product testing, test safety, problem reporting, safety certification",
    "advisors": ["Nancy G. Leveson"],
    "text": "Using STPA to inform developmental product testing Developmental product testing currently evaluates system safety the same way it evaluates system performance: it attempts to isolate individual components' behaviors to evaluate their reliability. However, today's systems are often irreducible because of their complexity, leaving current practices ineffective at identifying safety deficiencies. Evolving to a modern systems-based hazard analysis is important for product development. Products stand to benefit during the testing stage, before initial fielding. In test, designs meet operation for the first time, and use practices and organizational influences both contribute to the safety of the system. By evaluating safety as an emergent property, hazards that emerge because of the testing process itself can be mitigated, and hazards that exist because of the inherent system design and use philosophy can be identified and traced throughout development and fielding. System-Theoretic Process Analysis (STPA), developed by Nancy Leveson at the Massachusetts Institute of Technology, is a modern hazard analysis technique that identifies unsafe scenarios in a system in order to generate requirements to eliminate or control those scenarios. It improves on traditional reductionist approaches that treat accident causation only as a linear chain of events or probabilistic occurrence of simultaneous component failures (including human error). While systems-based and complete, STPA could benefit from additional guidance, particularly in the identification of human contributions to accidents. The present research begins by extending STPA to include more guidance for the controller analysis, including refinements to the process model, fundamental human-engineering considerations, and socio-organizational influences. Next, Leveson's organizational control structure example is updated to include a test stage that serves as an intermediary between design and field use. Model inclusion criteria are updated, and Explicit-Influence Maps are introduced as a tool to understand the organization and aid in hazard analysis. Finally, this research investigates the U.S. Air Force developmental testing enterprise and applies STPA to a product test. Results are compared to that of the test-safety planning and reporting techniques traditionally in use, and utility is assessed with a research survey administered to developmental test professionals. Keywords: STAMP, STPA, system safety, hazard analysis, product testing, test safety, problem reporting, safety certification"
}, {
    "id": "oai:dspace.mit.edu:1721.1/16836",
    "title": "Piezoelectric-based in-situ damage detection of composite materials for structural health monitoring systems",
    "abstract": "Cost-effective and reliable damage detection is critical for the utilization of composite materials. This thesis presents the conclusions of an analytical and experimental survey of candidate methods for in-situ damage detection in composite materials. Finite element results are presented for the application of modal analysis and Lamb wave techniques to quasi-isotropic graphite/epoxy test specimens containing representative damage. These results were then verified experimentally by using piezoelectric patches as actuators and sensors for both sets of experiments. The passive modal analysis method was reliable for detecting small amounts of global damage in a simple composite structures. By comparison, the active Lamb wave method was sensitive to all types of local damage present between the sensor and actuator, provided useful information about damage presence and severity, and presents the possibility of estimating damage type and location. Analogous experiments were also performed for more complex built-up structures such as sandwich beams, stiffened plates and composite cylinders.",
    "advisors": ["S. Mark Spearing"],
    "text": "Piezoelectric-based in-situ damage detection of composite materials for structural health monitoring systems Cost-effective and reliable damage detection is critical for the utilization of composite materials. This thesis presents the conclusions of an analytical and experimental survey of candidate methods for in-situ damage detection in composite materials. Finite element results are presented for the application of modal analysis and Lamb wave techniques to quasi-isotropic graphite/epoxy test specimens containing representative damage. These results were then verified experimentally by using piezoelectric patches as actuators and sensors for both sets of experiments. The passive modal analysis method was reliable for detecting small amounts of global damage in a simple composite structures. By comparison, the active Lamb wave method was sensitive to all types of local damage present between the sensor and actuator, provided useful information about damage presence and severity, and presents the possibility of estimating damage type and location. Analogous experiments were also performed for more complex built-up structures such as sandwich beams, stiffened plates and composite cylinders."
}, {
    "id": "oai:dspace.mit.edu:1721.1/40856",
    "title": "Experimental and theoretical characterization of a Hall thruster plume",
    "abstract": "Despite the considerable flight heritage of the Hall thruster, the interaction of its plume with the spacecraft remains an important integration issue. Because in-flight data fully characterizing the plume in the space environment are currently unavailable, laboratory measurements are often used to understand plasma expansion and thereby minimize adverse plume-spacecraft interactions. However, experimental measurements obtained in ground facilities do not properly capture the wide angle plume effects most important for plume-spacecraft interactions because of the high background pressure of the laboratory environment. This research describes a method to determine the in-orbit plume divergence of a Hall thruster from laboratory measurements and characterizes the plasma properties of the in-orbit plume. Plume measurements were taken with a Faraday probe and a Retarding Potential Analyzer at various background pressures to correlate changes in current density and ion energy distribution with changes in pressure. Results showed that current density increases linearly with background pressure at any given angle. This linear relationship was used to extrapolate laboratory measurements to zero background pressure, the in-orbit condition. Measurements from the Faraday probe and the Retarding Potential Analyzer were compared to ensure consistency. The effect of discharge voltage on plume divergence was also investigated. Measurements from both probes revealed that plume divergence decreases with an increase in discharge voltage. Hall thruster plume expansion was also characterized using a numerical plume simulation. Comparison of plume simulation results for in-orbit conditions to extrapolated current density at zero pressure demonstrated good agreement.",
    "advisors": ["Manuel Martinez-Sanchez"],
    "text": "Experimental and theoretical characterization of a Hall thruster plume Despite the considerable flight heritage of the Hall thruster, the interaction of its plume with the spacecraft remains an important integration issue. Because in-flight data fully characterizing the plume in the space environment are currently unavailable, laboratory measurements are often used to understand plasma expansion and thereby minimize adverse plume-spacecraft interactions. However, experimental measurements obtained in ground facilities do not properly capture the wide angle plume effects most important for plume-spacecraft interactions because of the high background pressure of the laboratory environment. This research describes a method to determine the in-orbit plume divergence of a Hall thruster from laboratory measurements and characterizes the plasma properties of the in-orbit plume. Plume measurements were taken with a Faraday probe and a Retarding Potential Analyzer at various background pressures to correlate changes in current density and ion energy distribution with changes in pressure. Results showed that current density increases linearly with background pressure at any given angle. This linear relationship was used to extrapolate laboratory measurements to zero background pressure, the in-orbit condition. Measurements from the Faraday probe and the Retarding Potential Analyzer were compared to ensure consistency. The effect of discharge voltage on plume divergence was also investigated. Measurements from both probes revealed that plume divergence decreases with an increase in discharge voltage. Hall thruster plume expansion was also characterized using a numerical plume simulation. Comparison of plume simulation results for in-orbit conditions to extrapolated current density at zero pressure demonstrated good agreement."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8107",
    "title": "Integral twist actuation of helicolpter rotor blades for vibration reduction",
    "abstract": "Active integral twist control for vibration reduction of helicopter rotors during forward flight is investigated. The twist deformation is obtained using embedded anisotropic piezocomposite actuators. An analytical framework is developed to examine integrally-twisted blades and their aeroelastic response during different flight conditions: frequency domain analysis for hover, and time domain analysis for forward flight. Both stem from the same three-dimensional electroelastic beam formulation with geometrical-exactness, and are coupled with a finite-state dynamic inflow aerodynamics model. A prototype Active Twist Rotor blade was designed with this framework using Active Fiber Composites as the actuator. The ATR prototype blade was successfully tested under non-rotating conditions. Hover testing was conducted to evaluate structural integrity and dynamic response. In both conditions, a very good correlation was obtained against the analysis. Finally, a four-bladed ATR system is built and tested to demonstrate its concept in forward flight. This experiment was conducted at NASA Langley Transonic Dynamics Tunnel and represents the first-of-a-kind Mach-scaled fully-active-twist rotor system to undergo forward flight test. In parallel, the impact upon the fixed- and rotating-system loads is estimated by the analysis. While discrepancies are found in the amplitude of the loads under actuation, the predicted trend of load variation with respect to its control phase correlates well. It was also shown, both experimentally and numerically, that the ATR blade design has the potential for hub vibratory load reduction of up to 90% using individual blade control actuation.",
    "advisors": ["Carlos E.S. Cesnik"],
    "text": "Integral twist actuation of helicolpter rotor blades for vibration reduction Active integral twist control for vibration reduction of helicopter rotors during forward flight is investigated. The twist deformation is obtained using embedded anisotropic piezocomposite actuators. An analytical framework is developed to examine integrally-twisted blades and their aeroelastic response during different flight conditions: frequency domain analysis for hover, and time domain analysis for forward flight. Both stem from the same three-dimensional electroelastic beam formulation with geometrical-exactness, and are coupled with a finite-state dynamic inflow aerodynamics model. A prototype Active Twist Rotor blade was designed with this framework using Active Fiber Composites as the actuator. The ATR prototype blade was successfully tested under non-rotating conditions. Hover testing was conducted to evaluate structural integrity and dynamic response. In both conditions, a very good correlation was obtained against the analysis. Finally, a four-bladed ATR system is built and tested to demonstrate its concept in forward flight. This experiment was conducted at NASA Langley Transonic Dynamics Tunnel and represents the first-of-a-kind Mach-scaled fully-active-twist rotor system to undergo forward flight test. In parallel, the impact upon the fixed- and rotating-system loads is estimated by the analysis. While discrepancies are found in the amplitude of the loads under actuation, the predicted trend of load variation with respect to its control phase correlates well. It was also shown, both experimentally and numerically, that the ATR blade design has the potential for hub vibratory load reduction of up to 90% using individual blade control actuation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/9362",
    "title": "Design and analysis of axial aspirated compressor stages",
    "abstract": "The pressure ratio of axial compressor stages can be significantly increased by controlling the development of blade and endwall boundary layers in regions of adverse pressure gradient by means of boundary layer suction. This concept is validated and demonstrated through the design and analysis of two unique aspirated compressor stages: a low-speed stage with a design pressure ratio of 1.6 at a tip speed of 750 ft/s, and a high-speed stage with a design pressure ratio of 3.5 at a tip speed of 1500 ft/s. The aspirated compressor stages were designed using a new procedure which is a synthesis of low speed and high speed blade design techniques combined with a flexible inverse design method which enabled precise independent control over the shape of the blade suction and pressure surfaces. Integration of the boundary layer suction calculation into the overall design process is an essential ingredient of the new procedure. The blade design system consists of two axisymmetric through-flow codes coupled with a quasi three-dimensional viscous cascade plane code with inverse design capability. Validation of the completed designs were carried out with three-dimensional Euler and Navier-Stokes calculations. A single spanwise slot on the blade suction surface is used to bleed the boundary layer. The suction mass flow requirement for the low-speed and high-speed stages are 1 % and 4% of the inlet mass flow, respectively. Additional suction between 1-2% is also required on the compressor end walls near shock impingement locations. The rotor is modeled with a tip shroud to eliminate tip clearance effects and to discharge the suction flow radially from the flowpath. Three-dimensional viscous evaluation of the designs showed good agreement with the quasi three-dimensional design intent, except in the endwall regions. The suction requirements predicted by the quasi three-dimensional calculation were confirmed by the three-dimensional viscous calculations. The three-dimensional viscous analysis predicted a peak pressure ratio of 1.59 at an isentropic efficiency of 89% for the low-speed stage, and a peak pressure ratio of 3.68 at an isentropic efficiency of 94% for the high-speed rotor.",
    "advisors": ["Mark Drela"],
    "text": "Design and analysis of axial aspirated compressor stages The pressure ratio of axial compressor stages can be significantly increased by controlling the development of blade and endwall boundary layers in regions of adverse pressure gradient by means of boundary layer suction. This concept is validated and demonstrated through the design and analysis of two unique aspirated compressor stages: a low-speed stage with a design pressure ratio of 1.6 at a tip speed of 750 ft/s, and a high-speed stage with a design pressure ratio of 3.5 at a tip speed of 1500 ft/s. The aspirated compressor stages were designed using a new procedure which is a synthesis of low speed and high speed blade design techniques combined with a flexible inverse design method which enabled precise independent control over the shape of the blade suction and pressure surfaces. Integration of the boundary layer suction calculation into the overall design process is an essential ingredient of the new procedure. The blade design system consists of two axisymmetric through-flow codes coupled with a quasi three-dimensional viscous cascade plane code with inverse design capability. Validation of the completed designs were carried out with three-dimensional Euler and Navier-Stokes calculations. A single spanwise slot on the blade suction surface is used to bleed the boundary layer. The suction mass flow requirement for the low-speed and high-speed stages are 1 % and 4% of the inlet mass flow, respectively. Additional suction between 1-2% is also required on the compressor end walls near shock impingement locations. The rotor is modeled with a tip shroud to eliminate tip clearance effects and to discharge the suction flow radially from the flowpath. Three-dimensional viscous evaluation of the designs showed good agreement with the quasi three-dimensional design intent, except in the endwall regions. The suction requirements predicted by the quasi three-dimensional calculation were confirmed by the three-dimensional viscous calculations. The three-dimensional viscous analysis predicted a peak pressure ratio of 1.59 at an isentropic efficiency of 89% for the low-speed stage, and a peak pressure ratio of 3.68 at an isentropic efficiency of 94% for the high-speed rotor."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46560",
    "title": "Mitigating complexity in Air Traffic Control : the role of structure-based abstractions",
    "abstract": "Cognitive complexity is a limiting factor on the capacity and efficiency of the Air Traffic Control (ATC) system. A multi-faceted cognitive ethnography approach shows that structure, defined as the physical and informational elements that organize and arrange the ATC environment, plays an important role in helping controllers mitigate cognitive complexity. Key influences of structure in the operational environment and on controller cognitive processes are incorporated into a cognitive process model. Controllers are hypothesized to internalize the structural influences in the form of abstractions simplifying their working mental model of the situation. By simplifying their working mental model, these structure-based abstractions reduce cognitive complexity. Four examples of structure-based abstractions are identified and mechanisms by which they reduce cognitive complexity described. Experimental evidence is presented to support a key cognitive complexity reduction mechanism, the reduction of the \"order\", or the degrees-of-freedom, of a controller's working mental model. The use of structure-based abstractions is dynamic and responsive to changes in task conditions; these changes are hypothesized to reflect transitions between distinct operating modes. Experimental evidence of such changes in the use of standard flows in the airspace is presented. The cognitive process model and the concept of structure-based abstractions are shown to be useful tools for identifying cognitive complexity considerations arising from changes to the structure of the ATC system. Examples of cognitive complexity considerations for four opportunities to increase the efficiency, capacity, and robustness of the ATC system are presented. The cognitive process model is also used as part of a cognitive review of the current en route controller training system. This review revealed key pedagogical techniques used to teach structure, factors creating the need for sector-specific mental models and abstractions, and opportunities to improve the efficiency of controller training, such as developing more generic airspace.",
    "advisors": ["R. John Hansman"],
    "text": "Mitigating complexity in Air Traffic Control : the role of structure-based abstractions Cognitive complexity is a limiting factor on the capacity and efficiency of the Air Traffic Control (ATC) system. A multi-faceted cognitive ethnography approach shows that structure, defined as the physical and informational elements that organize and arrange the ATC environment, plays an important role in helping controllers mitigate cognitive complexity. Key influences of structure in the operational environment and on controller cognitive processes are incorporated into a cognitive process model. Controllers are hypothesized to internalize the structural influences in the form of abstractions simplifying their working mental model of the situation. By simplifying their working mental model, these structure-based abstractions reduce cognitive complexity. Four examples of structure-based abstractions are identified and mechanisms by which they reduce cognitive complexity described. Experimental evidence is presented to support a key cognitive complexity reduction mechanism, the reduction of the \"order\", or the degrees-of-freedom, of a controller's working mental model. The use of structure-based abstractions is dynamic and responsive to changes in task conditions; these changes are hypothesized to reflect transitions between distinct operating modes. Experimental evidence of such changes in the use of standard flows in the airspace is presented. The cognitive process model and the concept of structure-based abstractions are shown to be useful tools for identifying cognitive complexity considerations arising from changes to the structure of the ATC system. Examples of cognitive complexity considerations for four opportunities to increase the efficiency, capacity, and robustness of the ATC system are presented. The cognitive process model is also used as part of a cognitive review of the current en route controller training system. This review revealed key pedagogical techniques used to teach structure, factors creating the need for sector-specific mental models and abstractions, and opportunities to improve the efficiency of controller training, such as developing more generic airspace."
}, {
    "id": "oai:dspace.mit.edu:1721.1/82476",
    "title": "Mission design for safe traverse of planetary hoppers",
    "abstract": "Planetary hoppers are a new class of vehicle being developed that will provide planetary surface mobility by reusing the landing platform and its actuators to propulsively ascend, translate, and descend to new landing points on the surface of a planetary body. Hoppers enhance regional exploration, with the capability of rapid traverse over hundreds to thousands of meters, traverse over hazardous terrain, and exploration of cliffs and craters. These planetary mobility vehicles are fuel limited and as a result are enabled by carrying sensor payloads that require low mass, low volume, and low onboard computational resources. This thesis describes methods for hoppers to traverse and land safely in this constrained environment. The key questions of this research are: - What types of missions will hoppers perform and how does a hopper traverse as part of these missions? - How does a hopper traverse from its current location to a new landing site safely? This thesis: - describes various hopper mission scenarios and considerations for their mission designs. - creates an operational concept for safe landing for the traverse hop mission scenario. - develops a method that can be used to rapidly and safely detect landing areas at long ranges and low path angles. - develops a method to do fine detection of hazards once at the landing area.",
    "advisors": ["Jeffrey A. Hoffman"],
    "text": "Mission design for safe traverse of planetary hoppers Planetary hoppers are a new class of vehicle being developed that will provide planetary surface mobility by reusing the landing platform and its actuators to propulsively ascend, translate, and descend to new landing points on the surface of a planetary body. Hoppers enhance regional exploration, with the capability of rapid traverse over hundreds to thousands of meters, traverse over hazardous terrain, and exploration of cliffs and craters. These planetary mobility vehicles are fuel limited and as a result are enabled by carrying sensor payloads that require low mass, low volume, and low onboard computational resources. This thesis describes methods for hoppers to traverse and land safely in this constrained environment. The key questions of this research are: - What types of missions will hoppers perform and how does a hopper traverse as part of these missions? - How does a hopper traverse from its current location to a new landing site safely? This thesis: - describes various hopper mission scenarios and considerations for their mission designs. - creates an operational concept for safe landing for the traverse hop mission scenario. - develops a method that can be used to rapidly and safely detect landing areas at long ranges and low path angles. - develops a method to do fine detection of hazards once at the landing area."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34134",
    "title": "An a posteriori error control framework for adaptive precision optimization using discontinuous Galerkin finite element method",
    "abstract": "Introduction: Aerodynamic design optimization has seen significant development over the past decade. Adjoint-based shape design for elliptic systems was first proposed by Pironneau and applied to transonic flow by Jameson . A review of the aerodynamic shape optimization literature and a large list of references is given in. Over the years much technology has been developed, allowing engineers to contemplate applying optimization methods to a wide variety of problems. In the context of structured grids, adjoint-based applications include multipoint, multi-objective airfoil design using compressible Navier-Stokes equations and 3D multipoint design of aircraft configurations using inviscid Euler equations. There have also been significant effort in applying adjoint methods to the unstructured grid setting. In this context, Newman et al., Elliot and Peraire were among the first to develop discrete adjoint approaches for the inviscid Euler equations.",
    "advisors": ["David L. Darmofal"],
    "text": "An a posteriori error control framework for adaptive precision optimization using discontinuous Galerkin finite element method Introduction: Aerodynamic design optimization has seen significant development over the past decade. Adjoint-based shape design for elliptic systems was first proposed by Pironneau and applied to transonic flow by Jameson . A review of the aerodynamic shape optimization literature and a large list of references is given in. Over the years much technology has been developed, allowing engineers to contemplate applying optimization methods to a wide variety of problems. In the context of structured grids, adjoint-based applications include multipoint, multi-objective airfoil design using compressible Navier-Stokes equations and 3D multipoint design of aircraft configurations using inviscid Euler equations. There have also been significant effort in applying adjoint methods to the unstructured grid setting. In this context, Newman et al., Elliot and Peraire were among the first to develop discrete adjoint approaches for the inviscid Euler equations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/82499",
    "title": "Factors affecting piston ring friction",
    "abstract": "The piston ring pack friction is a major contributor to the internal combustion engine mechanical friction loss. The oil control ring decides the oil supply to the top two rings in addition to being the major friction contributor in the ring pack. This work concentrated on the oil control ring friction. A large range of ring land widths and tensions, liner finish, and oil viscosity were investigated both experimentally and numerically to reveal how different factors affect the piston ring friction. A floating liner engine (FLE) was modified for motoring tests. The engine system repeatability and self-consistency were demonstrated. The thesis then discussed proper methods to select and measure the rings, liners and oil, which were important to generating meaningful results from the experiment. The ranges of engine speeds and liner temperatures were designed to ensure that all the lubrication modes, namely, boundary, mixed and hydrodynamic, can become dominant in both the instantaneous friction over a cycle and the FMEP over the engine speed range for any combination of rings, liners and lubricants. A parallel modeling effort was made to the experiments. The work showed that with careful preparation of adequate information on rings, liners and lubricants, the model can match the friction trends observed in the experiment over a large range of operating parameters and designs on the ring, liner finish and lubricant viscosity. The ring friction change over the liner break-in was studied using liners covering a wide range of surface roughness. The hydrodynamic pressure generation ability of the liner appears to be decided by the large surface structure. Therefore, the break-in process, which removes individual asperities from the plateau, does not affect pure hydrodynamic lubrication, and only the mixed lubrication is affected by the plateau roughness change. By keeping the same hydrodynamic pressure - ring/liner clearance (P-h) correlation and changing the plateau roughness, the model can predict the ring friction change over different lubrication regimes during the break-in. Compared to the current industry norm, a new engine power cylinder system design using a smaller land width twin land oil control ring with a lower ring tension and accompanied by a smoother liner surface gives lower friction and better oil control at the same time.",
    "advisors": ["Tian Tian"],
    "text": "Factors affecting piston ring friction The piston ring pack friction is a major contributor to the internal combustion engine mechanical friction loss. The oil control ring decides the oil supply to the top two rings in addition to being the major friction contributor in the ring pack. This work concentrated on the oil control ring friction. A large range of ring land widths and tensions, liner finish, and oil viscosity were investigated both experimentally and numerically to reveal how different factors affect the piston ring friction. A floating liner engine (FLE) was modified for motoring tests. The engine system repeatability and self-consistency were demonstrated. The thesis then discussed proper methods to select and measure the rings, liners and oil, which were important to generating meaningful results from the experiment. The ranges of engine speeds and liner temperatures were designed to ensure that all the lubrication modes, namely, boundary, mixed and hydrodynamic, can become dominant in both the instantaneous friction over a cycle and the FMEP over the engine speed range for any combination of rings, liners and lubricants. A parallel modeling effort was made to the experiments. The work showed that with careful preparation of adequate information on rings, liners and lubricants, the model can match the friction trends observed in the experiment over a large range of operating parameters and designs on the ring, liner finish and lubricant viscosity. The ring friction change over the liner break-in was studied using liners covering a wide range of surface roughness. The hydrodynamic pressure generation ability of the liner appears to be decided by the large surface structure. Therefore, the break-in process, which removes individual asperities from the plateau, does not affect pure hydrodynamic lubrication, and only the mixed lubrication is affected by the plateau roughness change. By keeping the same hydrodynamic pressure - ring/liner clearance (P-h) correlation and changing the plateau roughness, the model can predict the ring friction change over different lubrication regimes during the break-in. Compared to the current industry norm, a new engine power cylinder system design using a smaller land width twin land oil control ring with a lower ring tension and accompanied by a smoother liner surface gives lower friction and better oil control at the same time."
}, {
    "id": "oai:dspace.mit.edu:1721.1/36176",
    "title": "Quantifying and modelling adaptive astronaut movement : motion strategies for long-duration spaceflight missions",
    "abstract": "Past spaceflight experience has shown that astronauts adapt their motor control strategies to microgravity movements after approximately four weeks of microgravity exposure. A similar (but typically shorter) re-adaptation period is required upon return to Earth or partial gravity environment such as the Moon or Mars. During these adaptation periods, astronaut performance is considerably degraded and can lead to falls and mission-threatening injuries. This dissertation describes a research program to quantitatively study the dynamics and control aspects of human motor control adaptation to a spectrum of gravity environments. The key hypotheses of this research were that a) locomotor control adaptation could be observed following short exposure (on the order of hours) to a different dynamic environment and b) the observed adaptation could be predicted using a single model that applied to a spectrum of gravitational environments. Experiments were conducted on a 1-G air-bearing floor microgravity simulator and underwater to provide contrasting dynamic and gravitational environments. Subjects performed leg push-offs and hand landings to demonstrate their control strategies as they adapted.",
    "advisors": ["Dava J. Newman"],
    "text": "Quantifying and modelling adaptive astronaut movement : motion strategies for long-duration spaceflight missions Past spaceflight experience has shown that astronauts adapt their motor control strategies to microgravity movements after approximately four weeks of microgravity exposure. A similar (but typically shorter) re-adaptation period is required upon return to Earth or partial gravity environment such as the Moon or Mars. During these adaptation periods, astronaut performance is considerably degraded and can lead to falls and mission-threatening injuries. This dissertation describes a research program to quantitatively study the dynamics and control aspects of human motor control adaptation to a spectrum of gravity environments. The key hypotheses of this research were that a) locomotor control adaptation could be observed following short exposure (on the order of hours) to a different dynamic environment and b) the observed adaptation could be predicted using a single model that applied to a spectrum of gravitational environments. Experiments were conducted on a 1-G air-bearing floor microgravity simulator and underwater to provide contrasting dynamic and gravitational environments. Subjects performed leg push-offs and hand landings to demonstrate their control strategies as they adapted."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28916",
    "title": "Characterization and design of non-adiabatic micro-compressor impeller and preliminary design of self-sustained micro engine system",
    "abstract": "As part of the MIT research program on micro-engines (of size [approximately] 1 cm), this thesis defines concepts and designs to improve micro-turbomachinery and overall system performance. Three-dimensional Reynolds-averaged Navier-Stokes computations (FLUENT) have been carried out to quantify the performance limiting processes in micro-impellers. These processes include (i) heat transfer to the compressor flow responsible for up to 25 points efficiency penalty, (ii) impeller casing drag (17 points penalty) and (iii) passage boundary layer loss (10 points penalty). The magnitude of the first effect is a result of the engine small length scale selection and is characterized by the total heat to impeller flow as fraction of inlet flow enthalpy. The magnitudes of the last two effects can be attributed to low Reynolds number. Scaling laws for elucidating the parametric controlling trend in these effects have been formulated. A mean-line analysis and design tool based on the above micro-impeller characterization is developed to formulate design guidelines. The guidelines show that the optimal micro-impeller geometry changes with impeller wall temperature, an effect, not present for large turbomachinery. In particular, impeller inlet angle, back-sweep angle, solidity and radial size for peak efficiency decrease with increasing impeller wall temperature. This behavior is a result of the competing effects of geometry on (i) aerodynamic loss and (ii) on heat transfer to impeller flow. In accord with these findings, CFD calculations show that configuring a micro-impeller excluding the heat addition as a design variable can incur a penalty of more than 10 efficiency points. An aero-thermal system model is developed to enable micro-engine system analysis and",
    "advisors": ["Choon-Sooi Tan"],
    "text": "Characterization and design of non-adiabatic micro-compressor impeller and preliminary design of self-sustained micro engine system As part of the MIT research program on micro-engines (of size [approximately] 1 cm), this thesis defines concepts and designs to improve micro-turbomachinery and overall system performance. Three-dimensional Reynolds-averaged Navier-Stokes computations (FLUENT) have been carried out to quantify the performance limiting processes in micro-impellers. These processes include (i) heat transfer to the compressor flow responsible for up to 25 points efficiency penalty, (ii) impeller casing drag (17 points penalty) and (iii) passage boundary layer loss (10 points penalty). The magnitude of the first effect is a result of the engine small length scale selection and is characterized by the total heat to impeller flow as fraction of inlet flow enthalpy. The magnitudes of the last two effects can be attributed to low Reynolds number. Scaling laws for elucidating the parametric controlling trend in these effects have been formulated. A mean-line analysis and design tool based on the above micro-impeller characterization is developed to formulate design guidelines. The guidelines show that the optimal micro-impeller geometry changes with impeller wall temperature, an effect, not present for large turbomachinery. In particular, impeller inlet angle, back-sweep angle, solidity and radial size for peak efficiency decrease with increasing impeller wall temperature. This behavior is a result of the competing effects of geometry on (i) aerodynamic loss and (ii) on heat transfer to impeller flow. In accord with these findings, CFD calculations show that configuring a micro-impeller excluding the heat addition as a design variable can incur a penalty of more than 10 efficiency points. An aero-thermal system model is developed to enable micro-engine system analysis and"
}, {
    "id": "oai:dspace.mit.edu:1721.1/63043",
    "title": "Gradient projection anti-windup scheme",
    "abstract": "It is a well-recognized fact that control saturation affects virtually all practical control systems. It leads to controller windup, which degrades/limits the system's closed-loop performance, and may cause catastrophic failures if it induces instability. Anti-windup compensation is one of two main approaches to mitigate the effects of windup, and is conceptually and practically attractive. For the idealized case of constrained linear time invariant (LTI) plants driven by LTI controllers, numerous anti-windup schemes exist. However, most practical control systems are inherently nonlinear, and anti-windup compensation for nonlinear systems remains largely an open problem. To this end, we propose the gradient projection anti-windup (GPAW) scheme, which is an extension of the conditional integration method to multi-input-multi-output (MIMO) nonlinear systems, using Rosen's gradient projection method for nonlinear programming. It achieves controller state-output consistency by projecting the controller state onto the unsaturated region induced by the control saturation constraints. The GPAW-compensated controller is a hybrid controller defined by the online solution to either a combinatorial optimization subproblem, a convex quadratic program, or a projection onto a convex polyhedral cone problem. We show that the GPAW-compensated system is obtained by modifying the uncompensated system with a passive operator. Qualitative weaknesses of some existing anti-windup results are established, which motivated a new paradigm to address the anti-windup problem. It is shown that for a constrained first order LTI plant driven by a first order LTI controller, GPAW compensation can only maintain/enlarge its region of attraction (ROA). In this new paradigm, we derived some ROA comparison and stability results for MIMO nonlinear as well as MIMO LTI systems. The thesis is not that the GPAW scheme solves a centuries-old open problem of immense practical importance, but rather, that it provides a potential path to a solution. We invite the reader to join us in this quest at the confluence of nonlinear systems, hybrid systems, projected dynamical systems, differential equations with discontinuous right-hand sides, combinatorial optimization, convex analysis and optimization, and passive systems.",
    "advisors": ["Jonathan Patrick How"],
    "text": "Gradient projection anti-windup scheme It is a well-recognized fact that control saturation affects virtually all practical control systems. It leads to controller windup, which degrades/limits the system's closed-loop performance, and may cause catastrophic failures if it induces instability. Anti-windup compensation is one of two main approaches to mitigate the effects of windup, and is conceptually and practically attractive. For the idealized case of constrained linear time invariant (LTI) plants driven by LTI controllers, numerous anti-windup schemes exist. However, most practical control systems are inherently nonlinear, and anti-windup compensation for nonlinear systems remains largely an open problem. To this end, we propose the gradient projection anti-windup (GPAW) scheme, which is an extension of the conditional integration method to multi-input-multi-output (MIMO) nonlinear systems, using Rosen's gradient projection method for nonlinear programming. It achieves controller state-output consistency by projecting the controller state onto the unsaturated region induced by the control saturation constraints. The GPAW-compensated controller is a hybrid controller defined by the online solution to either a combinatorial optimization subproblem, a convex quadratic program, or a projection onto a convex polyhedral cone problem. We show that the GPAW-compensated system is obtained by modifying the uncompensated system with a passive operator. Qualitative weaknesses of some existing anti-windup results are established, which motivated a new paradigm to address the anti-windup problem. It is shown that for a constrained first order LTI plant driven by a first order LTI controller, GPAW compensation can only maintain/enlarge its region of attraction (ROA). In this new paradigm, we derived some ROA comparison and stability results for MIMO nonlinear as well as MIMO LTI systems. The thesis is not that the GPAW scheme solves a centuries-old open problem of immense practical importance, but rather, that it provides a potential path to a solution. We invite the reader to join us in this quest at the confluence of nonlinear systems, hybrid systems, projected dynamical systems, differential equations with discontinuous right-hand sides, combinatorial optimization, convex analysis and optimization, and passive systems."
}, {
    "id": "oai:dspace.mit.edu:1721.1/115653",
    "title": "Network navigation with scheduling",
    "abstract": "Network navigation is a promising paradigm for enabling location-awareness in dynamic wireless networks. A wireless navigation network consists of agents (mobile with unknown locations) and anchors (possibly mobile with known locations). An agent can estimate its locations based on inter- and intra-node measurements, as well as prior knowledge. In the presence of limited wireless resources, only a subset rather than all of the node pairs can perform inter-node measurements at a time. The procedure of selecting node pairs at different time instants for inter-node measurements, referred to as network scheduling, affects the time evolution of agents' localization errors. The key to achieve high navigation accuracy and efficient channel usage is to maximize the benefit from agents' inter-node measurements. Therefore, it is critical to design scheduling algorithms that decide for each agent with whom and when to perform inter-node measurements. This thesis introduces situation-aware scheduling that exploits network states to adaptively schedule agents' inter-node measurements. In particular, an analytical framework is developed to determine the effects of scheduling strategies and network settings on the localization error evolution. Furthermore, efficient and distributed situation-aware scheduling algorithms tailored for wireless navigation networks are designed, leading to high navigation accuracy and efficient channel usage. The first part of the thesis develops an analytical framework to determine the localization error evolution as a function of scheduling algorithms and network settings. In particular, both sufficient and necessary conditions for the boundedness of the error evolution are provided. Furthermore, opportunistic and random situation-aware scheduling strategies are proposed, and bounds on the corresponding time-averaged network localization errors are derived. These strategies are proved to be optimal in terms of the error scaling with the number of agents. Finally, the navigation accuracy is shown to be improved by sharing the wireless resources among multiple measurement pairs instead of allocating all the resources to a single pair at a time. The second part of the thesis designs efficient slotted and unslotted situation-aware scheduling algorithms tailored for wireless navigation networks based on the analytical results from the first part. The algorithm parameters, such as access probabilities and access rates, are optimized based on bounds for the time-averaged network localization error (NLE). The proposed algorithms lead to significant performance improvement compared with scheduling algorithms from wireless communication networks. The third part of the thesis develops a framework for the design of random-access-based distributed and asynchronous scheduling algorithms for wireless navigation networks, in which the channel access probabilities are optimized based on the evolution of agents' localization errors. The proposed algorithm achieves higher navigation accuracy and more efficient channel usage than the commonly used carrier sensing multiple access (CSMA) algorithm from wireless communication networks, at the cost of minimal communication overhead and computational complexity. The performance improvement is shown via numerical and experimental results. The contributions of this thesis provide a framework for the analysis and design of scheduling algorithms for wireless navigation networks, leading to high-accuracy, efficient, and flexible network navigation.",
    "advisors": ["Moe Z. Win"],
    "text": "Network navigation with scheduling Network navigation is a promising paradigm for enabling location-awareness in dynamic wireless networks. A wireless navigation network consists of agents (mobile with unknown locations) and anchors (possibly mobile with known locations). An agent can estimate its locations based on inter- and intra-node measurements, as well as prior knowledge. In the presence of limited wireless resources, only a subset rather than all of the node pairs can perform inter-node measurements at a time. The procedure of selecting node pairs at different time instants for inter-node measurements, referred to as network scheduling, affects the time evolution of agents' localization errors. The key to achieve high navigation accuracy and efficient channel usage is to maximize the benefit from agents' inter-node measurements. Therefore, it is critical to design scheduling algorithms that decide for each agent with whom and when to perform inter-node measurements. This thesis introduces situation-aware scheduling that exploits network states to adaptively schedule agents' inter-node measurements. In particular, an analytical framework is developed to determine the effects of scheduling strategies and network settings on the localization error evolution. Furthermore, efficient and distributed situation-aware scheduling algorithms tailored for wireless navigation networks are designed, leading to high navigation accuracy and efficient channel usage. The first part of the thesis develops an analytical framework to determine the localization error evolution as a function of scheduling algorithms and network settings. In particular, both sufficient and necessary conditions for the boundedness of the error evolution are provided. Furthermore, opportunistic and random situation-aware scheduling strategies are proposed, and bounds on the corresponding time-averaged network localization errors are derived. These strategies are proved to be optimal in terms of the error scaling with the number of agents. Finally, the navigation accuracy is shown to be improved by sharing the wireless resources among multiple measurement pairs instead of allocating all the resources to a single pair at a time. The second part of the thesis designs efficient slotted and unslotted situation-aware scheduling algorithms tailored for wireless navigation networks based on the analytical results from the first part. The algorithm parameters, such as access probabilities and access rates, are optimized based on bounds for the time-averaged network localization error (NLE). The proposed algorithms lead to significant performance improvement compared with scheduling algorithms from wireless communication networks. The third part of the thesis develops a framework for the design of random-access-based distributed and asynchronous scheduling algorithms for wireless navigation networks, in which the channel access probabilities are optimized based on the evolution of agents' localization errors. The proposed algorithm achieves higher navigation accuracy and more efficient channel usage than the commonly used carrier sensing multiple access (CSMA) algorithm from wireless communication networks, at the cost of minimal communication overhead and computational complexity. The performance improvement is shown via numerical and experimental results. The contributions of this thesis provide a framework for the analysis and design of scheduling algorithms for wireless navigation networks, leading to high-accuracy, efficient, and flexible network navigation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101444",
    "title": "Optical communications for small satellites",
    "abstract": "Small satellites, particularly CubeSats, have become popular platforms for a wide variety of scientific, commercial and military remote sensing applications. Inexpensive commercial o the shelf (COTS) hardware and relatively low launch costs make these platforms candidates for deployment in large constellations that can offer unprecedented temporal and geospatial sampling of the entire planet. However, productivity for both individual and constellations of CubeSats in low earth orbit (LEO) is limited by the capabilities of the communications subsystem. Generally, these constraints stem from limited available electrical power, low-gain antennas and the general scarcity of available radio spectrum. In this thesis, we assess the ability of free space optical communication (lasercom) to address these limitations, identify key technology developments that enable its application in small satellites, and develop a functional prototype that demonstrates predicted performance. We first establish design goals for a lasercom payload archi- tecture that offers performance improvements (joules-per-bit) over radio-frequency (RF) solutions, yet is compatible with the severe size, weight and power (SWaP) constraints common to CubeSats. The key design goal is direct LEO-to-ground downlink capability with data rates exceeding 10 Mbps, an order of magnitude better than COTS radio solutions available today, within typical CubeSat SWaP constraints on the space terminal, and with similar COTS and low-complexity constraints on the ground terminal. After defining the goals for this architecture, we identify gaps in previous implementations that limit their performance: the lack of compact, power-efficient optical transmitters and the need for pointing capability on small satellites to be as much as a factor of ten better than what is commonly achieved today. One approach is to address these shortcomings using low-cost COTS components that are compatible with CubeSat budgets and development schedules. In design trade studies we identify potential solutions for the transmitter and pointing implementation gaps. Two distinct transmitter architectures, one based on a high-power laser diode and another using an optical amplifier, are considered. Analysis shows that both configurations meet system requirements, however, the optical amplifier offers better scalability to higher data rates. To address platform pointing limitations, we dene a staged control framework incorporating a COTS optical steering mechanism that is used to manage pointing errors from the coarse stage (host satellite body-pointing). A variety of ne steering solutions are considered, and microelectromechanical systems (MEMS) tip-tilt mirrors are selected due to their advantage in size, weight and power. We experimentally validate the designs resulting from the trade studies for these key subsystems. We construct a prototype transmitter using a modified COTS fiber amplifier and a directly-modulated seed laser capable of producing a 200mW average power, pulse position modulated optical output. This prototype is used to confirm power consumption predictions, modulation rate scalability (10 Mbps to 100 Mbps), and peak transmit power (e.g., 24.6W for PPM-128). The transmitter optical output, along with a simple loopback receiver, is used to validate the sensitivity of the avalanche photodiode receiver used for the ground receiver in the flight experiment configuration. The MEMS fine steering mechanisms, which are not rated for space use, are characterized using a purpose-built test apparatus. Characterization experiments of the MEMS devices focused on ensuring repeatable behavior (+/-0:11 mrad, 3-[sigma]) over the expected operating temperature range on the spacecraft (0°C to 40°C). Finally, we provide an assessment of the work that remains to move from the prototype to flight model and into on-orbit operations. Space terminal packaging and integration needs, as well as host spacecraft interface requirements are detailed. We also describe the remaining ground station integration tasks and operational procedures. Having developed a pragmatic COTS-based lasercom architecture for CubeSats, and having addressed the need for a compact laser transmitter and optical ne steering mechanisms with both analysis and experimental validation, this thesis has set the stage for the practical use of lasercom techniques in resource-constrained CubeSats which can yield order-of-magnitude enhancements in communications link eciency relative to existing RF technologies currently in use.",
    "advisors": ["Kerri L. Cahoy"],
    "text": "Optical communications for small satellites Small satellites, particularly CubeSats, have become popular platforms for a wide variety of scientific, commercial and military remote sensing applications. Inexpensive commercial o the shelf (COTS) hardware and relatively low launch costs make these platforms candidates for deployment in large constellations that can offer unprecedented temporal and geospatial sampling of the entire planet. However, productivity for both individual and constellations of CubeSats in low earth orbit (LEO) is limited by the capabilities of the communications subsystem. Generally, these constraints stem from limited available electrical power, low-gain antennas and the general scarcity of available radio spectrum. In this thesis, we assess the ability of free space optical communication (lasercom) to address these limitations, identify key technology developments that enable its application in small satellites, and develop a functional prototype that demonstrates predicted performance. We first establish design goals for a lasercom payload archi- tecture that offers performance improvements (joules-per-bit) over radio-frequency (RF) solutions, yet is compatible with the severe size, weight and power (SWaP) constraints common to CubeSats. The key design goal is direct LEO-to-ground downlink capability with data rates exceeding 10 Mbps, an order of magnitude better than COTS radio solutions available today, within typical CubeSat SWaP constraints on the space terminal, and with similar COTS and low-complexity constraints on the ground terminal. After defining the goals for this architecture, we identify gaps in previous implementations that limit their performance: the lack of compact, power-efficient optical transmitters and the need for pointing capability on small satellites to be as much as a factor of ten better than what is commonly achieved today. One approach is to address these shortcomings using low-cost COTS components that are compatible with CubeSat budgets and development schedules. In design trade studies we identify potential solutions for the transmitter and pointing implementation gaps. Two distinct transmitter architectures, one based on a high-power laser diode and another using an optical amplifier, are considered. Analysis shows that both configurations meet system requirements, however, the optical amplifier offers better scalability to higher data rates. To address platform pointing limitations, we dene a staged control framework incorporating a COTS optical steering mechanism that is used to manage pointing errors from the coarse stage (host satellite body-pointing). A variety of ne steering solutions are considered, and microelectromechanical systems (MEMS) tip-tilt mirrors are selected due to their advantage in size, weight and power. We experimentally validate the designs resulting from the trade studies for these key subsystems. We construct a prototype transmitter using a modified COTS fiber amplifier and a directly-modulated seed laser capable of producing a 200mW average power, pulse position modulated optical output. This prototype is used to confirm power consumption predictions, modulation rate scalability (10 Mbps to 100 Mbps), and peak transmit power (e.g., 24.6W for PPM-128). The transmitter optical output, along with a simple loopback receiver, is used to validate the sensitivity of the avalanche photodiode receiver used for the ground receiver in the flight experiment configuration. The MEMS fine steering mechanisms, which are not rated for space use, are characterized using a purpose-built test apparatus. Characterization experiments of the MEMS devices focused on ensuring repeatable behavior (+/-0:11 mrad, 3-[sigma]) over the expected operating temperature range on the spacecraft (0°C to 40°C). Finally, we provide an assessment of the work that remains to move from the prototype to flight model and into on-orbit operations. Space terminal packaging and integration needs, as well as host spacecraft interface requirements are detailed. We also describe the remaining ground station integration tasks and operational procedures. Having developed a pragmatic COTS-based lasercom architecture for CubeSats, and having addressed the need for a compact laser transmitter and optical ne steering mechanisms with both analysis and experimental validation, this thesis has set the stage for the practical use of lasercom techniques in resource-constrained CubeSats which can yield order-of-magnitude enhancements in communications link eciency relative to existing RF technologies currently in use."
}, {
    "id": "oai:dspace.mit.edu:1721.1/90602",
    "title": "Mechanical counter-pressure space suit design using active materials",
    "abstract": "Mechanical counter-pressure (MCP) space suits have the potential to greatly improve the mobility of astronauts as they conduct planetary exploration activities; however, the underlying technologies required to provide uniform compression in an MCP garment at sucient pressures (29.6 kPa) for space exploration have not yet been demonstrated, and donning and dong of such a suit remains a signicant challenge. This research effort focuses on the novel use of active material technologies to produce a garment with controllable compression capabilities to address these problems. We the describe the modeling, development, and testing of low spring index (C = 3) nickel titanium (NiTi) shape memory alloy (SMA) coil actuators designed for use in wearable compression garments. Several actuators were manufactured, annealed, and tested to assess their de-twinning and activation characteristics. We then describe the derivation and development of a complete two-spring model to predict the performance of hybrid compression textiles combining passive elastic fabrics and integrated SMA coil actuators based on 11 design parameters. Design studies (including two specifically tailored for MCP applications) are presented using the derived model to demonstrate the range of possible garment performance outcomes based on strategically chosen SMA and material parameters. Finally, we present a novel methodology for producing modular 3D-printed SMA actuator cartridges designed for use in compression garments, and test 5 active tourniquet prototypes (made using these cartridges and commercially available fabrics) to assess the eect of SMA actuation on the tourniquet compression characteristics. Our results demonstrate that hybrid active tourniquet prototypes are highly effective, with counter-pressures increasing by an average of 81.9% when activated (taking an average of only 23.7 seconds to achieve steady state). Maximum average counter-pressures reached 34.3 kPa, achieving 115.9% of the target MCP counter-pressure. We observed signicant spatial variability in the active counter-pressure profiles, stemming from high friction, asymmetric fabric stretching, and near-field pressure spikes/voids caused by the SMA cartridge. Modifications to reduce tourniquet friction were effective at mitigating a proportion of this variability. System performance and repeatability were found to depend heavily on the passive fabric characteristics, with performance losses attributable to irrecoverable fabric strain, degradation in fabric elastic modulus, and non-linear modulus behavior. The results of this research open the door to new opportunities to advance the field of MCP spacesuit design, as well as opportunities to improve compression garments used in healthcare therapies, competitive athletics, and battlefield medicine.",
    "advisors": ["Dava J. Newman"],
    "text": "Mechanical counter-pressure space suit design using active materials Mechanical counter-pressure (MCP) space suits have the potential to greatly improve the mobility of astronauts as they conduct planetary exploration activities; however, the underlying technologies required to provide uniform compression in an MCP garment at sucient pressures (29.6 kPa) for space exploration have not yet been demonstrated, and donning and dong of such a suit remains a signicant challenge. This research effort focuses on the novel use of active material technologies to produce a garment with controllable compression capabilities to address these problems. We the describe the modeling, development, and testing of low spring index (C = 3) nickel titanium (NiTi) shape memory alloy (SMA) coil actuators designed for use in wearable compression garments. Several actuators were manufactured, annealed, and tested to assess their de-twinning and activation characteristics. We then describe the derivation and development of a complete two-spring model to predict the performance of hybrid compression textiles combining passive elastic fabrics and integrated SMA coil actuators based on 11 design parameters. Design studies (including two specifically tailored for MCP applications) are presented using the derived model to demonstrate the range of possible garment performance outcomes based on strategically chosen SMA and material parameters. Finally, we present a novel methodology for producing modular 3D-printed SMA actuator cartridges designed for use in compression garments, and test 5 active tourniquet prototypes (made using these cartridges and commercially available fabrics) to assess the eect of SMA actuation on the tourniquet compression characteristics. Our results demonstrate that hybrid active tourniquet prototypes are highly effective, with counter-pressures increasing by an average of 81.9% when activated (taking an average of only 23.7 seconds to achieve steady state). Maximum average counter-pressures reached 34.3 kPa, achieving 115.9% of the target MCP counter-pressure. We observed signicant spatial variability in the active counter-pressure profiles, stemming from high friction, asymmetric fabric stretching, and near-field pressure spikes/voids caused by the SMA cartridge. Modifications to reduce tourniquet friction were effective at mitigating a proportion of this variability. System performance and repeatability were found to depend heavily on the passive fabric characteristics, with performance losses attributable to irrecoverable fabric strain, degradation in fabric elastic modulus, and non-linear modulus behavior. The results of this research open the door to new opportunities to advance the field of MCP spacesuit design, as well as opportunities to improve compression garments used in healthcare therapies, competitive athletics, and battlefield medicine."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101492",
    "title": "Fluid mechanics of ported shroud centrifugal compressor for vehicular turbocharger applications",
    "abstract": "This thesis presents an investigation of the effects of Ported Shroud (PS) self-recirculating casing treatment used in turbocharger centrifugal compressors for increasing the operable range. Computed results, assessed with experimental measurements on ported and non-ported variants of a representative turbocharger compressor, are used to determine the impact of the PS on the flow field and hence performance. It is shown that the main flow path perceives the PS flow as a combination of flow actuations that include injection and removal of mass flow, and injection of axial momentum and tangential momentum. A computational model in which the presence of the PS is replaced by imposed boundary conditions that reflect the individual flow actuations has thus been formulated and implemented. The removal of a fraction of the inducer mass flow was determined to be the dominant flow actuation in setting the performance of PS compressors. Mass flow removal reduces the flow blockage associated with the impeller tip leakage flow and increases the diffusion in the main flow path. Adding swirl to the injected flow in the direction opposite of the wheel rotation results in an increase of the stagnation pressure ratio and a decrease of the efficiency. The loss generation in the flow path has been defined to rationalize efficiency changes associated with PS operation.",
    "advisors": ["Choon S. Tan"],
    "text": "Fluid mechanics of ported shroud centrifugal compressor for vehicular turbocharger applications This thesis presents an investigation of the effects of Ported Shroud (PS) self-recirculating casing treatment used in turbocharger centrifugal compressors for increasing the operable range. Computed results, assessed with experimental measurements on ported and non-ported variants of a representative turbocharger compressor, are used to determine the impact of the PS on the flow field and hence performance. It is shown that the main flow path perceives the PS flow as a combination of flow actuations that include injection and removal of mass flow, and injection of axial momentum and tangential momentum. A computational model in which the presence of the PS is replaced by imposed boundary conditions that reflect the individual flow actuations has thus been formulated and implemented. The removal of a fraction of the inducer mass flow was determined to be the dominant flow actuation in setting the performance of PS compressors. Mass flow removal reduces the flow blockage associated with the impeller tip leakage flow and increases the diffusion in the main flow path. Adding swirl to the injected flow in the direction opposite of the wheel rotation results in an increase of the stagnation pressure ratio and a decrease of the efficiency. The loss generation in the flow path has been defined to rationalize efficiency changes associated with PS operation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107550",
    "title": "Enhancement of perception with the application of stochastic vestibular stimulation",
    "abstract": "Astronauts experience sensorimotor changes during spaceflight, which may degrade their operational capabilities. A sensorimotor countermeasure that mitigates these effects would improve crewmember safety and decrease mission risk. The goal of this research was to investigate the potential use of electrical stochastic vestibular stimulation (SVS) as a sensorimotor aid to lower vestibular thresholds and improve manual control. We hypothesized that low-level, subsensory bandlimited white noise (±200 - ±700 [mu]A peak, 52 - 182 ±5% [mu]A RMS) passed between surface electrodes located on the mastoid bone behind each ear would enhance perceptual sensitivity to physical motions due to the phenomenon of stochastic resonance (SR). The goals of this research were to 1) demonstrate that SVS can significantly reduce vestibular direction recognition thresholds, 2) investigate whether the SR phenomenon involves stimulation of the semicircular canals, otoliths or both, and 3) demonstrate that SVS can improve perception-based manual control performance in a task relevant to piloting during planetary landing, docking, or other vehicle maneuvers. In a first experiment, upright roll tilt direction recognition thresholds, which elicit responses from both the semicircular canals and otoliths, were measured with varying levels of SVS applied. Upright roll tilt direction recognition thresholds exhibited characteristic SR dependency on current level that was statistically significant in 6/12 subjects. However, SR repeatability across days was found to be weak, only present in 3/12 subjects. In a second experiment, supine roll rotation (primarily stimulating the semicircular canals) and inter-aural translation (primarily stimulating the otoliths) direction recognition thresholds were measured with varying levels of SVS applied. SR was exhibited in inter-aural translation (6/11 subjects) but not supine roll rotation (1/12 subjects), suggesting that stimulation of the otolith organs may be vital to vestibular perceptual SR. Simulations of the experimental test procedure were used to create a dataset of direction recognition thresholds with several predefined underlying SR exhibition levels. A comparison of the results form Experiment I and 2 to the simulated datasets allowed us to more confidently draw conclusions from data that are prone to the existence of false positives. In a third experiment, subjects used vestibular information to null out pseudo-random vehicle disturbance in a manual control task. SVS (±300 [mu]A) improved the group mean position variability when motions were near threshold and extended the range of frequencies in which motions could be nulled out. The results of this thesis are consistent with the concept that SVS is able to extend the operating range of the vestibular perceptual system in some individuals. In the context of human spaceflight, results from this research further our understanding of how SVS may be implemented in the future as a component of a comprehensive spaceflight countermeasure plan.",
    "advisors": ["Charles Oman"],
    "text": "Enhancement of perception with the application of stochastic vestibular stimulation Astronauts experience sensorimotor changes during spaceflight, which may degrade their operational capabilities. A sensorimotor countermeasure that mitigates these effects would improve crewmember safety and decrease mission risk. The goal of this research was to investigate the potential use of electrical stochastic vestibular stimulation (SVS) as a sensorimotor aid to lower vestibular thresholds and improve manual control. We hypothesized that low-level, subsensory bandlimited white noise (±200 - ±700 [mu]A peak, 52 - 182 ±5% [mu]A RMS) passed between surface electrodes located on the mastoid bone behind each ear would enhance perceptual sensitivity to physical motions due to the phenomenon of stochastic resonance (SR). The goals of this research were to 1) demonstrate that SVS can significantly reduce vestibular direction recognition thresholds, 2) investigate whether the SR phenomenon involves stimulation of the semicircular canals, otoliths or both, and 3) demonstrate that SVS can improve perception-based manual control performance in a task relevant to piloting during planetary landing, docking, or other vehicle maneuvers. In a first experiment, upright roll tilt direction recognition thresholds, which elicit responses from both the semicircular canals and otoliths, were measured with varying levels of SVS applied. Upright roll tilt direction recognition thresholds exhibited characteristic SR dependency on current level that was statistically significant in 6/12 subjects. However, SR repeatability across days was found to be weak, only present in 3/12 subjects. In a second experiment, supine roll rotation (primarily stimulating the semicircular canals) and inter-aural translation (primarily stimulating the otoliths) direction recognition thresholds were measured with varying levels of SVS applied. SR was exhibited in inter-aural translation (6/11 subjects) but not supine roll rotation (1/12 subjects), suggesting that stimulation of the otolith organs may be vital to vestibular perceptual SR. Simulations of the experimental test procedure were used to create a dataset of direction recognition thresholds with several predefined underlying SR exhibition levels. A comparison of the results form Experiment I and 2 to the simulated datasets allowed us to more confidently draw conclusions from data that are prone to the existence of false positives. In a third experiment, subjects used vestibular information to null out pseudo-random vehicle disturbance in a manual control task. SVS (±300 [mu]A) improved the group mean position variability when motions were near threshold and extended the range of frequencies in which motions could be nulled out. The results of this thesis are consistent with the concept that SVS is able to extend the operating range of the vestibular perceptual system in some individuals. In the context of human spaceflight, results from this research further our understanding of how SVS may be implemented in the future as a component of a comprehensive spaceflight countermeasure plan."
}, {
    "id": "oai:dspace.mit.edu:1721.1/17833",
    "title": "Human-centered systems analysis of aircraft separation from adverse weather",
    "abstract": "Adverse weather significantly impacts the safety and efficiency of flight operations. Weather information plays a key role in mitigating the impact of adverse weather on flight operations by supporting air transportation decision-makers' awareness of operational and mission risks. The emergence of new technologies for the surveillance, modeling, dissemination and presentation of information provides opportunities for improving both weather information and user decision-making. In order to support the development of new weather information systems, it is important to understand this complex problem thoroughly. This thesis applies a human-centered systems engineering approach to study the problem of separating aircraft from adverse weather. The approach explicitly considers the role of the human operator as part of the larger operational system. A series of models describing the interaction of the key elements of the adverse aircraft-weather encounter problem and a framework that characterizes users' temporal decision- making were developed. Another framework that better matches pilots' perspectives compared to traditional forecast verification methods articulated the value of forecast valid time according to a space-time reference frame. The models and frameworks were validated using focused interviews with ten national subject matter experts in aviation meteorology or flight operations. The experts unanimously supported the general structure of the models and made suggestions on clarifications and refinements which were integrated in the final models. In addition, a cognitive walk-through of three adverse aircraft-weather encounters was conducted to provide an experiential perspective on the aviation weather problem.",
    "advisors": ["R. John Hansman"],
    "text": "Human-centered systems analysis of aircraft separation from adverse weather Adverse weather significantly impacts the safety and efficiency of flight operations. Weather information plays a key role in mitigating the impact of adverse weather on flight operations by supporting air transportation decision-makers' awareness of operational and mission risks. The emergence of new technologies for the surveillance, modeling, dissemination and presentation of information provides opportunities for improving both weather information and user decision-making. In order to support the development of new weather information systems, it is important to understand this complex problem thoroughly. This thesis applies a human-centered systems engineering approach to study the problem of separating aircraft from adverse weather. The approach explicitly considers the role of the human operator as part of the larger operational system. A series of models describing the interaction of the key elements of the adverse aircraft-weather encounter problem and a framework that characterizes users' temporal decision- making were developed. Another framework that better matches pilots' perspectives compared to traditional forecast verification methods articulated the value of forecast valid time according to a space-time reference frame. The models and frameworks were validated using focused interviews with ten national subject matter experts in aviation meteorology or flight operations. The experts unanimously supported the general structure of the models and made suggestions on clarifications and refinements which were integrated in the final models. In addition, a cognitive walk-through of three adverse aircraft-weather encounters was conducted to provide an experiential perspective on the aviation weather problem."
}, {
    "id": "oai:dspace.mit.edu:1721.1/77100",
    "title": "Robust distributed planning strategies for autonomous multi-agent teams",
    "abstract": "The increased use of autonomous robotic agents, such as unmanned aerial vehicles (UAVs) and ground rovers, for complex missions has motivated the development of autonomous task allocation and planning methods that ensure spatial and temporal coordination for teams of cooperating agents. The basic problem can be formulated as a combinatorial optimization (mixed-integer program) involving nonlinear and time-varying system dynamics. For most problems of interest, optimal solution methods are computationally intractable (NP-Hard), and centralized planning approaches, which usually require high bandwidth connections with a ground station (e.g. to transmit received sensor data, and to dispense agent plans), are resource intensive and react slowly to local changes in dynamic environments. Distributed approximate algorithms, where agents plan individually and coordinate with each other locally through consensus protocols, can alleviate many of these issues and have been successfully used to develop real-time conflict-free solutions for heterogeneous networked teams. An important issue associated with autonomous planning is that many of the algorithms rely on underlying system models and parameters which are often subject to uncertainty. This uncertainty can result from many sources including: inaccurate modeling due to simplifications, assumptions, and/or parameter errors; fundamentally nondeterministic processes (e.g. sensor readings, stochastic dynamics); and dynamic local information changes. As discrepancies between the planner models and the actual system dynamics increase, mission performance typically degrades. The impact of these discrepancies on the overall quality of the plan is usually hard to quantify in advance due to nonlinear effects, coupling between tasks and agents, and interdependencies between system constraints. However, if uncertainty models of planning parameters are available, they can be leveraged to create robust plans that explicitly hedge against the inherent uncertainty given allowable risk thresholds. This thesis presents real-time robust distributed planning strategies that can be used to plan for multi-agent networked teams operating in stochastic and dynamic environments. One class of distributed combinatorial planning algorithms involves using auction algorithms augmented with consensus protocols to allocate tasks amongst a team of agents while resolving conflicting assignments locally between the agents. A particular algorithm in this class is the Consensus-Based Bundle Algorithm (CBBA), a distributed auction protocol that guarantees conflict-free solutions despite inconsistencies in situational awareness across the team. CBBA runs in polynomial time, demonstrating good scalability with increasing numbers of agents and tasks. This thesis builds upon the CBBA framework to address many realistic considerations associated with planning for networked teams, including time-critical mission constraints, limited communication between agents, and stochastic operating environments. A particular focus of this work is a robust extension to CBBA that handles distributed planning in stochastic environments given probabilistic parameter models and different stochastic metrics. The Robust CBBA algorithm proposed in this thesis provides a distributed real-time framework which can leverage different stochastic metrics to hedge against parameter uncertainty. In mission scenarios where low probability of failure is required, a chance-constrained stochastic metric can be used to provide probabilistic guarantees on achievable mission performance given allowable risk thresholds. This thesis proposes a distributed chance-constrained approximation that can be used within the Robust CBBA framework, and derives constraints on individual risk allocations to guarantee equivalence between the centralized chance-constrained optimization and the distributed approximation. Different risk allocation strategies for homogeneous and heterogeneous teams are proposed that approximate the agent and mission score distributions a priori, and results are provided showing improved performance in time-critical mission scenarios given allowable risk thresholds.",
    "advisors": ["Jonathan P. How"],
    "text": "Robust distributed planning strategies for autonomous multi-agent teams The increased use of autonomous robotic agents, such as unmanned aerial vehicles (UAVs) and ground rovers, for complex missions has motivated the development of autonomous task allocation and planning methods that ensure spatial and temporal coordination for teams of cooperating agents. The basic problem can be formulated as a combinatorial optimization (mixed-integer program) involving nonlinear and time-varying system dynamics. For most problems of interest, optimal solution methods are computationally intractable (NP-Hard), and centralized planning approaches, which usually require high bandwidth connections with a ground station (e.g. to transmit received sensor data, and to dispense agent plans), are resource intensive and react slowly to local changes in dynamic environments. Distributed approximate algorithms, where agents plan individually and coordinate with each other locally through consensus protocols, can alleviate many of these issues and have been successfully used to develop real-time conflict-free solutions for heterogeneous networked teams. An important issue associated with autonomous planning is that many of the algorithms rely on underlying system models and parameters which are often subject to uncertainty. This uncertainty can result from many sources including: inaccurate modeling due to simplifications, assumptions, and/or parameter errors; fundamentally nondeterministic processes (e.g. sensor readings, stochastic dynamics); and dynamic local information changes. As discrepancies between the planner models and the actual system dynamics increase, mission performance typically degrades. The impact of these discrepancies on the overall quality of the plan is usually hard to quantify in advance due to nonlinear effects, coupling between tasks and agents, and interdependencies between system constraints. However, if uncertainty models of planning parameters are available, they can be leveraged to create robust plans that explicitly hedge against the inherent uncertainty given allowable risk thresholds. This thesis presents real-time robust distributed planning strategies that can be used to plan for multi-agent networked teams operating in stochastic and dynamic environments. One class of distributed combinatorial planning algorithms involves using auction algorithms augmented with consensus protocols to allocate tasks amongst a team of agents while resolving conflicting assignments locally between the agents. A particular algorithm in this class is the Consensus-Based Bundle Algorithm (CBBA), a distributed auction protocol that guarantees conflict-free solutions despite inconsistencies in situational awareness across the team. CBBA runs in polynomial time, demonstrating good scalability with increasing numbers of agents and tasks. This thesis builds upon the CBBA framework to address many realistic considerations associated with planning for networked teams, including time-critical mission constraints, limited communication between agents, and stochastic operating environments. A particular focus of this work is a robust extension to CBBA that handles distributed planning in stochastic environments given probabilistic parameter models and different stochastic metrics. The Robust CBBA algorithm proposed in this thesis provides a distributed real-time framework which can leverage different stochastic metrics to hedge against parameter uncertainty. In mission scenarios where low probability of failure is required, a chance-constrained stochastic metric can be used to provide probabilistic guarantees on achievable mission performance given allowable risk thresholds. This thesis proposes a distributed chance-constrained approximation that can be used within the Robust CBBA framework, and derives constraints on individual risk allocations to guarantee equivalence between the centralized chance-constrained optimization and the distributed approximation. Different risk allocation strategies for homogeneous and heterogeneous teams are proposed that approximate the agent and mission score distributions a priori, and results are provided showing improved performance in time-critical mission scenarios given allowable risk thresholds."
}, {
    "id": "oai:dspace.mit.edu:1721.1/90603",
    "title": "Decision making in the presence of complex dynamics from limited, batch data",
    "abstract": "Robot decision making in real-world domains can be extremely difficult when the robot has to interact with a complex, poorly understood environment. In these environments, a data-driven approach is commonly taken where a model is first learned and then used for decision making since expert knowledge is rarely sucient for specifying the world's dynamics. Unfortunately, learning a model for a complex environment often involves fitting a large number of parameters which can require an unobtainable amount of data. In real-world domains we are also typically confronted with fitting a model that is only an approximation of the true dynamics, causing difficulties for standard learning approaches. In this thesis we explore two core methodologies for learning a model for decision making in the presence of complex dynamics: explicitly selecting the model which achieves the highest estimated performance and allowing the model class to grow as more data is seen. We show that our approach for explicitly selecting the model with the highest estimated performance has desirable theoretical properties and outperforms standard minimum error fitting techniques on benchmark and real-world problems. To grow the size of model class with the amount of data, we first show how this can be accomplished by using Bayesian nonparametric statistics to model the dynamics, which can then be used for planning. We then present an alternative approach which grows the policy class using the principle of structural risk minimization, for which the resulting algorithm has provable performance bounds with weak assumptions on the true world's dynamics.",
    "advisors": ["Nicholas Roy"],
    "text": "Decision making in the presence of complex dynamics from limited, batch data Robot decision making in real-world domains can be extremely difficult when the robot has to interact with a complex, poorly understood environment. In these environments, a data-driven approach is commonly taken where a model is first learned and then used for decision making since expert knowledge is rarely sucient for specifying the world's dynamics. Unfortunately, learning a model for a complex environment often involves fitting a large number of parameters which can require an unobtainable amount of data. In real-world domains we are also typically confronted with fitting a model that is only an approximation of the true dynamics, causing difficulties for standard learning approaches. In this thesis we explore two core methodologies for learning a model for decision making in the presence of complex dynamics: explicitly selecting the model which achieves the highest estimated performance and allowing the model class to grow as more data is seen. We show that our approach for explicitly selecting the model with the highest estimated performance has desirable theoretical properties and outperforms standard minimum error fitting techniques on benchmark and real-world problems. To grow the size of model class with the amount of data, we first show how this can be accomplished by using Bayesian nonparametric statistics to model the dynamics, which can then be used for planning. We then present an alternative approach which grows the policy class using the principle of structural risk minimization, for which the resulting algorithm has provable performance bounds with weak assumptions on the true world's dynamics."
}, {
    "id": "oai:dspace.mit.edu:1721.1/90598",
    "title": "Lengthscale effects in the damage and failure of composites",
    "abstract": "The primary objective of this work is to investigate and identify lengthscale effects associated with damage in composite materials and their structures, and to determine how these lengthscales vary across levels of composites and can be used in assessing the overall response of composite structures. This is an advancement in a much larger pursuit towards developing a new methodology that utilizes composite failure and material data collected across all levels in order to predict the occurrence of damage and its effects at any operative level of composite structures. Documentation procedures are developed to capture qualitative and quantitative information on damage within experimental specimens, and computed microtomography provides additional information on the damage process. Specimens containing structural details are investigated postmortem to identify lengthscales associated with damage modes. Finite element models are developed in order to investigate the interaction of lengthscales associated with structural details with those associated with the basic damage modes. Based on these experimental and numerical results, lengthscales associated with five basic damage modes, as identified from previous studies, and the four structural details included in this investigation are identified and discussed, as are their interactions and importance. It is found that it is important to recognize two damage regimes, initiation and propagation, in characterizing lengthscales associated with damage modes. Identifying key lengthscales within each regime allows investigation of how the critical lengthscale(s) controlling the damage mode(s) change(s) across regimes. The concept of the \"observable lengthscale\" is identified as an important consideration when investigating lengthscales in experimental specimens and structures in that the observable lengthscale sets the ability to resolve damage and interactions of such. In a manner analogous to the \"observable lengthscale,\" key lengthscales of basic damage modes and of structural details need to be used when choosing the scale of finite element models so that models have a resolution at least as fine as the key lengthscale of the mode under investigation. The results of the work show that the concept of lengthscales is a viable tool to characterize the overall response of composite structures, particularly involving damage initiation, damage propagation, and overall failure. The determination of how these lengthscales vary across levels in composites provides an important tool that can be used to assess this overall response of composite structures. Particular conclusions considering each damage mode are offered. In addition, a new damage type, called \"transverse zigzag,\" is identified and studied, resulting in a finding that loads can \"bypass\" and \"carry-through\" regions of damage, depending on the geometry and laminate. Recommendations for further investigations are proposed based on the understanding of the role of lengthscales in the damage and failure of composites acquired from this work, and the needs identified to further this understanding.",
    "advisors": ["Paul A. Lagacé"],
    "text": "Lengthscale effects in the damage and failure of composites The primary objective of this work is to investigate and identify lengthscale effects associated with damage in composite materials and their structures, and to determine how these lengthscales vary across levels of composites and can be used in assessing the overall response of composite structures. This is an advancement in a much larger pursuit towards developing a new methodology that utilizes composite failure and material data collected across all levels in order to predict the occurrence of damage and its effects at any operative level of composite structures. Documentation procedures are developed to capture qualitative and quantitative information on damage within experimental specimens, and computed microtomography provides additional information on the damage process. Specimens containing structural details are investigated postmortem to identify lengthscales associated with damage modes. Finite element models are developed in order to investigate the interaction of lengthscales associated with structural details with those associated with the basic damage modes. Based on these experimental and numerical results, lengthscales associated with five basic damage modes, as identified from previous studies, and the four structural details included in this investigation are identified and discussed, as are their interactions and importance. It is found that it is important to recognize two damage regimes, initiation and propagation, in characterizing lengthscales associated with damage modes. Identifying key lengthscales within each regime allows investigation of how the critical lengthscale(s) controlling the damage mode(s) change(s) across regimes. The concept of the \"observable lengthscale\" is identified as an important consideration when investigating lengthscales in experimental specimens and structures in that the observable lengthscale sets the ability to resolve damage and interactions of such. In a manner analogous to the \"observable lengthscale,\" key lengthscales of basic damage modes and of structural details need to be used when choosing the scale of finite element models so that models have a resolution at least as fine as the key lengthscale of the mode under investigation. The results of the work show that the concept of lengthscales is a viable tool to characterize the overall response of composite structures, particularly involving damage initiation, damage propagation, and overall failure. The determination of how these lengthscales vary across levels in composites provides an important tool that can be used to assess this overall response of composite structures. Particular conclusions considering each damage mode are offered. In addition, a new damage type, called \"transverse zigzag,\" is identified and studied, resulting in a finding that loads can \"bypass\" and \"carry-through\" regions of damage, depending on the geometry and laminate. Recommendations for further investigations are proposed based on the understanding of the role of lengthscales in the damage and failure of composites acquired from this work, and the needs identified to further this understanding."
}, {
    "id": "oai:dspace.mit.edu:1721.1/67174",
    "title": "Photoplethysmography for non-invasive measurement of bone hemodynamic responses to changes in external pressure",
    "abstract": "Adequate blood supply and circulation in bones is required to maintain a healthy skeleton, and inadequate blood perfusion is associated with numerous bone pathologies and a decrease in bone mineral density (BMD). Bone hemodynamics remains poorly understood and loss of BMD is still one of the limiting factors to long duration human spaceflight. Developments in photoplethysmography (PPG) hardware have made it a promising tool for non-invasive bone hemodynamic measurements. The aims of this thesis are to: 1) validate the use of PPG as a tool for non-invasive bone hemodynamic measurements, 2) characterize bone hemodynamic responses to changes in external pressure, and 3) identify the predominant mechanisms regulating bone hemodynamic responses to pressure changes. A new PPG device capable of measuring bone hemodynamic responses was designed and tested. It represents the state-of-the-art in deep-tissue PPG instrumentation. Validation experiments including arterial occlusion, cold exposure, skin occlusion and nitroglycerin exposure were performed. Single-limb pressure chamber experiments were performed over a range of pressures from -50 to +50 mmHg to characterize the responses to changes in external pressure and to identify the predominant control mechanisms. Our results support the use of PPG as a valid tool for measuring bone hemodynamic responses. Bone hemodynamic responses to changes in external pressure have been characterized for the first time. We also present the first report of a myogenic response in bone and show that the myogenic effect is the predominant control mechanism in bone over a wide range of pressure levels. Myogenic-induced vasoconstriction is observed at all negative pressure levels, with increasing vasoconstriction at the more extreme pressure differences. At positive pressures we observed an initial myogenic-induced vasodilation followed by activation of the intramuscular pressure receptors at +30 mmHg which overrides the initial response and causes vasoconstriction at the highest positive pressure. The availability of a new tool for non-invasive bone hemodynamic measurements opens the door to several new research opportunities with clinical, Earth-based as well as human spaceflight applications.",
    "advisors": ["Alan R. Hargens", "Dava J. Newman"],
    "text": "Photoplethysmography for non-invasive measurement of bone hemodynamic responses to changes in external pressure Adequate blood supply and circulation in bones is required to maintain a healthy skeleton, and inadequate blood perfusion is associated with numerous bone pathologies and a decrease in bone mineral density (BMD). Bone hemodynamics remains poorly understood and loss of BMD is still one of the limiting factors to long duration human spaceflight. Developments in photoplethysmography (PPG) hardware have made it a promising tool for non-invasive bone hemodynamic measurements. The aims of this thesis are to: 1) validate the use of PPG as a tool for non-invasive bone hemodynamic measurements, 2) characterize bone hemodynamic responses to changes in external pressure, and 3) identify the predominant mechanisms regulating bone hemodynamic responses to pressure changes. A new PPG device capable of measuring bone hemodynamic responses was designed and tested. It represents the state-of-the-art in deep-tissue PPG instrumentation. Validation experiments including arterial occlusion, cold exposure, skin occlusion and nitroglycerin exposure were performed. Single-limb pressure chamber experiments were performed over a range of pressures from -50 to +50 mmHg to characterize the responses to changes in external pressure and to identify the predominant control mechanisms. Our results support the use of PPG as a valid tool for measuring bone hemodynamic responses. Bone hemodynamic responses to changes in external pressure have been characterized for the first time. We also present the first report of a myogenic response in bone and show that the myogenic effect is the predominant control mechanism in bone over a wide range of pressure levels. Myogenic-induced vasoconstriction is observed at all negative pressure levels, with increasing vasoconstriction at the more extreme pressure differences. At positive pressures we observed an initial myogenic-induced vasodilation followed by activation of the intramuscular pressure receptors at +30 mmHg which overrides the initial response and causes vasoconstriction at the highest positive pressure. The availability of a new tool for non-invasive bone hemodynamic measurements opens the door to several new research opportunities with clinical, Earth-based as well as human spaceflight applications."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54607",
    "title": "Environmental impact assessment of commercial aircraft operations in the United States",
    "abstract": "The objective of this thesis was to evaluate the environmental trade-offs inherent in multi-criteria objectives of an integrated environmental policy. A probabilistic multi-attribute impact pathway analysis (MAIPA) was formulated to assess the environmental damages of US commercial aircraft operations from 1991-2003. The initial contribution of this work was demonstrating the feasibility of, and identifying requirements for, the FAA Aviation-environmental Portfolio Management Tool (APMT), an integrated assessment capability for US regulatory decision-making. Non-aircraft sources have been found to dictate marginal emissions costs. The implication is that aviation emissions reductions influence neither the magnitudes nor trends in per-unit marginal damages. In contrast, noise mitigation is the dominant influence on the value of per-unit marginal damages. Trends in sum damages were found to depend on the growth rates of air transport relative to other source emissions. Growth in air transport emissions outpaced non-aircraft sources from 1991-2003. Because growth in marginal costs is nonlinear over this period, aviation emissions damages grow faster than inventories. Applying methods similar to MAIPA to estimate damages for future scenarios suggests that stemming climate impacts is fast becoming the priority. A reassessment of the environmental benefits derived from mandated phase-outs of noisy aircraft during the 1990's has been carried out. Previous studies estimated a -80% reduction in population exposure. In contrast, the reassessment estimates a ~2% reduction, providing benefits 17-20 times lower than published estimates of abatement costs.",
    "advisors": ["Ian A. Waitz"],
    "text": "Environmental impact assessment of commercial aircraft operations in the United States The objective of this thesis was to evaluate the environmental trade-offs inherent in multi-criteria objectives of an integrated environmental policy. A probabilistic multi-attribute impact pathway analysis (MAIPA) was formulated to assess the environmental damages of US commercial aircraft operations from 1991-2003. The initial contribution of this work was demonstrating the feasibility of, and identifying requirements for, the FAA Aviation-environmental Portfolio Management Tool (APMT), an integrated assessment capability for US regulatory decision-making. Non-aircraft sources have been found to dictate marginal emissions costs. The implication is that aviation emissions reductions influence neither the magnitudes nor trends in per-unit marginal damages. In contrast, noise mitigation is the dominant influence on the value of per-unit marginal damages. Trends in sum damages were found to depend on the growth rates of air transport relative to other source emissions. Growth in air transport emissions outpaced non-aircraft sources from 1991-2003. Because growth in marginal costs is nonlinear over this period, aviation emissions damages grow faster than inventories. Applying methods similar to MAIPA to estimate damages for future scenarios suggests that stemming climate impacts is fast becoming the priority. A reassessment of the environmental benefits derived from mandated phase-outs of noisy aircraft during the 1990's has been carried out. Previous studies estimated a -80% reduction in population exposure. In contrast, the reassessment estimates a ~2% reduction, providing benefits 17-20 times lower than published estimates of abatement costs."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98679",
    "title": "Effects of imperfections on the response of composite structures with scarf joints",
    "abstract": "The increased use of carbon fiber reinforced polymer (CFRP) composites as a primary structural material leads to a need for repairs of such structures. One such repair method is the scarf joint, an adhesive joint that allows for the original surface profile of a structure to be maintained. Studies of scarf joints have primarily considered an idealized version of the configuration, whereas there will be imperfections that arise during manufacture and installation of a real scarf joint. Parametric studies that consider scarf joint configurations have generally considered only the linear behavior of the adhesive material, although adhesive materials typically have large nonlinear capability. Finally, in studies of scarf joints, only the stress state in the adhesive tends to be considered. In this work, the stress and strain states in both the adherends and in the adhesive are studied for imperfect scarf joints via finite element analyses. Seven parameters that characterize different imperfections, with two to four values each, are studied and compared to the baseline configuration, which is an idealized scarf joint, for the case of linear behavior of the adhesive. One to two values of each parameter are studied for the case of nonlinear behavior of the adhesive. The parameters investigated are tip bluntness, through-thickness patch offset, mismatched adherend thickness, mismatched scarf angle, adhesive thickness variation, rotated patch adherend, and delamination in the patch. The imperfections are applied to one of the adherends. The resulting strains of the different configurations are presented for the adhesive and adherends. The yield progressions are presented for the nonlinear behavior. Results show that the nonlinear behavior case can be estimated from the results of the linear behavior. The onset of yield occurs at the location of maximum strain and is primarily associated with shear strain. The yield typically occurs at the sharp adherend tip, and failure typically coincides with the location of the onset of yield. The full nonlinear behavior of the adhesive needs to be considered for any complete study of failure. The effects of imperfections are both local and global in nature for the scarf joint configuration. There is an effect on the global strain response as a result of load eccentricity due to asymmetry that leads to bending and other overall equilibrium considerations, while local effects are manifested in the strain fields near the imperfection. The tip bluntness parameter, adherend rotation parameter, and delamination parameter are found to have the most significant effects on the response of the scarf joint configuration in both the adhesive and adherend due to the specifics of the local strain field due to the imperfections. All other parameters have reduced effects on the behavior that are a result of global bending and resulting changes in the overall length of the joint. Key lengthscales are identified for imperfections and for overall joint behavior. Recommendations for further work are presented.",
    "advisors": ["Paul A. Lagacé"],
    "text": "Effects of imperfections on the response of composite structures with scarf joints The increased use of carbon fiber reinforced polymer (CFRP) composites as a primary structural material leads to a need for repairs of such structures. One such repair method is the scarf joint, an adhesive joint that allows for the original surface profile of a structure to be maintained. Studies of scarf joints have primarily considered an idealized version of the configuration, whereas there will be imperfections that arise during manufacture and installation of a real scarf joint. Parametric studies that consider scarf joint configurations have generally considered only the linear behavior of the adhesive material, although adhesive materials typically have large nonlinear capability. Finally, in studies of scarf joints, only the stress state in the adhesive tends to be considered. In this work, the stress and strain states in both the adherends and in the adhesive are studied for imperfect scarf joints via finite element analyses. Seven parameters that characterize different imperfections, with two to four values each, are studied and compared to the baseline configuration, which is an idealized scarf joint, for the case of linear behavior of the adhesive. One to two values of each parameter are studied for the case of nonlinear behavior of the adhesive. The parameters investigated are tip bluntness, through-thickness patch offset, mismatched adherend thickness, mismatched scarf angle, adhesive thickness variation, rotated patch adherend, and delamination in the patch. The imperfections are applied to one of the adherends. The resulting strains of the different configurations are presented for the adhesive and adherends. The yield progressions are presented for the nonlinear behavior. Results show that the nonlinear behavior case can be estimated from the results of the linear behavior. The onset of yield occurs at the location of maximum strain and is primarily associated with shear strain. The yield typically occurs at the sharp adherend tip, and failure typically coincides with the location of the onset of yield. The full nonlinear behavior of the adhesive needs to be considered for any complete study of failure. The effects of imperfections are both local and global in nature for the scarf joint configuration. There is an effect on the global strain response as a result of load eccentricity due to asymmetry that leads to bending and other overall equilibrium considerations, while local effects are manifested in the strain fields near the imperfection. The tip bluntness parameter, adherend rotation parameter, and delamination parameter are found to have the most significant effects on the response of the scarf joint configuration in both the adhesive and adherend due to the specifics of the local strain field due to the imperfections. All other parameters have reduced effects on the behavior that are a result of global bending and resulting changes in the overall length of the joint. Key lengthscales are identified for imperfections and for overall joint behavior. Recommendations for further work are presented."
}, {
    "id": "oai:dspace.mit.edu:1721.1/119268",
    "title": "Fine-pointing system development and photometric precision assessment for the transiting exoplanet survey satellite",
    "abstract": "The Transiting Exoplanet Survey Satellite (TESS) is an MIT-led, NASA-funded Explorer-class planet finder with the primary mission of detecting transiting exoplanets in a 2-year all-sky survey. The TESS instrument consists of four wide-field optical cameras, mounted in a stacked configuration. During science operations, TESS uses the instrument cameras in the loop for attitude determination as part of the fine-pointing system in order to achieve the desired photometric precision of the mission. In this work, we present our approach toward improving and quantifying the fine-pointing performance of TESS and assessing the impact of pointing errors on the overall photometric precision of the mission. First, a guide-star selection method was developed to generate a set of desirable stars for guidance during any arbitrary observation sector based on stellar properties and their proximities to neighboring objects. Next, a comprehensive testing and validation framework was developed to assess the attitude determination flight software as well as to quantify the attitude determination performance of the instrument cameras during key mission scenarios. This framework allows the attitude determination system to be significantly improved, leading to a reduction in open-loop attitude errors by more than 65%. The final attitude determination performance was estimated to meet all relevant open-loop pointing requirements with margin. To assess the closed-loop fine-pointing performance of the system, multiple mission scenarios were simulated and analyzed using a comprehensive framework including both instrument performance as well as spacecraft control and dynamics, showing that the relevant closed-loop pointing requirements at multiple time scales are met with margin. The performance of the system over longer time scales and in temporary camera unavailability periods was also quantified through end-to-end simulations and is in agreement with analytical predictions. Finally, a high-fidelity simulation and analysis framework was developed to assess the photometric precision of the system, including realistic optical responses of the cameras, major photometry noise processes, and expected fine-pointing errors. The simulation results show that with basic co-trending techniques, the impact of pointing errors on science data can be significantly reduced, resulting in shot-noise-limited stellar photometry signals over the magnitude range of interest.",
    "advisors": ["Kerri L. Cahoy"],
    "text": "Fine-pointing system development and photometric precision assessment for the transiting exoplanet survey satellite The Transiting Exoplanet Survey Satellite (TESS) is an MIT-led, NASA-funded Explorer-class planet finder with the primary mission of detecting transiting exoplanets in a 2-year all-sky survey. The TESS instrument consists of four wide-field optical cameras, mounted in a stacked configuration. During science operations, TESS uses the instrument cameras in the loop for attitude determination as part of the fine-pointing system in order to achieve the desired photometric precision of the mission. In this work, we present our approach toward improving and quantifying the fine-pointing performance of TESS and assessing the impact of pointing errors on the overall photometric precision of the mission. First, a guide-star selection method was developed to generate a set of desirable stars for guidance during any arbitrary observation sector based on stellar properties and their proximities to neighboring objects. Next, a comprehensive testing and validation framework was developed to assess the attitude determination flight software as well as to quantify the attitude determination performance of the instrument cameras during key mission scenarios. This framework allows the attitude determination system to be significantly improved, leading to a reduction in open-loop attitude errors by more than 65%. The final attitude determination performance was estimated to meet all relevant open-loop pointing requirements with margin. To assess the closed-loop fine-pointing performance of the system, multiple mission scenarios were simulated and analyzed using a comprehensive framework including both instrument performance as well as spacecraft control and dynamics, showing that the relevant closed-loop pointing requirements at multiple time scales are met with margin. The performance of the system over longer time scales and in temporary camera unavailability periods was also quantified through end-to-end simulations and is in agreement with analytical predictions. Finally, a high-fidelity simulation and analysis framework was developed to assess the photometric precision of the system, including realistic optical responses of the cameras, major photometry noise processes, and expected fine-pointing errors. The simulation results show that with basic co-trending techniques, the impact of pointing errors on science data can be significantly reduced, resulting in shot-noise-limited stellar photometry signals over the magnitude range of interest."
}, {
    "id": "oai:dspace.mit.edu:1721.1/119292",
    "title": "Neuromorphic control of dynamic systems",
    "abstract": "Arguably, the agility of a robotic system is dictated by the speed of its processing pipeline, i.e., the speed of data acquisition plus data processing from a robot's on-board vision sensors. Specifically, one ideally hopes that this pipeline offer fresh measurements at a high temporal resolution with low-latency in a computationally-cheap manner for efficient control. This desirable situation may be hard to come by for majority of the current vision-based robotic applications that rely on the traditional CCD-/CMOS-pipeline, as one would be in search for traditional cameras that offer a high sampling rate (thus, high temporal resolution) whose potentially redundant (thus, not fresh) and synchronous output must be processed with low-latency in a computationally-cheap manner. For instance, processing the synchronous series of conventional camera images, which embed possibly redundant levels of intensities may greatly hinder the fast reaction times required by robots while expending power. This issue warrants the need for faster sensors in order to truly address the underlying perception problem for high-performance systems that need to operate under power constraints. To this end, we capitalize upon the merits of a recently introduced biologically inspired and computationally-cheap alternative to traditional cameras-called Neuromorphic Vision Sensors whose pixels independently and asynchronously (thus, high temporal resolution) fire, in the order of micro-seconds (thus, low-latency and high temporal resolution), a stream of non-redundant (thus, fresh) brightness changes represented as binary numbers (±1), termed retinal events, based on a trigger condition that is defined on a logarithmic scale. These properties offer a faster processing pipeline and hint that the Neuromorphic sensor would be a promising candidate to facilitate high-speed robotic applications. However, existing computer-vision based algorithms designed for processing periodic measurements cannot be directly adapted to process retinal events, as these are fired aperiodically, and are ambiguous since they are binary. As an additional challenge, in practice, many retinal events are misfired due to the presence of underlying sensor circuitry noise (not associated to physical brightness changes in the environment) and we term these as spurious events. The merits and operational constraints of this vision sensor mandates the development of a corresponding control-theoretic setup. Thus, the contributions of this dissertation are twofold: 1) to design a control algorithm that processes de-noised retinal events to facilitate a prescribed control task, and 2) to propose a de-noising procedure that mitigates the effect of spuriosity in retinal events. The first part of this dissertation, investigates the problem of controlling (i.e., stabilization and regulation) a Continuous-Time Linear Time Invariant (CT-LTI) system using retinal events generated from an idealistic model of a Neuromorphic Vision Sensor, which is an instance of a broad family of signal change detection sensors frequently encountered in practice. The contribution is to present a novel control design procedure that stabilizes and regulates a hybrid system, consisting of the CT-LTI system and the discrete-event signal change observation model, to a desired set-point. Moreover, the set of thresholds (sufficient conditions) for the given system to fulfill the prescribed control task is provided. The proposed controller is then extended to handle the case of noise in both the system dynamics as well as the observation model; thus, accounts for spurious events in this setting. The second part of this dissertation proposes a de-noising algorithm-Spuriosity Filter (SF)-and is motivated by the practical need to reduce spurious events whilst working with general observation models. The construction of SF is based on the fundamental lack of spatial correlation between spurious events and the algorithm trades off pixel resolution to produce a cleaner event stream on larger spatial scales by seeking a form of 'consensus' between neighboring pixels. At the core of our analysis lies a formal equivalence relation, defined as a means to track brightness, between our filter and a lower-resolution neuromorphic sensor with reduced noise levels. As a consequence of the principled analysis, we highlight important properties that any filter, which processes asynchronous noisy retinal events must respect and have not been accounted for by existing works. The effectiveness of the proposed control-theoretic setup for the illustrative task of heading regulation is illustrated over a range of systems: from numerical experiments to a laboratory testbed.",
    "advisors": ["Emilio Frazzoli"],
    "text": "Neuromorphic control of dynamic systems Arguably, the agility of a robotic system is dictated by the speed of its processing pipeline, i.e., the speed of data acquisition plus data processing from a robot's on-board vision sensors. Specifically, one ideally hopes that this pipeline offer fresh measurements at a high temporal resolution with low-latency in a computationally-cheap manner for efficient control. This desirable situation may be hard to come by for majority of the current vision-based robotic applications that rely on the traditional CCD-/CMOS-pipeline, as one would be in search for traditional cameras that offer a high sampling rate (thus, high temporal resolution) whose potentially redundant (thus, not fresh) and synchronous output must be processed with low-latency in a computationally-cheap manner. For instance, processing the synchronous series of conventional camera images, which embed possibly redundant levels of intensities may greatly hinder the fast reaction times required by robots while expending power. This issue warrants the need for faster sensors in order to truly address the underlying perception problem for high-performance systems that need to operate under power constraints. To this end, we capitalize upon the merits of a recently introduced biologically inspired and computationally-cheap alternative to traditional cameras-called Neuromorphic Vision Sensors whose pixels independently and asynchronously (thus, high temporal resolution) fire, in the order of micro-seconds (thus, low-latency and high temporal resolution), a stream of non-redundant (thus, fresh) brightness changes represented as binary numbers (±1), termed retinal events, based on a trigger condition that is defined on a logarithmic scale. These properties offer a faster processing pipeline and hint that the Neuromorphic sensor would be a promising candidate to facilitate high-speed robotic applications. However, existing computer-vision based algorithms designed for processing periodic measurements cannot be directly adapted to process retinal events, as these are fired aperiodically, and are ambiguous since they are binary. As an additional challenge, in practice, many retinal events are misfired due to the presence of underlying sensor circuitry noise (not associated to physical brightness changes in the environment) and we term these as spurious events. The merits and operational constraints of this vision sensor mandates the development of a corresponding control-theoretic setup. Thus, the contributions of this dissertation are twofold: 1) to design a control algorithm that processes de-noised retinal events to facilitate a prescribed control task, and 2) to propose a de-noising procedure that mitigates the effect of spuriosity in retinal events. The first part of this dissertation, investigates the problem of controlling (i.e., stabilization and regulation) a Continuous-Time Linear Time Invariant (CT-LTI) system using retinal events generated from an idealistic model of a Neuromorphic Vision Sensor, which is an instance of a broad family of signal change detection sensors frequently encountered in practice. The contribution is to present a novel control design procedure that stabilizes and regulates a hybrid system, consisting of the CT-LTI system and the discrete-event signal change observation model, to a desired set-point. Moreover, the set of thresholds (sufficient conditions) for the given system to fulfill the prescribed control task is provided. The proposed controller is then extended to handle the case of noise in both the system dynamics as well as the observation model; thus, accounts for spurious events in this setting. The second part of this dissertation proposes a de-noising algorithm-Spuriosity Filter (SF)-and is motivated by the practical need to reduce spurious events whilst working with general observation models. The construction of SF is based on the fundamental lack of spatial correlation between spurious events and the algorithm trades off pixel resolution to produce a cleaner event stream on larger spatial scales by seeking a form of 'consensus' between neighboring pixels. At the core of our analysis lies a formal equivalence relation, defined as a means to track brightness, between our filter and a lower-resolution neuromorphic sensor with reduced noise levels. As a consequence of the principled analysis, we highlight important properties that any filter, which processes asynchronous noisy retinal events must respect and have not been accounted for by existing works. The effectiveness of the proposed control-theoretic setup for the illustrative task of heading regulation is illustrated over a range of systems: from numerical experiments to a laboratory testbed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/63224",
    "title": "Accident analysis and hazard analysis for human and organizational factors",
    "abstract": "Pressures and incentives to operate complex socio-technical aerospace systems in a high-risk state are ever present. Without consideration of the role humans and organizations play in system safety during the development of these systems, accidents will occur. Safe design of the \"socio\" parts of the sociotechnical system is challenging. Even if the system, including the human and organizational aspects of the system, are designed to be safe for anticipated system needs and operating environments, without consideration of pressures for increased performance and efficiency and shifting system goals, the system will migrate to a high-risk operating regime and safety can be compromised. Accident analysis is conducted to discover the reasons why an accident occurred and to prevent future accidents. Safety professionals have attributed 70-80% of aviation accidents to human error. Investigators have long known that the human and organizational aspects of systems are key contributors to accidents, yet they lack a rigorous approach for analyzing their impacts. Many safety engineers strive for blame-free reports that will foster reflection and learning from the accident, but struggle with methods that require direct technical causality, do not consider systemic factors, and seem to leave individuals looking culpable. An accident analysis method is needed that will guide the work, aid in the analysis of the role of human and organizations in accidents and promote blame-free accounting of accidents that will support learning from the events. Current hazard analysis methods, adapted from traditional accident models, are not able to evaluate the potential for risk migration, or comprehensively identify accident scenarios involving humans and organizations. Thus, system engineers are not able to design systems that prevent loss events related to human error or organizational factors. State of the art methods for human and organization hazard analysis are, at best, elaborate event-based classification schemes for potential errors. Current human and organization hazard analysis methods are not suitable for use as part of the system engineering process. Systems must be analyzed with methods that identify all human and organization related hazards during the design process, so that this information can be used to change the design so that human error and organization errors do not occur. Errors must be more than classified and categorized, errors must be prevented in design. A new type of hazard analysis method that identifies hazardous scenarios involving humans and organizations is needed for both systems in conception and those already in the field. This thesis contains novel new approaches to accident analysis and hazard analysis. Both methods are based on principles found in the Human Factors, Organizational Safety and System Safety literature. It is hoped that the accident analysis method should aid engineers in understanding how human actions and decisions are connected to the accident and aid in the development of blame-free reports that encourage learning from accidents. The goal for the hazard analysis method is that it will be useful in: 1) designing systems to be safe; 2) diagnosing policies or pressures and identifying design flaws that contribute to high-risk operations; 3) identifying designs that are resistant to pressures that increase risk; and 4) allowing system decision-makers to predict how proposed or current policies will affect safety. To assess the accident analysis method, a comparison with state of the art methods is conducted. To demonstrate the feasibility of the method applied to hazard analysis; it is applied to several systems in various domains.",
    "advisors": ["Nancy G. Leveson"],
    "text": "Accident analysis and hazard analysis for human and organizational factors Pressures and incentives to operate complex socio-technical aerospace systems in a high-risk state are ever present. Without consideration of the role humans and organizations play in system safety during the development of these systems, accidents will occur. Safe design of the \"socio\" parts of the sociotechnical system is challenging. Even if the system, including the human and organizational aspects of the system, are designed to be safe for anticipated system needs and operating environments, without consideration of pressures for increased performance and efficiency and shifting system goals, the system will migrate to a high-risk operating regime and safety can be compromised. Accident analysis is conducted to discover the reasons why an accident occurred and to prevent future accidents. Safety professionals have attributed 70-80% of aviation accidents to human error. Investigators have long known that the human and organizational aspects of systems are key contributors to accidents, yet they lack a rigorous approach for analyzing their impacts. Many safety engineers strive for blame-free reports that will foster reflection and learning from the accident, but struggle with methods that require direct technical causality, do not consider systemic factors, and seem to leave individuals looking culpable. An accident analysis method is needed that will guide the work, aid in the analysis of the role of human and organizations in accidents and promote blame-free accounting of accidents that will support learning from the events. Current hazard analysis methods, adapted from traditional accident models, are not able to evaluate the potential for risk migration, or comprehensively identify accident scenarios involving humans and organizations. Thus, system engineers are not able to design systems that prevent loss events related to human error or organizational factors. State of the art methods for human and organization hazard analysis are, at best, elaborate event-based classification schemes for potential errors. Current human and organization hazard analysis methods are not suitable for use as part of the system engineering process. Systems must be analyzed with methods that identify all human and organization related hazards during the design process, so that this information can be used to change the design so that human error and organization errors do not occur. Errors must be more than classified and categorized, errors must be prevented in design. A new type of hazard analysis method that identifies hazardous scenarios involving humans and organizations is needed for both systems in conception and those already in the field. This thesis contains novel new approaches to accident analysis and hazard analysis. Both methods are based on principles found in the Human Factors, Organizational Safety and System Safety literature. It is hoped that the accident analysis method should aid engineers in understanding how human actions and decisions are connected to the accident and aid in the development of blame-free reports that encourage learning from accidents. The goal for the hazard analysis method is that it will be useful in: 1) designing systems to be safe; 2) diagnosing policies or pressures and identifying design flaws that contribute to high-risk operations; 3) identifying designs that are resistant to pressures that increase risk; and 4) allowing system decision-makers to predict how proposed or current policies will affect safety. To assess the accident analysis method, a comparison with state of the art methods is conducted. To demonstrate the feasibility of the method applied to hazard analysis; it is applied to several systems in various domains."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8704",
    "title": "Mechanics of fatigue damage in titanium-graphite hybrid laminates",
    "abstract": "Titanium-graphite hybrid laminates are being developed for high-temperature aerospace applications. Experimental observations have indicated that cracks in the titanium facesheets initiate at free edges as well as in areas of high stress concentration, such as holes and notches and that a delaminated region between the facesheet and the intact plies develops and propagates in the wake of the facesheet crack. This thesis experimentally and analytically evaluates the facesheet crack and delamination growth behavior of TiGr laminates. The delamination growth behavior is studied by isolating delamination growth using TiGr specimens with facesheet seams. The growth rate of delamination from facesheet seams is related via a power law to the applied strain energy release rate. It is shown that elevated temperatures significantly increase the growth rate of facesheet delamination. The facesheet crack growth behavior in TiGr laminates was measured experimentally and compared to model predictions. It is shown that the crack growth rate in TiGr facesheets is significantly lower for TiGr laminates as compared to monolithic titanium. After an initial decrease in crack growth rate, the crack propagates at a constant rate while the crack growth rate for monolithic titanium increases as the crack extends.",
    "advisors": ["S. Mark Spearing"],
    "text": "Mechanics of fatigue damage in titanium-graphite hybrid laminates Titanium-graphite hybrid laminates are being developed for high-temperature aerospace applications. Experimental observations have indicated that cracks in the titanium facesheets initiate at free edges as well as in areas of high stress concentration, such as holes and notches and that a delaminated region between the facesheet and the intact plies develops and propagates in the wake of the facesheet crack. This thesis experimentally and analytically evaluates the facesheet crack and delamination growth behavior of TiGr laminates. The delamination growth behavior is studied by isolating delamination growth using TiGr specimens with facesheet seams. The growth rate of delamination from facesheet seams is related via a power law to the applied strain energy release rate. It is shown that elevated temperatures significantly increase the growth rate of facesheet delamination. The facesheet crack growth behavior in TiGr laminates was measured experimentally and compared to model predictions. It is shown that the crack growth rate in TiGr facesheets is significantly lower for TiGr laminates as compared to monolithic titanium. After an initial decrease in crack growth rate, the crack propagates at a constant rate while the crack growth rate for monolithic titanium increases as the crack extends."
}, {
    "id": "oai:dspace.mit.edu:1721.1/85760",
    "title": "Practical algorithms for distributed network control",
    "abstract": "Optimal routing and scheduling algorithms have been studied for decades, however several practical issues prevent the adoption of these network control policies on the Internet. This thesis considers two distinct topics in distributed network control: (i) maximizing throughput in wireless networks using network coding, and (ii) deploying controllable nodes in legacy networks. Network coding is a relatively new technique that allows for an increase in throughput under certain topological and routing conditions. The first part of this thesis considers jointly optimal routing, scheduling, and network coding strategies to maximize throughput in wireless networks. We introduce a simple network coding strategy and fully characterize the region of arrival rates supported. We propose a centralized dynamic control policy for routing, scheduling, and our network coding strategy, and prove this policy to be throughput optimal subject to our coding constraint. We further propose a distributed control policy based on random access that optimizes for routing, scheduling, and pairwise coding, where pairwise coding captures most of the coding opportunities on random topologies. We prove this second policy to also be throughput optimal subject to the coding constraint. Finally, we reduce the gap between theory and practice by identifying and solving several problems that may occur in system implementations of these policies. Throughput optimal policies typically require every device in the network to make dynamic routing decisions. In the second part of this thesis, we propose an overlay routing architecture such that only a subset of devices (overlay nodes) need to make dynamic routing decisions, and yet maximum throughput can still be achieved. We begin by formulating an optimization problem that searches for the minimum overlay node placement that achieves maximum throughput. We devise an efficient placement algorithm which solves this problem optimally for networks not subject to interference constraints. Then we propose a heuristic control policy for use at overlay nodes, and show by simulation that this policy performs optimally in all studied scenarios.",
    "advisors": ["Eytan Modiano", "Brooke Shrader"],
    "text": "Practical algorithms for distributed network control Optimal routing and scheduling algorithms have been studied for decades, however several practical issues prevent the adoption of these network control policies on the Internet. This thesis considers two distinct topics in distributed network control: (i) maximizing throughput in wireless networks using network coding, and (ii) deploying controllable nodes in legacy networks. Network coding is a relatively new technique that allows for an increase in throughput under certain topological and routing conditions. The first part of this thesis considers jointly optimal routing, scheduling, and network coding strategies to maximize throughput in wireless networks. We introduce a simple network coding strategy and fully characterize the region of arrival rates supported. We propose a centralized dynamic control policy for routing, scheduling, and our network coding strategy, and prove this policy to be throughput optimal subject to our coding constraint. We further propose a distributed control policy based on random access that optimizes for routing, scheduling, and pairwise coding, where pairwise coding captures most of the coding opportunities on random topologies. We prove this second policy to also be throughput optimal subject to the coding constraint. Finally, we reduce the gap between theory and practice by identifying and solving several problems that may occur in system implementations of these policies. Throughput optimal policies typically require every device in the network to make dynamic routing decisions. In the second part of this thesis, we propose an overlay routing architecture such that only a subset of devices (overlay nodes) need to make dynamic routing decisions, and yet maximum throughput can still be achieved. We begin by formulating an optimization problem that searches for the minimum overlay node placement that achieves maximum throughput. We devise an efficient placement algorithm which solves this problem optimally for networks not subject to interference constraints. Then we propose a heuristic control policy for use at overlay nodes, and show by simulation that this policy performs optimally in all studied scenarios."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42173",
    "title": "Learning bounded optimal behavior using Markov decision processes",
    "abstract": "Creating agents that behave rationally in the real-world is one goal of Artificial Intelligence. A rational agent is one that takes, at each point in time, the optimal action such that its expected utility is maximized. However, to determine the optimal action the agent may need to engage in lengthy deliberations or computations. The effect of computation is generally not explicitly considered when performing deliberations. In reality, spending too much time in deliberation may yield high quality plans that do not satisfy the natural timing constraints of a problem, making them effectively useless. Enforcing shortened deliberation times may yield timely plans, but these may be of diminished utility. These two cases suggest the possibility of optimizing an agent's deliberation process. This thesis proposes a framework for generating meta level controllers that select computational actions to perform by optimally trading off their benefit against their costs. The metalevel optimization problem is posed within a Markov Decision Process framework and is solved off-line to determine a policy for carrying out computations. Once the optimal policy is determined, it serves efficiently as an online metalevel controller that selects computational actions conditioned upon the current state of computation. Solving for the exact policy of the metalevel optimization problem becomes computationally intractable with problem size. A learning approach that takes advantage of the problem structure is proposed to generate approximate policies that are shown to perform relatively well in comparison to optimal policies. Metalevel policies are generated for two types of problem scenarios, distinguished by the representation of the cost of computation. In the first case, the cost of computation is explicitly defined as part of the problem description. In the second case, it is implicit in the timing constraints of problem. Results are presented to validate the beneficial effects of metalevel planning over traditional methods when the cost of computation has a significant effect on the utility of a plan.",
    "advisors": ["Nicholas Roy", "Milton B. Adams"],
    "text": "Learning bounded optimal behavior using Markov decision processes Creating agents that behave rationally in the real-world is one goal of Artificial Intelligence. A rational agent is one that takes, at each point in time, the optimal action such that its expected utility is maximized. However, to determine the optimal action the agent may need to engage in lengthy deliberations or computations. The effect of computation is generally not explicitly considered when performing deliberations. In reality, spending too much time in deliberation may yield high quality plans that do not satisfy the natural timing constraints of a problem, making them effectively useless. Enforcing shortened deliberation times may yield timely plans, but these may be of diminished utility. These two cases suggest the possibility of optimizing an agent's deliberation process. This thesis proposes a framework for generating meta level controllers that select computational actions to perform by optimally trading off their benefit against their costs. The metalevel optimization problem is posed within a Markov Decision Process framework and is solved off-line to determine a policy for carrying out computations. Once the optimal policy is determined, it serves efficiently as an online metalevel controller that selects computational actions conditioned upon the current state of computation. Solving for the exact policy of the metalevel optimization problem becomes computationally intractable with problem size. A learning approach that takes advantage of the problem structure is proposed to generate approximate policies that are shown to perform relatively well in comparison to optimal policies. Metalevel policies are generated for two types of problem scenarios, distinguished by the representation of the cost of computation. In the first case, the cost of computation is explicitly defined as part of the problem description. In the second case, it is implicit in the timing constraints of problem. Results are presented to validate the beneficial effects of metalevel planning over traditional methods when the cost of computation has a significant effect on the utility of a plan."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8888",
    "title": "Applications of axial and radial compressor dynamic system modeling",
    "abstract": "The presented work is a compilation of four different projects related to axial and centrifugal compression systems. The projects are related by the underlying dynamic system modeling approach that is common in all of them. Two types of models are introduced, suitable for modeling the dynamic behavior of axial and centrifugal compression systems: a compact single semi-actuator disk model, Model I, and a new modular multi semi-actuator disk model, Model II. The first project analyzes aerodynamically induced whirling forces in axial-flow compressors and a new unsteady low order model is introduced to predict the destabilizing whirling forces. The model consists of two parts: compressor Model I with the effect of tip-clearance induced distortion, and an aerodynamically induced force model. The modeling results are compared to experimental data obtained from the GE Aircraft Engines test program on compressor whirl. Previously outstanding whirl-instability issues are resolved, including prediction of the direction and magnitude of rotor whirl-inducing forces; such issues are important in the design of modern axial-flow compressors.",
    "advisors": ["James D. Paduano"],
    "text": "Applications of axial and radial compressor dynamic system modeling The presented work is a compilation of four different projects related to axial and centrifugal compression systems. The projects are related by the underlying dynamic system modeling approach that is common in all of them. Two types of models are introduced, suitable for modeling the dynamic behavior of axial and centrifugal compression systems: a compact single semi-actuator disk model, Model I, and a new modular multi semi-actuator disk model, Model II. The first project analyzes aerodynamically induced whirling forces in axial-flow compressors and a new unsteady low order model is introduced to predict the destabilizing whirling forces. The model consists of two parts: compressor Model I with the effect of tip-clearance induced distortion, and an aerodynamically induced force model. The modeling results are compared to experimental data obtained from the GE Aircraft Engines test program on compressor whirl. Previously outstanding whirl-instability issues are resolved, including prediction of the direction and magnitude of rotor whirl-inducing forces; such issues are important in the design of modern axial-flow compressors."
}, {
    "id": "oai:dspace.mit.edu:1721.1/90728",
    "title": "Persistent patrolling in the presence of adversarial observers",
    "abstract": "The majority of persistent patrolling strategies seek to minimize the time between visits or \"idleness\" of any target or location within an environment in an attempt to locate a hidden adversary as quickly as possible. Such strategies generally fail, however, to consider the game theoretic impacts of the adversary seeking to avoid the patroller's detection. The field of patrolling security games that addresses this two-player game is maturing with several authors posing the patrolling scenario as a leader-follower Stackelberg game where the adversary chooses to attack at a location and time as a best response to the patroller's policy. The state of the art grants the adversary complete global information regarding the patroller's location so as to choose the optimal time and location to attack, and this global information creates a considerable advantage for the adversary. We propose a significant improvement to this patrolling game state of the art by restricting the adversary access to only local information. We model the adversary as capable of collecting a sequence of local observations who must use this information to determine the optimal time to attack. This work proposes to find the optimal patrolling policy in different environments given this adversary model. We extensively study this patrolling game set on a perimeter with extensions to other environments. Teams of patrolling agents following this optimal policy achieve a higher capture probability, and we can determine the marginal improvement for each additional patroller. We pose several novel patrolling techniques inspired by a combination of discrete and continuous random walks, Markov processes, and random walks on Cayley graphs to ultimately model the game equilibrium when the team of patrollers execute so-called \"presence patrols.\" Police and military forces commonly execute this type of patrolling to project their presence across an environment in an effort to deter crime or aggression, and we provide a rigorous analysis of the trade-off between increased patrolling speed and decreased probability of detection.",
    "advisors": ["Emilio Frazzoli", " Jonathan How", "Philip Tokumaru"],
    "text": "Persistent patrolling in the presence of adversarial observers The majority of persistent patrolling strategies seek to minimize the time between visits or \"idleness\" of any target or location within an environment in an attempt to locate a hidden adversary as quickly as possible. Such strategies generally fail, however, to consider the game theoretic impacts of the adversary seeking to avoid the patroller's detection. The field of patrolling security games that addresses this two-player game is maturing with several authors posing the patrolling scenario as a leader-follower Stackelberg game where the adversary chooses to attack at a location and time as a best response to the patroller's policy. The state of the art grants the adversary complete global information regarding the patroller's location so as to choose the optimal time and location to attack, and this global information creates a considerable advantage for the adversary. We propose a significant improvement to this patrolling game state of the art by restricting the adversary access to only local information. We model the adversary as capable of collecting a sequence of local observations who must use this information to determine the optimal time to attack. This work proposes to find the optimal patrolling policy in different environments given this adversary model. We extensively study this patrolling game set on a perimeter with extensions to other environments. Teams of patrolling agents following this optimal policy achieve a higher capture probability, and we can determine the marginal improvement for each additional patroller. We pose several novel patrolling techniques inspired by a combination of discrete and continuous random walks, Markov processes, and random walks on Cayley graphs to ultimately model the game equilibrium when the team of patrollers execute so-called \"presence patrols.\" Police and military forces commonly execute this type of patrolling to project their presence across an environment in an effort to deter crime or aggression, and we provide a rigorous analysis of the trade-off between increased patrolling speed and decreased probability of detection."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112452",
    "title": "Electro-aerodynamic thrust for fixed-wing aircraft propulsion",
    "abstract": "Aviation operations negatively impact global climate, degrade surface air quality, and create noise. Towards mitigating these effects, this thesis considers electro-aerodynamic (EAD) propulsion, a form of in-atmosphere electrostatic propulsion, which requires no on-board propellant and has zero primary gaseous emissions. In addition, thrust generation has the potential to be nearly silent and requires no moving parts. Despite these advantages, however, EAD propulsion has yet to be implemented in fixed-wing aircraft, in part due to the limited understanding of EAD thruster performance. The objective of this thesis is to determine the feasibility and viability of EAD propulsion in fixed-wing aircraft applications. This thesis begins with a theoretical assessment of EAD thruster performance. This includes quantification of fundamental thrust density limits and the effect of interacting electric fields on thrust-to-power performance due to closely spaced electrode pairs. Additionally, performance as a function of altitude and vehicle flight speed is quantified, where thrust-to-power ratio is estimated to decrease as both increase. Next, this thesis experimentally assesses the achievable thrust density of EAD propulsion. Current and thrust generated from arrays of electrode pairs are observed to be a function of non-dimensional pair spacings for both parallel and staged operation. A thrust per unit area of 2 - 3 N/m² and per unit volume of 5 - 15 N/m³ are estimated, achieving approximately 50 and 10% of the corresponding one-dimensional space-charge limits, respectively. Results suggest that EAD propulsion is most readily viable at the small unmanned aerial vehicle (UAV) scale. Finally, based on the conclusions of the thrust density assessment, this thesis presents the development of a first-of-its-kind EAD-propelled, small-UAV prototype with the goal of achieving steady-level flight. A design space analysis is performed, determining that designs capable of steady-level flight potentially exist. The prototype development effort concludes with at-scale performance quantification of the primary EAD UAV subsystems. Results indicate that the achievable weight-to-thrust ratio is comparable to the vehicle lift-to-drag ratio. This thesis concludes that at the selected scale of the UAV prototype, EAD propulsion is potentially viable, and steady-level flight is, at worst, \"nearly\" feasible with the current design.",
    "advisors": ["Steven R.H. Barrett"],
    "text": "Electro-aerodynamic thrust for fixed-wing aircraft propulsion Aviation operations negatively impact global climate, degrade surface air quality, and create noise. Towards mitigating these effects, this thesis considers electro-aerodynamic (EAD) propulsion, a form of in-atmosphere electrostatic propulsion, which requires no on-board propellant and has zero primary gaseous emissions. In addition, thrust generation has the potential to be nearly silent and requires no moving parts. Despite these advantages, however, EAD propulsion has yet to be implemented in fixed-wing aircraft, in part due to the limited understanding of EAD thruster performance. The objective of this thesis is to determine the feasibility and viability of EAD propulsion in fixed-wing aircraft applications. This thesis begins with a theoretical assessment of EAD thruster performance. This includes quantification of fundamental thrust density limits and the effect of interacting electric fields on thrust-to-power performance due to closely spaced electrode pairs. Additionally, performance as a function of altitude and vehicle flight speed is quantified, where thrust-to-power ratio is estimated to decrease as both increase. Next, this thesis experimentally assesses the achievable thrust density of EAD propulsion. Current and thrust generated from arrays of electrode pairs are observed to be a function of non-dimensional pair spacings for both parallel and staged operation. A thrust per unit area of 2 - 3 N/m² and per unit volume of 5 - 15 N/m³ are estimated, achieving approximately 50 and 10% of the corresponding one-dimensional space-charge limits, respectively. Results suggest that EAD propulsion is most readily viable at the small unmanned aerial vehicle (UAV) scale. Finally, based on the conclusions of the thrust density assessment, this thesis presents the development of a first-of-its-kind EAD-propelled, small-UAV prototype with the goal of achieving steady-level flight. A design space analysis is performed, determining that designs capable of steady-level flight potentially exist. The prototype development effort concludes with at-scale performance quantification of the primary EAD UAV subsystems. Results indicate that the achievable weight-to-thrust ratio is comparable to the vehicle lift-to-drag ratio. This thesis concludes that at the selected scale of the UAV prototype, EAD propulsion is potentially viable, and steady-level flight is, at worst, \"nearly\" feasible with the current design."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16995",
    "title": "Probabilistic aerothermal design of compressor airfoils",
    "abstract": "Despite the generally accepted notion that geometric variability is undesirable in turbomachinery airfoils, little is known in detail about its impact on aerothermal compressor performance. In this work, statistical and probabilistic techniques were used to assess the impact of geometric and operating condition uncertainty on axial compressor performance. High-fidelity models of geometric variability were constructed from surface measurements of existing hardware using principal component analysis (PCA). A quasi-two-dimensional cascade analysis code, at the core of a parallel probabilistic analysis framework, was used to assess the impact of uncertainty on aerodynamic performance of compressor rotor airfoils. Three rotor blades with inlet relative Mach numbers of 0.82, 0.90 and 1.25 were considered. Discrepancies between nominal and mean loss (mean-shift) of up to 20% were observed. Loss and turning variability were found to grow linearly with geometric noise amplitude. A probabilistic, gradient-based approach to compressor blade optimization was presented. Probabilistic objectives, constraints and gradients are approximated using low-resolution Monte Carlo sampling. Test airfoils were optimized both deterministically and probabilistically and then analyzed probabilistically to account for geometric variability. Probabilistically redesigned airfoils exhibited reductions in mean loss of up to 25% and in loss variability of as much as 65% from corresponding values for deterministically redesigned airfoils.",
    "advisors": ["David L. Darmofal"],
    "text": "Probabilistic aerothermal design of compressor airfoils Despite the generally accepted notion that geometric variability is undesirable in turbomachinery airfoils, little is known in detail about its impact on aerothermal compressor performance. In this work, statistical and probabilistic techniques were used to assess the impact of geometric and operating condition uncertainty on axial compressor performance. High-fidelity models of geometric variability were constructed from surface measurements of existing hardware using principal component analysis (PCA). A quasi-two-dimensional cascade analysis code, at the core of a parallel probabilistic analysis framework, was used to assess the impact of uncertainty on aerodynamic performance of compressor rotor airfoils. Three rotor blades with inlet relative Mach numbers of 0.82, 0.90 and 1.25 were considered. Discrepancies between nominal and mean loss (mean-shift) of up to 20% were observed. Loss and turning variability were found to grow linearly with geometric noise amplitude. A probabilistic, gradient-based approach to compressor blade optimization was presented. Probabilistic objectives, constraints and gradients are approximated using low-resolution Monte Carlo sampling. Test airfoils were optimized both deterministically and probabilistically and then analyzed probabilistically to account for geometric variability. Probabilistically redesigned airfoils exhibited reductions in mean loss of up to 25% and in loss variability of as much as 65% from corresponding values for deterministically redesigned airfoils."
}, {
    "id": "oai:dspace.mit.edu:1721.1/82468",
    "title": "Fabrication of high-throughput critical-angle X-ray transmission gratings for wavelength-dispersive spectroscopy",
    "abstract": "The development of the critical-angle transmission (CAT) grating seeks both an order of magnitude improvement in the effective area, and a factor of three increase in the resolving power of future space-based, soft x-ray spectrometers. This will enhance further studies of the universe's make-up, such as the composition of the intergalactic medium, black holes, neutron stars and other high energy sources. Conceptually, x-rays are reflected in the device off nanoscale silicon grating bars at shallow angles, such that the diffraction orders are at the specular reflection angle, which is designed to be less than the critical-angle for total external reflection. This blazing effect boosts the efficiency of the device; however, the grating bars are required to form very deep channels to reflect all the incoming x-rays at shallow angles. Previous attempts to fabricate the grating were done with wet potassium hydroxide (KOH) etching of silicon. This process successfully fabricated small areas of grating and enabled a successful demonstration of the soft x-ray diffraction efficiency. However, the open-area fraction was limited to below 20 percent for four micron-tall CAT grating bars due to diagonal etch stops in the silicon crystal lattice. This limitation prevents the past fabrication technique from achieving the desired open-area fraction for a future x-ray observatory. New nanofabrication techniques are presented that can lead to CAT gratings with an open-area fraction in excess of 50 percent. Specifically, three major nanofabrication processes were developed and are described in detail; a two-dimensional, thermal, silicon dioxide mask, an integrated plasma-etch process to create free-standing, ultra-high aspect ratio gratings, and a polishing process to smooth the grating sidewalls. The two-dimensional mask was used to develop a record-performance deep reactive-ion etch (DRIE) for ultra-high aspect ratio gratings. The mask is the integration of a 5 micron and 200 nanometer-pitch grating into a single layer of 300 nanometer-thick thermal silicon dioxide. It spans 5 centimeters on a side, with vertical sidewalls, and is cleanable which enables consistent high quality etches. Experiments with chrome and polymer masking materials for DRIE are also presented. The DRIE was critical for the integrated process, which combined two plasma-etch processes on the front and back side of a silicon-on-insulator wafer. DRIE is not significantly affected by the silicon crystal orientation and therefore avoids the open-area restrictions of wet etching. The result of the process was a free-standing grating with a period of 200 nanometers, a depth of four microns, and a span of three centimeters. These free-standing gratings exceed the state-of-the-art by more than a factor of two in aspect ratio at the nanoscale. The sidewall roughness is one shortcoming of DRIE, which is often greater than 4 nanometers RMS, and it needs to be approximately one nanometer to efficiently reflect soft x-rays. To address this, the world's first reported nanoscale polishing process has been developed to smooth the sidewalls of DRIE'd, ultra-high aspect ratio silicon. This process utilizes potassium hydroxide etching, an anisotropic etch of single crystal silicon. Specifically, the [111] planes etch approximately 100 times slower than the non-[111] planes. A novel alignment technique is presented to align the CAT grating pattern to the [111] silicon planes to within 0.2 degrees. This precise alignment enables KOH to etch away sidewall roughness and slowly widen the channels without fully destroying the structure. The result of polishing was a reduction in sidewall roughness to approximately 1 nm RMS, while decreasing the widths of the grating bars.   In addition to the nanofabrication developments, this work provides a preliminary analysis of launching and deploying CAT gratings in space. The nanofabrication developments are focused towards the CAT grating; however, they have other applications as well. High quality masks have applications in MEMS structures and photonic devices. The free-standing structure as a stand-alone device has applications such as neutral mass spectroscopy, ultraviolet filtration, and x-ray phase contrast imaging. The polishing process is valuable to numerous optical applications where smooth sidewalls are critical, as well as filtration techniques which seek to maximize open-area.",
    "advisors": ["Mark L. Schattenburg"],
    "text": "Fabrication of high-throughput critical-angle X-ray transmission gratings for wavelength-dispersive spectroscopy The development of the critical-angle transmission (CAT) grating seeks both an order of magnitude improvement in the effective area, and a factor of three increase in the resolving power of future space-based, soft x-ray spectrometers. This will enhance further studies of the universe's make-up, such as the composition of the intergalactic medium, black holes, neutron stars and other high energy sources. Conceptually, x-rays are reflected in the device off nanoscale silicon grating bars at shallow angles, such that the diffraction orders are at the specular reflection angle, which is designed to be less than the critical-angle for total external reflection. This blazing effect boosts the efficiency of the device; however, the grating bars are required to form very deep channels to reflect all the incoming x-rays at shallow angles. Previous attempts to fabricate the grating were done with wet potassium hydroxide (KOH) etching of silicon. This process successfully fabricated small areas of grating and enabled a successful demonstration of the soft x-ray diffraction efficiency. However, the open-area fraction was limited to below 20 percent for four micron-tall CAT grating bars due to diagonal etch stops in the silicon crystal lattice. This limitation prevents the past fabrication technique from achieving the desired open-area fraction for a future x-ray observatory. New nanofabrication techniques are presented that can lead to CAT gratings with an open-area fraction in excess of 50 percent. Specifically, three major nanofabrication processes were developed and are described in detail; a two-dimensional, thermal, silicon dioxide mask, an integrated plasma-etch process to create free-standing, ultra-high aspect ratio gratings, and a polishing process to smooth the grating sidewalls. The two-dimensional mask was used to develop a record-performance deep reactive-ion etch (DRIE) for ultra-high aspect ratio gratings. The mask is the integration of a 5 micron and 200 nanometer-pitch grating into a single layer of 300 nanometer-thick thermal silicon dioxide. It spans 5 centimeters on a side, with vertical sidewalls, and is cleanable which enables consistent high quality etches. Experiments with chrome and polymer masking materials for DRIE are also presented. The DRIE was critical for the integrated process, which combined two plasma-etch processes on the front and back side of a silicon-on-insulator wafer. DRIE is not significantly affected by the silicon crystal orientation and therefore avoids the open-area restrictions of wet etching. The result of the process was a free-standing grating with a period of 200 nanometers, a depth of four microns, and a span of three centimeters. These free-standing gratings exceed the state-of-the-art by more than a factor of two in aspect ratio at the nanoscale. The sidewall roughness is one shortcoming of DRIE, which is often greater than 4 nanometers RMS, and it needs to be approximately one nanometer to efficiently reflect soft x-rays. To address this, the world's first reported nanoscale polishing process has been developed to smooth the sidewalls of DRIE'd, ultra-high aspect ratio silicon. This process utilizes potassium hydroxide etching, an anisotropic etch of single crystal silicon. Specifically, the [111] planes etch approximately 100 times slower than the non-[111] planes. A novel alignment technique is presented to align the CAT grating pattern to the [111] silicon planes to within 0.2 degrees. This precise alignment enables KOH to etch away sidewall roughness and slowly widen the channels without fully destroying the structure. The result of polishing was a reduction in sidewall roughness to approximately 1 nm RMS, while decreasing the widths of the grating bars.   In addition to the nanofabrication developments, this work provides a preliminary analysis of launching and deploying CAT gratings in space. The nanofabrication developments are focused towards the CAT grating; however, they have other applications as well. High quality masks have applications in MEMS structures and photonic devices. The free-standing structure as a stand-alone device has applications such as neutral mass spectroscopy, ultraviolet filtration, and x-ray phase contrast imaging. The polishing process is valuable to numerous optical applications where smooth sidewalls are critical, as well as filtration techniques which seek to maximize open-area."
}, {
    "id": "oai:dspace.mit.edu:1721.1/47790",
    "title": "Adaptive sampling and forecasting with mobile sensor networks",
    "abstract": "This thesis addresses planning of mobile sensor networks to extract the best information possible out of the environment to improve the (ensemble) forecast at some verification region in the future. To define the information reward associated with sensing paths, the mutual information is adopted to represent the influence of the measurement actions on the reduction of the uncertainty in the verification variables. The sensor networks planning problems are posed in both discrete and continuous time/space, each of which represents a different level of abstraction of the decision space. In the discrete setting, the targeting problem is formulated to determine the sequence of information-rich waypoints for mobile sensors. A backward formulation is developed to efficiently quantify the information rewards in this combinatorial decision process. This approach computes the reward of each possible sensing choice by propagating the information backwards from the verification time/space to the search space/time. It is shown that this backward method provides an equivalent solution to a standard forward approach, while only requiring the calculation of a single covariance update. This work proves that the backward approach works significantly faster than the forward approach for the ensemble-based representation. In the continuous setting, the motion planning problem that finds the best steering commands of the sensor platforms is posed. The main difficulty in this continuous decision lies in the quantification the mutual information between the future verification variables and a continuous history of the measurement.",
    "advisors": ["Jonathan P. How"],
    "text": "Adaptive sampling and forecasting with mobile sensor networks This thesis addresses planning of mobile sensor networks to extract the best information possible out of the environment to improve the (ensemble) forecast at some verification region in the future. To define the information reward associated with sensing paths, the mutual information is adopted to represent the influence of the measurement actions on the reduction of the uncertainty in the verification variables. The sensor networks planning problems are posed in both discrete and continuous time/space, each of which represents a different level of abstraction of the decision space. In the discrete setting, the targeting problem is formulated to determine the sequence of information-rich waypoints for mobile sensors. A backward formulation is developed to efficiently quantify the information rewards in this combinatorial decision process. This approach computes the reward of each possible sensing choice by propagating the information backwards from the verification time/space to the search space/time. It is shown that this backward method provides an equivalent solution to a standard forward approach, while only requiring the calculation of a single covariance update. This work proves that the backward approach works significantly faster than the forward approach for the ensemble-based representation. In the continuous setting, the motion planning problem that finds the best steering commands of the sensor platforms is posed. The main difficulty in this continuous decision lies in the quantification the mutual information between the future verification variables and a continuous history of the measurement."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120420",
    "title": "Network optimization in adversarial environments",
    "abstract": "Stochastic models have been dominant in network optimization theory for over two decades, due to their analytical tractability. However, an increasing number of real-world networked systems exhibit complex behaviors that cannot be captured by the simple stochastic models, such as networks under malicious attacks and networks with uncontrollable nodes. In this thesis, we study efficient network control policies that can optimize network performance in different types of complex environments. First, we investigate network optimization under adversarial dynamics, where the evolution of network conditions follows some non-stationary and possibly adversarial process. Such an adversarial network dynamics model can be used to capture many real-world scenarios, such as networks under Distributed Denial-of-Service (DDoS) attacks or ad-hoc networks with unpredictable mobility. We focus on two network control problems: (1) achieving network stability, and (2) maximizing network utility subject to stability constraints. New adversarial network models are developed to characterize the adversary's behavior, and the notion of regret is used to measure network performance in adversarial environments. We provide lower bounds on the regret performance that could be achieved by any causal control policies and analyze the performance of several network control policies (e.g., MaxWeight and Drift-plus-Penalty). It is proved that these policies are throughput-optimal and achieve good utility-delay tradeoffs even under adversarial dynamics. Second, we study network optimization in a partially-controllable environment where a subset of nodes are uncontrollable and adopt a stationary but unknown control policy. Such a partially-controllable model is of increasing importance in real-world networked systems such as overlay-underlay networks and uncooperative wireless networks. We consider the problem of stabilizing a partially-controllable network. It is shown that many well-known network control algorithms (e.g., MaxWeight) may fail to stabilize the network when some nodes adopt non-stabilizing policies. We first study the scenario where uncontrollable nodes use a queue-agnostic policy and propose a low-complexity throughput-optimal algorithm, called Tracking-MaxWeight (TMW), which enhances the original MaxWeight algorithm with an explicit learning of the policy used by uncontrollable nodes. Next, we investigate the scenario where uncontrollable nodes use a queue-dependent policy and the problem is formulated as an MDP with unknown queueing dynamics. We propose a new reinforcement learning algorithm, called Truncated Upper Confidence Reinforcement Learning (TUCRL), and prove that TUCRL achieves tunable three-way tradeoffs between throughput, delay and convergence rate under mild conditions. Finally, we focus on network optimization with adversarial uncontrollable nodes where the sequence of actions taken by uncontrollable nodes may be non-stationary and adversarial. We first investigate the network stability problem and develop a lower bound on the total queue length that can be achieved by any causal policy. We prove that the Tracking-MaxWeight (TMW) algorithm can achieve network stability under any given sequence of actions of adversarial nodes whenever possible. Next, we study the network utility maximization problem and provide a lower bound on the utility-delay tradeoff. We develop the Tracking Drift-plus-Penalty (TDP) algorithm that achieves tunable utility-delay tradeoffs.",
    "advisors": ["Eytan Modiano"],
    "text": "Network optimization in adversarial environments Stochastic models have been dominant in network optimization theory for over two decades, due to their analytical tractability. However, an increasing number of real-world networked systems exhibit complex behaviors that cannot be captured by the simple stochastic models, such as networks under malicious attacks and networks with uncontrollable nodes. In this thesis, we study efficient network control policies that can optimize network performance in different types of complex environments. First, we investigate network optimization under adversarial dynamics, where the evolution of network conditions follows some non-stationary and possibly adversarial process. Such an adversarial network dynamics model can be used to capture many real-world scenarios, such as networks under Distributed Denial-of-Service (DDoS) attacks or ad-hoc networks with unpredictable mobility. We focus on two network control problems: (1) achieving network stability, and (2) maximizing network utility subject to stability constraints. New adversarial network models are developed to characterize the adversary's behavior, and the notion of regret is used to measure network performance in adversarial environments. We provide lower bounds on the regret performance that could be achieved by any causal control policies and analyze the performance of several network control policies (e.g., MaxWeight and Drift-plus-Penalty). It is proved that these policies are throughput-optimal and achieve good utility-delay tradeoffs even under adversarial dynamics. Second, we study network optimization in a partially-controllable environment where a subset of nodes are uncontrollable and adopt a stationary but unknown control policy. Such a partially-controllable model is of increasing importance in real-world networked systems such as overlay-underlay networks and uncooperative wireless networks. We consider the problem of stabilizing a partially-controllable network. It is shown that many well-known network control algorithms (e.g., MaxWeight) may fail to stabilize the network when some nodes adopt non-stabilizing policies. We first study the scenario where uncontrollable nodes use a queue-agnostic policy and propose a low-complexity throughput-optimal algorithm, called Tracking-MaxWeight (TMW), which enhances the original MaxWeight algorithm with an explicit learning of the policy used by uncontrollable nodes. Next, we investigate the scenario where uncontrollable nodes use a queue-dependent policy and the problem is formulated as an MDP with unknown queueing dynamics. We propose a new reinforcement learning algorithm, called Truncated Upper Confidence Reinforcement Learning (TUCRL), and prove that TUCRL achieves tunable three-way tradeoffs between throughput, delay and convergence rate under mild conditions. Finally, we focus on network optimization with adversarial uncontrollable nodes where the sequence of actions taken by uncontrollable nodes may be non-stationary and adversarial. We first investigate the network stability problem and develop a lower bound on the total queue length that can be achieved by any causal policy. We prove that the Tracking-MaxWeight (TMW) algorithm can achieve network stability under any given sequence of actions of adversarial nodes whenever possible. Next, we study the network utility maximization problem and provide a lower bound on the utility-delay tradeoff. We develop the Tracking Drift-plus-Penalty (TDP) algorithm that achieves tunable utility-delay tradeoffs."
}, {
    "id": "oai:dspace.mit.edu:1721.1/71272",
    "title": "Carbon nanotube growth on challenging substrates : applications for carbon-fiber composites",
    "abstract": "Nanoengineered hierarchal fiber architectures are promising approaches towards improving the inter- and intralaminar mechanical properties (e.g., toughness and strength) and non-mechanical properties of advanced fiber-reinforced composites such as graphite/epoxy. One fiber architecture of particular interest is carbon fiber coated with radially-aligned arrays of carbon nanotubes (CNTs), which can enable through-thickness and interply matrix reinforcement of carbon-fiber-reinforced composites while simultaneously providing additional multifunctional benefits such as electrical and thermal conductivity enhancement. Growth of CNTs on carbon fibers can be achieved by chemical vapor deposition (CVD) techniques, however previous processes for doing so have resulted in a significant reduction in the tensile strength and stiffness of the carbon fibers. This thesis aims to develop an understanding of catalyst-substrate and CVD environment-substrate interactions relevant to maintaining fiber mechanical properties in the growth of CNTs on carbon fibers by CVD and to use this understanding to develop practical approaches for growing CNTs on carbon fibers that simultaneously preserve fiber properties. Novel oxide-based catalysts are demonstrated for the first time to be effective for both CNT growth and graphitization of amorphous carbon and are characterized using in situ metrology. These catalysts show promise for use on substrates that exhibit sensitivity to conventional metal catalysts (such as carbon fibers). New CVD processing techniques based on materials properties unique to this class of catalysts are presented and explored. Coatings for enabling growth of aligned CNTs on carbon fibers, coatings for improving adhesion of materials to carbon fibers, and coatings for facilitating low-temperature growth of CNTs on carbon fibers are developed. The mechanochemical responses of carbon fibers to high-temperature processing, exposure to CVD gases relevant for CNT growth, and in situ tensioning during CVD growth at high temperatures are investigated. Methods for growing CNTs on carbon fibers that enable aligned CNT morphologies and that preserve fiber properties are presented. A new system for optimizing CNT growth on carbon fibers with special considerations for oxide-based catalysts is described. Finally, recommendations for manufacturing hierarchal carbon fibers for composites in an industrially practical way are made.",
    "advisors": ["Brian L. Wardle"],
    "text": "Carbon nanotube growth on challenging substrates : applications for carbon-fiber composites Nanoengineered hierarchal fiber architectures are promising approaches towards improving the inter- and intralaminar mechanical properties (e.g., toughness and strength) and non-mechanical properties of advanced fiber-reinforced composites such as graphite/epoxy. One fiber architecture of particular interest is carbon fiber coated with radially-aligned arrays of carbon nanotubes (CNTs), which can enable through-thickness and interply matrix reinforcement of carbon-fiber-reinforced composites while simultaneously providing additional multifunctional benefits such as electrical and thermal conductivity enhancement. Growth of CNTs on carbon fibers can be achieved by chemical vapor deposition (CVD) techniques, however previous processes for doing so have resulted in a significant reduction in the tensile strength and stiffness of the carbon fibers. This thesis aims to develop an understanding of catalyst-substrate and CVD environment-substrate interactions relevant to maintaining fiber mechanical properties in the growth of CNTs on carbon fibers by CVD and to use this understanding to develop practical approaches for growing CNTs on carbon fibers that simultaneously preserve fiber properties. Novel oxide-based catalysts are demonstrated for the first time to be effective for both CNT growth and graphitization of amorphous carbon and are characterized using in situ metrology. These catalysts show promise for use on substrates that exhibit sensitivity to conventional metal catalysts (such as carbon fibers). New CVD processing techniques based on materials properties unique to this class of catalysts are presented and explored. Coatings for enabling growth of aligned CNTs on carbon fibers, coatings for improving adhesion of materials to carbon fibers, and coatings for facilitating low-temperature growth of CNTs on carbon fibers are developed. The mechanochemical responses of carbon fibers to high-temperature processing, exposure to CVD gases relevant for CNT growth, and in situ tensioning during CVD growth at high temperatures are investigated. Methods for growing CNTs on carbon fibers that enable aligned CNT morphologies and that preserve fiber properties are presented. A new system for optimizing CNT growth on carbon fibers with special considerations for oxide-based catalysts is described. Finally, recommendations for manufacturing hierarchal carbon fibers for composites in an industrially practical way are made."
}, {
    "id": "oai:dspace.mit.edu:1721.1/71453",
    "title": "Light pulse atom interferometry at short interrogation times for inertial navigation",
    "abstract": "Light pulse atom interferometry with cold atoms is a promising inertial sensing technology for high accuracy navigation. At present, laboratory atom interferometers match or surpass state of the art mechanical and optical inertial sensors in terms of sensitivity and long term stability. Conventional laboratory systems, however, do not achieve sufficient bandwidth or dynamic range to operate in a dynamic environment; furthermore, the size, weight and power of laboratory sensors are unsuitable for many applications. In this thesis, atom interferometry is realized at shorter interrogation times (<15 ms as opposed to >100 ms), in which the required sensitivity, bandwidth and dynamic range of navigation systems becomes feasible. A cold atom gravimeter testbed using atom interferometry with stimulated Raman transitions was developed, which executed the entire measurement cycle in a compact vacuum cell (~ ~ 80 cc). The system demonstrated an inferred sensitivity of 2 [mu]g[square root] Hz for an interrogation time of 2T = 10 ms (based on measured phase SNR, scale factor, and repetition rate). With realistic improvements to the apparatus, it could achieve a sensitivity of <1 [mu]g[square root]Hz, advancing toward the realization of a compact, atom-based inertial measurement unit with unprecedented performance. In addition, a method for increasing the momentum splitting of Raman pulse interferometers with sequential Raman pulses was demonstrated, and interferometer area was increased by up to a factor of nine without altering the interrogation time (corresponding to a momentum splitting of 18hk, the largest reported for Raman pulse interferometry). Composite Raman pulses were implemented to improve population transfer efficiency, which limits the achievable increase in precision. Finally, the effect of coherent population trapping (CPT) induced by Raman pulse atom optics was identified as a source of systematic phase shifts in the [pi]/2 - [pi] - [pi]/2 interferometer used for sensing acceleration and rotation. CPT effects were modeled in a three-level (A) atom, and were experimentally characterized using atom interferometry. Based on the magnitude of measured coherences induced by Raman pulse atom optics, phase shifts of several milliradians should occur for a typical GHz-scale laser detuning. A method for suppressing this bias in realistic operation by Raman beam propagation direction reversal is proposed.",
    "advisors": ["Richard Stoner", " Shaoul Ezekiel", "Wolfgang Ketterle"],
    "text": "Light pulse atom interferometry at short interrogation times for inertial navigation Light pulse atom interferometry with cold atoms is a promising inertial sensing technology for high accuracy navigation. At present, laboratory atom interferometers match or surpass state of the art mechanical and optical inertial sensors in terms of sensitivity and long term stability. Conventional laboratory systems, however, do not achieve sufficient bandwidth or dynamic range to operate in a dynamic environment; furthermore, the size, weight and power of laboratory sensors are unsuitable for many applications. In this thesis, atom interferometry is realized at shorter interrogation times (<15 ms as opposed to >100 ms), in which the required sensitivity, bandwidth and dynamic range of navigation systems becomes feasible. A cold atom gravimeter testbed using atom interferometry with stimulated Raman transitions was developed, which executed the entire measurement cycle in a compact vacuum cell (~ ~ 80 cc). The system demonstrated an inferred sensitivity of 2 [mu]g[square root] Hz for an interrogation time of 2T = 10 ms (based on measured phase SNR, scale factor, and repetition rate). With realistic improvements to the apparatus, it could achieve a sensitivity of <1 [mu]g[square root]Hz, advancing toward the realization of a compact, atom-based inertial measurement unit with unprecedented performance. In addition, a method for increasing the momentum splitting of Raman pulse interferometers with sequential Raman pulses was demonstrated, and interferometer area was increased by up to a factor of nine without altering the interrogation time (corresponding to a momentum splitting of 18hk, the largest reported for Raman pulse interferometry). Composite Raman pulses were implemented to improve population transfer efficiency, which limits the achievable increase in precision. Finally, the effect of coherent population trapping (CPT) induced by Raman pulse atom optics was identified as a source of systematic phase shifts in the [pi]/2 - [pi] - [pi]/2 interferometer used for sensing acceleration and rotation. CPT effects were modeled in a three-level (A) atom, and were experimentally characterized using atom interferometry. Based on the magnitude of measured coherences induced by Raman pulse atom optics, phase shifts of several milliradians should occur for a typical GHz-scale laser detuning. A method for suppressing this bias in realistic operation by Raman beam propagation direction reversal is proposed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32464",
    "title": "Electromagnetic formation flight dipole solution planning",
    "abstract": "Electromagnetic Formation Flight (EMFF) describes the concept of using electromagnets (coupled with reaction wheels) to provide all of the necessary forces and torques needed to maintain a satellite's relative position and attitude in a formation of satellites. With EMFF, this formation can be controlled without the use of traditional thrusters. This thesis demonstrates the feasibility of the EMFF system. First three different models for the forces and torques produced by the electromagnets are created, and the equations of motion are developed and described. The equations of motion are determined to be polynomial functions of each satellite's magnetic dipole (which is directly related to the current in the electromagnets). Methods for solving the equations of motion are presented along with examples showing that any desired maneuver can be performed as long as the formation's center of mass is not required to change. An effect of Newton's third law causes torques to be applied to the individual vehicles in a direction opposite to the formation's angular acceleration. Reaction wheels are used to absorb the angular momentum. Next, the thesis describes methods for distributing the angular momentum evenly among the satellites. Finally, the additional challenges of operating in low Earth orbit are addressed. These include operating in the Earth's gravitational field (including the J₂ disturbance), and operating in the Earth's magnetic field. The latter is a mixed blessing due to the large disturbance torques produced from the Earth's magnetic field. However, it is shown in this thesis that it is possible to control and utilize these disturbance torques.",
    "advisors": ["Raymond Sedwick"],
    "text": "Electromagnetic formation flight dipole solution planning Electromagnetic Formation Flight (EMFF) describes the concept of using electromagnets (coupled with reaction wheels) to provide all of the necessary forces and torques needed to maintain a satellite's relative position and attitude in a formation of satellites. With EMFF, this formation can be controlled without the use of traditional thrusters. This thesis demonstrates the feasibility of the EMFF system. First three different models for the forces and torques produced by the electromagnets are created, and the equations of motion are developed and described. The equations of motion are determined to be polynomial functions of each satellite's magnetic dipole (which is directly related to the current in the electromagnets). Methods for solving the equations of motion are presented along with examples showing that any desired maneuver can be performed as long as the formation's center of mass is not required to change. An effect of Newton's third law causes torques to be applied to the individual vehicles in a direction opposite to the formation's angular acceleration. Reaction wheels are used to absorb the angular momentum. Next, the thesis describes methods for distributing the angular momentum evenly among the satellites. Finally, the additional challenges of operating in low Earth orbit are addressed. These include operating in the Earth's gravitational field (including the J₂ disturbance), and operating in the Earth's magnetic field. The latter is a mixed blessing due to the large disturbance torques produced from the Earth's magnetic field. However, it is shown in this thesis that it is possible to control and utilize these disturbance torques."
}, {
    "id": "oai:dspace.mit.edu:1721.1/29747",
    "title": "Real-time probabilistic collision avoidance for autonomous vehicles, using order reductive conflict metrics",
    "abstract": "Contemporary collision avoidance systems such as the Traffic Alert and Collision Avoidance System (TCAS) have proven their effectiveness in the Commercial Aviation (CA) industry within the last decade. Yet, TCAS and many systems like it represent attempts at collision avoidance that do not fully recognize the uncertain nature of a conflict event. Most systems circumvent probabilistic representation through simplifying approximations and pre-compiled notions of hazard space, since probabilistic representation of collision in three dimensions is considered to be an intractable problem. Recent developments by Kuchar and Yang[70] and Paielli and Erzberger[50] have shown that collision avoidance may be cast as a probabilistic state-space problem. Innovative solution approaches may then allow systems of this nature to probe collision risk in real-time, based on real-time state estimates. The research documented in this thesis further develops the probabilistic approach for the non-cooperative, two-vehicle problem as applied in real-time to autonomous aircraft. The research is kept in a general form, thereby warranting application to a wide variety of multi-dimensional collision avoidance applications and scenario geometries. The work primarily improves the state of the art through the creation of order reductive collision metrics in order to simplify the intractable problem of multi-dimensional collision risk calculation. As a result, a tractable, real-time, probabilistic algorithm is developed for the calculation of collision risk as a function of time.",
    "advisors": ["John J. Deyst"],
    "text": "Real-time probabilistic collision avoidance for autonomous vehicles, using order reductive conflict metrics Contemporary collision avoidance systems such as the Traffic Alert and Collision Avoidance System (TCAS) have proven their effectiveness in the Commercial Aviation (CA) industry within the last decade. Yet, TCAS and many systems like it represent attempts at collision avoidance that do not fully recognize the uncertain nature of a conflict event. Most systems circumvent probabilistic representation through simplifying approximations and pre-compiled notions of hazard space, since probabilistic representation of collision in three dimensions is considered to be an intractable problem. Recent developments by Kuchar and Yang[70] and Paielli and Erzberger[50] have shown that collision avoidance may be cast as a probabilistic state-space problem. Innovative solution approaches may then allow systems of this nature to probe collision risk in real-time, based on real-time state estimates. The research documented in this thesis further develops the probabilistic approach for the non-cooperative, two-vehicle problem as applied in real-time to autonomous aircraft. The research is kept in a general form, thereby warranting application to a wide variety of multi-dimensional collision avoidance applications and scenario geometries. The work primarily improves the state of the art through the creation of order reductive collision metrics in order to simplify the intractable problem of multi-dimensional collision risk calculation. As a result, a tractable, real-time, probabilistic algorithm is developed for the calculation of collision risk as a function of time."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98586",
    "title": "Design and evaluation of distributed spacecraft missions for multi-angular Earth observation",
    "abstract": "Distributed Spacecraft Missions (DSMs) are gaining momentum in their application to Earth science missions owing to their ability to increase observation sampling in spatial, spectral and temporal dimensions. This thesis identifies a gap in the angular sampling abilities of monolithic spacecraft in Earth observation missions and proposes to use distributed spacecraft to address this gap. The science performance metric is chosen to be Bidirectional Reflectance-Distribution Function (BRDF), which describes the directional and spectral variation of reflectance of an optically thick surface element at any time instant. Airborne instruments are the gold standard for BRDF estimation (e.g. Cloud Absorption Radiometer/CAR). They can collect thousands of reflectance measurements at the same ground spot, but are localized in space and time. Spaceborne platforms estimate BRDF by combining angular measurements over time, made along-track, cross-track or by autonomous pointing. However, their plane of data acquisition is very restricted with respect to the sun and the target itself might change over time of acquisition. Formations with spectrometer payloads can make multi-spectral reflectance measurements of a ground target, at many zenith and azimuthal angles simultaneously and estimate the angular signature of the surface. Constellations with overlapping ground spots can capture the angular components of global and temporally varying science products. This work demonstrates the performance impact and feasibility of a BRDF estimation mission using a systems engineering tool (driven by model based systems engineering or MBSE), intricately coupled with a science evaluation tool (driven by observing system simulation experiments or OSSEs). Formations and payload pointing strategies are optimized to maximize angular spread and minimize estimation errors. The effect of angular spread on spatial, spectral and radiometric sampling dimensions is quantified for available spectrometer payloads that fit within the 6U CubeSat standard. Technical feasibility within 6U is verified for attitude determination and control, propulsion, communication and onboard processing modules. DSM architectures are generated and compared to each other and monoliths, in terms of BRDF, albedo, gross primary productivity and total outgoing radiation. Performance is benchmarked with respect to data from previous airborne campaigns (NASA CAR), tower measurements (AMSPEC II) and ideal values from radiative transfer or climate models (UMGLO, COART); and accepted BRDF models. A formation of 6 small satellites produces lesser average error (21.82%) than larger monoliths (23.2%) over extended time periods, purely in terms of angular sampling benefits. The monolithic albedo error of 3.6% is shown to be outperformed by a formation of 3 satellites (1.86%), when arranged optimally and by a formation of 5 satellites (3.36%) when arranged in any way. An 8-satellite formation pushes albedo errors to 0.67% and reduces gross primary productivity errors from 89.77% (monolithic) to 78.69%.",
    "advisors": ["Olivier L. de Weck"],
    "text": "Design and evaluation of distributed spacecraft missions for multi-angular Earth observation Distributed Spacecraft Missions (DSMs) are gaining momentum in their application to Earth science missions owing to their ability to increase observation sampling in spatial, spectral and temporal dimensions. This thesis identifies a gap in the angular sampling abilities of monolithic spacecraft in Earth observation missions and proposes to use distributed spacecraft to address this gap. The science performance metric is chosen to be Bidirectional Reflectance-Distribution Function (BRDF), which describes the directional and spectral variation of reflectance of an optically thick surface element at any time instant. Airborne instruments are the gold standard for BRDF estimation (e.g. Cloud Absorption Radiometer/CAR). They can collect thousands of reflectance measurements at the same ground spot, but are localized in space and time. Spaceborne platforms estimate BRDF by combining angular measurements over time, made along-track, cross-track or by autonomous pointing. However, their plane of data acquisition is very restricted with respect to the sun and the target itself might change over time of acquisition. Formations with spectrometer payloads can make multi-spectral reflectance measurements of a ground target, at many zenith and azimuthal angles simultaneously and estimate the angular signature of the surface. Constellations with overlapping ground spots can capture the angular components of global and temporally varying science products. This work demonstrates the performance impact and feasibility of a BRDF estimation mission using a systems engineering tool (driven by model based systems engineering or MBSE), intricately coupled with a science evaluation tool (driven by observing system simulation experiments or OSSEs). Formations and payload pointing strategies are optimized to maximize angular spread and minimize estimation errors. The effect of angular spread on spatial, spectral and radiometric sampling dimensions is quantified for available spectrometer payloads that fit within the 6U CubeSat standard. Technical feasibility within 6U is verified for attitude determination and control, propulsion, communication and onboard processing modules. DSM architectures are generated and compared to each other and monoliths, in terms of BRDF, albedo, gross primary productivity and total outgoing radiation. Performance is benchmarked with respect to data from previous airborne campaigns (NASA CAR), tower measurements (AMSPEC II) and ideal values from radiative transfer or climate models (UMGLO, COART); and accepted BRDF models. A formation of 6 small satellites produces lesser average error (21.82%) than larger monoliths (23.2%) over extended time periods, purely in terms of angular sampling benefits. The monolithic albedo error of 3.6% is shown to be outperformed by a formation of 3 satellites (1.86%), when arranged optimally and by a formation of 5 satellites (3.36%) when arranged in any way. An 8-satellite formation pushes albedo errors to 0.67% and reduces gross primary productivity errors from 89.77% (monolithic) to 78.69%."
}, {
    "id": "oai:dspace.mit.edu:1721.1/40305",
    "title": "Model-constrained optimization methods for reduction of parameterized large-scale systems",
    "abstract": "Most model reduction techniques employ a projection framework that utilizes a reduced-space basis. The basis is usually formed as the span of a set of solutions of the large-scale system, which are computed for selected values (samples) of input parameters and forcing inputs. In existing model reduction techniques, choosing where and how many samples to generate has been, in general, an ad-hoc process. A key challenge is therefore how to systematically sample the input space, which is of high dimension for many applications of interest. This thesis proposes and analyzes a model-constrained greedy-based adaptive sampling approach in which the parametric input sampling problem is formulated as an optimization problem that targets an error estimation of reduced model output prediction. The method solves the optimization problem to find a locally-optimal point in parameter space where the error estimator is largest, updates the reduced basis with information at this optimal sample location, forms a new reduced model, and repeats the process. Therefore, we use a systematic, adaptive error metric based on the ability of the reduced-order model to capture the outputs of interest in order to choose the snapshot locations that are locally the worst case scenarios.",
    "advisors": ["Karen E. Willcox"],
    "text": "Model-constrained optimization methods for reduction of parameterized large-scale systems Most model reduction techniques employ a projection framework that utilizes a reduced-space basis. The basis is usually formed as the span of a set of solutions of the large-scale system, which are computed for selected values (samples) of input parameters and forcing inputs. In existing model reduction techniques, choosing where and how many samples to generate has been, in general, an ad-hoc process. A key challenge is therefore how to systematically sample the input space, which is of high dimension for many applications of interest. This thesis proposes and analyzes a model-constrained greedy-based adaptive sampling approach in which the parametric input sampling problem is formulated as an optimization problem that targets an error estimation of reduced model output prediction. The method solves the optimization problem to find a locally-optimal point in parameter space where the error estimator is largest, updates the reduced basis with information at this optimal sample location, forms a new reduced model, and repeats the process. Therefore, we use a systematic, adaptive error metric based on the ability of the reduced-order model to capture the outputs of interest in order to choose the snapshot locations that are locally the worst case scenarios."
}, {
    "id": "oai:dspace.mit.edu:1721.1/72842",
    "title": "Optimization models for joint airline pricing and seat inventory control : multiple products, multiple periods",
    "abstract": "Pricing and revenue management are two essential levers to optimize the sales of an airline's seat inventory and maximize revenues. Over the past few decades, they have generated a great deal of research but have typically been studied and optimized separately. On the one hand, the pricing process focused on demand segmentation and optimal fares, regardless of any capacity constraints. On the other hand, researchers in revenue management developed algorithms to set booking limits by fare product, given a set of fares and capacity constraints. This thesis develops several approaches to solve for the optimal fares and booking limits jointly and simultaneously. The underlying demand volume in an airline market is modeled as a function of the fares. We propose an initial approach to the two-product, two-period revenue optimization problem by first assuming that the demand is deterministic. We show that the booking limit on sales of the lower-priced product is unnecessary in this case, allowing us to simplify the optimization problem. We then develop a stochastic optimization model and analyze the combined impacts of fares and booking limits on the total number of accepted bookings when the underlying demand is uncertain. We demonstrate that this joint optimization approach can provide a 3-4% increase in revenues from a traditional pricing and revenue management practices. The stochastic model is then extended to the joint pricing and seat inventory control optimization problem for booking horizons involving more than two booking periods, as is the case in reality. A generalized methodology for optimization is presented, and we show that the complexity of the joint optimization problem increases substantially with the number of booking periods. We thus develop three heuristics. Simulations for a three-period problem show that all heuristics outperform the deterministic optimization model. In addition, two of the heuristics can provide revenues close to those obtained with the stochastic model. This thesis provides a basis for the integration of pricing and revenue management. The combined effects of fares and booking limits on the number of accepted bookings, and thus on the revenues, are explicitly taken into account in our joint optimization models. We showed that the proposed approaches can further enhance revenues.",
    "advisors": ["Peter P. Belobaba"],
    "text": "Optimization models for joint airline pricing and seat inventory control : multiple products, multiple periods Pricing and revenue management are two essential levers to optimize the sales of an airline's seat inventory and maximize revenues. Over the past few decades, they have generated a great deal of research but have typically been studied and optimized separately. On the one hand, the pricing process focused on demand segmentation and optimal fares, regardless of any capacity constraints. On the other hand, researchers in revenue management developed algorithms to set booking limits by fare product, given a set of fares and capacity constraints. This thesis develops several approaches to solve for the optimal fares and booking limits jointly and simultaneously. The underlying demand volume in an airline market is modeled as a function of the fares. We propose an initial approach to the two-product, two-period revenue optimization problem by first assuming that the demand is deterministic. We show that the booking limit on sales of the lower-priced product is unnecessary in this case, allowing us to simplify the optimization problem. We then develop a stochastic optimization model and analyze the combined impacts of fares and booking limits on the total number of accepted bookings when the underlying demand is uncertain. We demonstrate that this joint optimization approach can provide a 3-4% increase in revenues from a traditional pricing and revenue management practices. The stochastic model is then extended to the joint pricing and seat inventory control optimization problem for booking horizons involving more than two booking periods, as is the case in reality. A generalized methodology for optimization is presented, and we show that the complexity of the joint optimization problem increases substantially with the number of booking periods. We thus develop three heuristics. Simulations for a three-period problem show that all heuristics outperform the deterministic optimization model. In addition, two of the heuristics can provide revenues close to those obtained with the stochastic model. This thesis provides a basis for the integration of pricing and revenue management. The combined effects of fares and booking limits on the number of accepted bookings, and thus on the revenues, are explicitly taken into account in our joint optimization models. We showed that the proposed approaches can further enhance revenues."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28912",
    "title": "Hidden Markov Models for analysis of pilot instrument scanning and attention switching",
    "abstract": "(cont.) high workload. The results of another flight simulation experiment demonstrated how the pilots' attention budgeting among these tasks estimated by HMM analysis, combined with the pilots' eye-movement statistical results, could enhance a cockpit display format study. The experiments demonstrated what additional insights can be obtained by incorporating HMM analysis into the analysis of pilots' eye movements.",
    "advisors": ["Charles M. Oman"],
    "text": "Hidden Markov Models for analysis of pilot instrument scanning and attention switching (cont.) high workload. The results of another flight simulation experiment demonstrated how the pilots' attention budgeting among these tasks estimated by HMM analysis, combined with the pilots' eye-movement statistical results, could enhance a cockpit display format study. The experiments demonstrated what additional insights can be obtained by incorporating HMM analysis into the analysis of pilots' eye movements."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120419",
    "title": "Catalysis and manufacturing of two-scale hierarchical nano- and microfiber advanced aerospace fiber-reinforced plastic composites",
    "abstract": "The development of hierarchical nanoengineered \"fuzzy fiber\" aerospace fiber-reinforced plastic (FRP) composite laminates holds the potential for enabling future generations of lightweight, durable, and multifunctional vehicle structures. By reinforcing the weak matrix-rich regions between individual fibers and plies, the circumferential growth of aligned carbon nanotubes (A-CNTs) on carbon microfibers (CFs) enables new composites with improved strength, toughness, electrical and thermal properties. While these improvements have been empirically demonstrated on alumina fiber FRPs, CNT growth degrades the CFs and sacrifices in-plane FRP properties for the benefits of CNT reinforcement. This thesis presents novel and scalable methods for realizing advanced fuzzy carbon fiber reinforced plastic (fuzzy CFRP) composite laminates with retained CF and interlaminar strength properties. Earth-abundant sodium (Na) is revealed as a new facile catalyst for CNT growth that allows for direct deposition of the catalyst precursor on carbon fabrics without any fiber pretreatments. This new catalyst discovery also enables high-yield CNT growth on a variety of low-temperature substrates. Simultaneously, this finding has led to other novel findings in carbon nanostructure catalysis including a core-shell morphology and the use of other alkali metals (e.g., potassium) for CNT growth. Towards the development of advanced composites, vacuum-assisted resin infusion processes are studied and refined, resulting in high-quality woven and unidirectional fuzzy (via Na-catalysis of CNTs) CFRP laminates. Growth uniformity improvement studies yielded strategies for increasing the quantity of CNT reinforcement within matrix-rich regions. Moreover, a new commercial unidirectional fabric enables the first retention of CF properties concomitant with interlaminar shear strength retention in the fuzzy CFRP architecture. The contributions of this thesis extend beyond CF composites: techniques developed for improving fuzzy CF synthesis were applied towards demonstrating A-CNT growth on SiC woven fabric, desired for creating damage tolerant and multifunctional lightweight vehicle systems. These advances pave the way for improvements in catalysis of nanostructures, electronics interfaces, energy storage devices, and advanced composite materials.",
    "advisors": ["Brian L. Wardle"],
    "text": "Catalysis and manufacturing of two-scale hierarchical nano- and microfiber advanced aerospace fiber-reinforced plastic composites The development of hierarchical nanoengineered \"fuzzy fiber\" aerospace fiber-reinforced plastic (FRP) composite laminates holds the potential for enabling future generations of lightweight, durable, and multifunctional vehicle structures. By reinforcing the weak matrix-rich regions between individual fibers and plies, the circumferential growth of aligned carbon nanotubes (A-CNTs) on carbon microfibers (CFs) enables new composites with improved strength, toughness, electrical and thermal properties. While these improvements have been empirically demonstrated on alumina fiber FRPs, CNT growth degrades the CFs and sacrifices in-plane FRP properties for the benefits of CNT reinforcement. This thesis presents novel and scalable methods for realizing advanced fuzzy carbon fiber reinforced plastic (fuzzy CFRP) composite laminates with retained CF and interlaminar strength properties. Earth-abundant sodium (Na) is revealed as a new facile catalyst for CNT growth that allows for direct deposition of the catalyst precursor on carbon fabrics without any fiber pretreatments. This new catalyst discovery also enables high-yield CNT growth on a variety of low-temperature substrates. Simultaneously, this finding has led to other novel findings in carbon nanostructure catalysis including a core-shell morphology and the use of other alkali metals (e.g., potassium) for CNT growth. Towards the development of advanced composites, vacuum-assisted resin infusion processes are studied and refined, resulting in high-quality woven and unidirectional fuzzy (via Na-catalysis of CNTs) CFRP laminates. Growth uniformity improvement studies yielded strategies for increasing the quantity of CNT reinforcement within matrix-rich regions. Moreover, a new commercial unidirectional fabric enables the first retention of CF properties concomitant with interlaminar shear strength retention in the fuzzy CFRP architecture. The contributions of this thesis extend beyond CF composites: techniques developed for improving fuzzy CF synthesis were applied towards demonstrating A-CNT growth on SiC woven fabric, desired for creating damage tolerant and multifunctional lightweight vehicle systems. These advances pave the way for improvements in catalysis of nanostructures, electronics interfaces, energy storage devices, and advanced composite materials."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46555",
    "title": "Model-based planning through constraint and causal order decomposition",
    "abstract": "One of the major challenges in autonomous planning and sequencing is the theoretical complexity of planning problems. Even a simple STRIPS planning problem is PSPACEcomplete, and depending on the expressivity of the planning problem, the complexity of the problem can be EXPTIME-complete or worse. This thesis improves on current approaches to sequencing the engineering operations of a spacecraft or ground-based asset through the explicit use of verifiable models and a decomposition approach to planning. Based on specifications of system behavior, the planner generates control sequences of engineering operations that achieve mission objectives specified by an operator. This work is novel in three ways. First, an innovative \"divide-and-conquer\" approach is used to assure efficiency and scalability of the planner. The key to the approach is in its combined use of constraint decomposition and causal order decomposition. This technique provides the means to decompose the problem into a set of subproblems and to identify the ordering by which each subproblem should be solved, thus reducing, and possibly eliminating, search. Second, the decomposed planning framework is able to solve complex planning problems with state constraints and temporally extended goals. Such complex system behavior is specified as concurrent, constraint automata (CCA) that provide the expressiveness necessary to model the behavior of the system components and their interactions. The mission objective is described as a desired evolution of goal states called a qualitative state plan (QSP), explicitly capturing the intent of the operators. Finally, the planner generates a partially-ordered plan called a qualitative control plan (QCP) that provides additional execution robustness through temporal flexibility. We demonstrate the decomposed approach to Model-based planning on a scenario based on the ongoing Autonomous Sciencecraft Experiment, onboard EO-1 spacecraft. The EO-1 problem has a large state space with well over 660 quintillion states, 6.6 x 10²⁰.",
    "advisors": ["Brian C. Williams"],
    "text": "Model-based planning through constraint and causal order decomposition One of the major challenges in autonomous planning and sequencing is the theoretical complexity of planning problems. Even a simple STRIPS planning problem is PSPACEcomplete, and depending on the expressivity of the planning problem, the complexity of the problem can be EXPTIME-complete or worse. This thesis improves on current approaches to sequencing the engineering operations of a spacecraft or ground-based asset through the explicit use of verifiable models and a decomposition approach to planning. Based on specifications of system behavior, the planner generates control sequences of engineering operations that achieve mission objectives specified by an operator. This work is novel in three ways. First, an innovative \"divide-and-conquer\" approach is used to assure efficiency and scalability of the planner. The key to the approach is in its combined use of constraint decomposition and causal order decomposition. This technique provides the means to decompose the problem into a set of subproblems and to identify the ordering by which each subproblem should be solved, thus reducing, and possibly eliminating, search. Second, the decomposed planning framework is able to solve complex planning problems with state constraints and temporally extended goals. Such complex system behavior is specified as concurrent, constraint automata (CCA) that provide the expressiveness necessary to model the behavior of the system components and their interactions. The mission objective is described as a desired evolution of goal states called a qualitative state plan (QSP), explicitly capturing the intent of the operators. Finally, the planner generates a partially-ordered plan called a qualitative control plan (QCP) that provides additional execution robustness through temporal flexibility. We demonstrate the decomposed approach to Model-based planning on a scenario based on the ongoing Autonomous Sciencecraft Experiment, onboard EO-1 spacecraft. The EO-1 problem has a large state space with well over 660 quintillion states, 6.6 x 10²⁰."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46558",
    "title": "Cooperative exploration under communication constraints",
    "abstract": "The cooperative exploration problem necessarily involves communication among agents, while the spatial separation inherent in this task places fundamental limits on the amount of data that can be transmitted. However, the impact of limited communication on the exploration process has not been fully characterized. Existing exploration algorithms do not realistically model the tradeoff between expansion, which allows more rapid exploration of the area of interest, and maintenance of close relative proximity among agents, which facilitates communication. This thesis develops new algorithms applicable to the problem of cooperative exploration under communication constraints. The exploration problem is decomposed into two parts. In the first part, cooperative exploration is considered in the context of a hierarchical communication framework known as a mobile backbone network. In such a network, mobile backbone nodes, which have good mobility and communication capabilities, provide communication support for regular nodes, which are constrained in movement and communication capabilities but which can sense the environment. New exact and approximation algorithms are developed for throughput optimization in networks composed of stationary regular nodes, and new extensions are formulated to take advantage of regular node mobility. These algorithms are then applied to a cooperative coverage problem. In the second part of this work, techniques are developed for utilizing a given level of throughput in the context of cooperative estimation. The mathematical properties of the information form of the Kalman filter are leveraged in the development of two algorithms for selecting highly informative portions of the information matrix for transmission. One algorithm, a fully polynomial time approximation scheme, provides provably good results in computationally tractable time for problem instances of a particular structure. The other, a heuristic method applicable to instances of arbitrary matrix structure, performs very well in simulation for randomly-generated problems of realistic dimension.",
    "advisors": ["Jonathan P. How", "Eytan Modiano"],
    "text": "Cooperative exploration under communication constraints The cooperative exploration problem necessarily involves communication among agents, while the spatial separation inherent in this task places fundamental limits on the amount of data that can be transmitted. However, the impact of limited communication on the exploration process has not been fully characterized. Existing exploration algorithms do not realistically model the tradeoff between expansion, which allows more rapid exploration of the area of interest, and maintenance of close relative proximity among agents, which facilitates communication. This thesis develops new algorithms applicable to the problem of cooperative exploration under communication constraints. The exploration problem is decomposed into two parts. In the first part, cooperative exploration is considered in the context of a hierarchical communication framework known as a mobile backbone network. In such a network, mobile backbone nodes, which have good mobility and communication capabilities, provide communication support for regular nodes, which are constrained in movement and communication capabilities but which can sense the environment. New exact and approximation algorithms are developed for throughput optimization in networks composed of stationary regular nodes, and new extensions are formulated to take advantage of regular node mobility. These algorithms are then applied to a cooperative coverage problem. In the second part of this work, techniques are developed for utilizing a given level of throughput in the context of cooperative estimation. The mathematical properties of the information form of the Kalman filter are leveraged in the development of two algorithms for selecting highly informative portions of the information matrix for transmission. One algorithm, a fully polynomial time approximation scheme, provides provably good results in computationally tractable time for problem instances of a particular structure. The other, a heuristic method applicable to instances of arbitrary matrix structure, performs very well in simulation for randomly-generated problems of realistic dimension."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35585",
    "title": "Chebyshev spectral method for incompressible viscous flow with boundary layer control via suction or blowing",
    "abstract": "The MISES quasi 3-D design/analysis code implements a two-equation integral method with empirical closure relations to solve the boundary layer flow problem with or without suction, but lacks the option of flow control via blowing. The integral method is parameterized with the shape parameter H _ 6*/0 which cannot be applied to the blowing problem since 0 - 0 downstream of the injection slot causing H -, co - a computational disaster. In this thesis, two alternate approaches are proposed to solve the blowing problem. First, a two-equation integral method parameterized with the profile parameters of a multi-deck representation of a turbulent jet based on Coles' law of the wake was formulated. The appearance of spurious singularities in the Jacobian matrices associated with the system of equations and the vector of unknowns prevented this method from being implemented. Second, a Chebyshev spectral method using the wall function technique was applied to the defect form of the incompressible viscous momentum equation. A turbulent jet profile was computed with N = 40 modes, a number low enough to allow the method's implementation into the MISES framework.",
    "advisors": ["Mark Drela"],
    "text": "Chebyshev spectral method for incompressible viscous flow with boundary layer control via suction or blowing The MISES quasi 3-D design/analysis code implements a two-equation integral method with empirical closure relations to solve the boundary layer flow problem with or without suction, but lacks the option of flow control via blowing. The integral method is parameterized with the shape parameter H _ 6*/0 which cannot be applied to the blowing problem since 0 - 0 downstream of the injection slot causing H -, co - a computational disaster. In this thesis, two alternate approaches are proposed to solve the blowing problem. First, a two-equation integral method parameterized with the profile parameters of a multi-deck representation of a turbulent jet based on Coles' law of the wake was formulated. The appearance of spurious singularities in the Jacobian matrices associated with the system of equations and the vector of unknowns prevented this method from being implemented. Second, a Chebyshev spectral method using the wall function technique was applied to the defect form of the incompressible viscous momentum equation. A turbulent jet profile was computed with N = 40 modes, a number low enough to allow the method's implementation into the MISES framework."
}, {
    "id": "oai:dspace.mit.edu:1721.1/59550",
    "title": "Integrated modeling and design of lightweight, active mirrors for launch survival and on-orbit performance",
    "abstract": "Lightweight, active mirrors are an enabling technology for large aperture, space-based optical systems. These mirrors have the potential to improve the optical resolution and sensitivity beyond what is currently possible. However, as with all technology development programs, there are remaining issues to be solved before such mirrors can be used in operational systems. As of yet, no efforts have been made to explore the design space or optimize the design of lightweight mirrors across operational environments and constraints. The extremely harsh launch environment is of particular concern because launch survival constraints could dictate aspects of the mirror design. Additionally, on-orbit optical performance, in terms of high spatial frequency wavefront error and low spatial frequency correctability, are extremely important aspects of mirror design. Due to the lack of heritage systems, the best designs for lightweight, active mirrors are not immediately apparent. Therefore, an integrated modeling methodology for technology development programs is developed. This framework uses model-based design and evolutionary models to guide the technology development program. This methodology is applied to the lightweight, active mirror systems of interest. The mirrors are modeled and analyzed in two distinct environments: on-orbit and during launch. The on-orbit model and analysis are presented, as well as the designs with the best optical performance, which tend to have many ribs and actuators. Additionally, a dynamic state-space model of the launch environment is developed. The designs that are most likely to survive launch have few ribs and actuators, directly in conflict with the best on-orbit designs. Launch load alleviation techniques, including techniques making use of the existing embedded actuators, are also implemented to increase the probability of launch survival. Finally, a fully integrated trade space analysis of designs is shown, along with families of designs that perform well with respect to different mission objectives. The integrated modeling approach allows for the seamless combination of the two analysis, as well as a way in which to determine the best performing designs. By using this approach, the model can be updated to include any new insights and to reflect the current state of the technology, making it useful throughout the life cycle of the program.",
    "advisors": ["David W. Miller", " Karen Willcox, Jonathan P. How", "Howard A. MacEwen"],
    "text": "Integrated modeling and design of lightweight, active mirrors for launch survival and on-orbit performance Lightweight, active mirrors are an enabling technology for large aperture, space-based optical systems. These mirrors have the potential to improve the optical resolution and sensitivity beyond what is currently possible. However, as with all technology development programs, there are remaining issues to be solved before such mirrors can be used in operational systems. As of yet, no efforts have been made to explore the design space or optimize the design of lightweight mirrors across operational environments and constraints. The extremely harsh launch environment is of particular concern because launch survival constraints could dictate aspects of the mirror design. Additionally, on-orbit optical performance, in terms of high spatial frequency wavefront error and low spatial frequency correctability, are extremely important aspects of mirror design. Due to the lack of heritage systems, the best designs for lightweight, active mirrors are not immediately apparent. Therefore, an integrated modeling methodology for technology development programs is developed. This framework uses model-based design and evolutionary models to guide the technology development program. This methodology is applied to the lightweight, active mirror systems of interest. The mirrors are modeled and analyzed in two distinct environments: on-orbit and during launch. The on-orbit model and analysis are presented, as well as the designs with the best optical performance, which tend to have many ribs and actuators. Additionally, a dynamic state-space model of the launch environment is developed. The designs that are most likely to survive launch have few ribs and actuators, directly in conflict with the best on-orbit designs. Launch load alleviation techniques, including techniques making use of the existing embedded actuators, are also implemented to increase the probability of launch survival. Finally, a fully integrated trade space analysis of designs is shown, along with families of designs that perform well with respect to different mission objectives. The integrated modeling approach allows for the seamless combination of the two analysis, as well as a way in which to determine the best performing designs. By using this approach, the model can be updated to include any new insights and to reflect the current state of the technology, making it useful throughout the life cycle of the program."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101490",
    "title": "A decomposition-based approach to uncertainty quantification of multicomponent systems",
    "abstract": "To support effective decision making, engineers should comprehend and manage various uncertainties throughout the design process. In today's modern systems, quantifying uncertainty can become cumbersome and computationally intractable for one individual or group to manage. This is particularly true for systems comprised of a large number of components. In many cases, these components may be developed by different groups and even run on different computational platforms, making it challenging or even impossible to achieve tight integration of the various models. This thesis presents an approach for overcoming this challenge by establishing a divide-and-conquer methodology, inspired by the decomposition-based approaches used in multidisciplinary analysis and optimization. Specifically, this research focuses on uncertainty analysis, also known as forward propagation of uncertainties, and sensitivity analysis. We present an approach for decomposing the uncertainty analysis task amongst the various components comprising a feed-forward system and synthesizing the local uncertainty analyses into a system uncertainty analysis. Our proposed decomposition-based multicomponent uncertainty analysis approach is shown to converge in distribution to the traditional all-at-once Monte Carlo uncertainty analysis under certain conditions. Our decomposition-based sensitivity analysis approach, which is founded on our decomposition-based uncertainty analysis algorithm, apportions the system output variance among the system inputs. The proposed decomposition-based uncertainty quantification approach is demonstrated on a multidisciplinary gas turbine system and is compared to the traditional all-at-once Monte Carlo uncertainty quantification approach. To extend the decomposition-based uncertainty quantification approach to high dimensions, this thesis proposes a novel optimization formulation to estimate statistics from a target distribution using random samples generated from a (different) proposal distribution. The proposed approach employs the well-defined and determinable empirical distribution function associated with the available samples. The resulting optimization problem is shown to be a single linear equality and box-constrained quadratic program and can be solved efficiently using optimization algorithms that scale well to high dimensions. Under some conditions restricting the class of distribution functions, the solution of the optimization problem yields importance weights that are shown to result in convergence in the Ll-norm of the weighted proposal empirical distribution function to the target distribution function, as the number of samples tends to infinity. Results on a variety of test cases show that the proposed approach performs well in comparison with other well-known approaches. The proposed approaches presented herein are demonstrated on a realistic application; environmental impacts of aviation technologies and operations. The results demonstrate that the decomposition-based uncertainty quantification approach can effectively quantify the uncertainty of a multicomponent system for which the models are housed in different locations and owned by different groups.",
    "advisors": ["Karen E. Willcox", " Youssef Marzouk", "Douglas Allaire"],
    "text": "A decomposition-based approach to uncertainty quantification of multicomponent systems To support effective decision making, engineers should comprehend and manage various uncertainties throughout the design process. In today's modern systems, quantifying uncertainty can become cumbersome and computationally intractable for one individual or group to manage. This is particularly true for systems comprised of a large number of components. In many cases, these components may be developed by different groups and even run on different computational platforms, making it challenging or even impossible to achieve tight integration of the various models. This thesis presents an approach for overcoming this challenge by establishing a divide-and-conquer methodology, inspired by the decomposition-based approaches used in multidisciplinary analysis and optimization. Specifically, this research focuses on uncertainty analysis, also known as forward propagation of uncertainties, and sensitivity analysis. We present an approach for decomposing the uncertainty analysis task amongst the various components comprising a feed-forward system and synthesizing the local uncertainty analyses into a system uncertainty analysis. Our proposed decomposition-based multicomponent uncertainty analysis approach is shown to converge in distribution to the traditional all-at-once Monte Carlo uncertainty analysis under certain conditions. Our decomposition-based sensitivity analysis approach, which is founded on our decomposition-based uncertainty analysis algorithm, apportions the system output variance among the system inputs. The proposed decomposition-based uncertainty quantification approach is demonstrated on a multidisciplinary gas turbine system and is compared to the traditional all-at-once Monte Carlo uncertainty quantification approach. To extend the decomposition-based uncertainty quantification approach to high dimensions, this thesis proposes a novel optimization formulation to estimate statistics from a target distribution using random samples generated from a (different) proposal distribution. The proposed approach employs the well-defined and determinable empirical distribution function associated with the available samples. The resulting optimization problem is shown to be a single linear equality and box-constrained quadratic program and can be solved efficiently using optimization algorithms that scale well to high dimensions. Under some conditions restricting the class of distribution functions, the solution of the optimization problem yields importance weights that are shown to result in convergence in the Ll-norm of the weighted proposal empirical distribution function to the target distribution function, as the number of samples tends to infinity. Results on a variety of test cases show that the proposed approach performs well in comparison with other well-known approaches. The proposed approaches presented herein are demonstrated on a realistic application; environmental impacts of aviation technologies and operations. The results demonstrate that the decomposition-based uncertainty quantification approach can effectively quantify the uncertainty of a multicomponent system for which the models are housed in different locations and owned by different groups."
}, {
    "id": "oai:dspace.mit.edu:1721.1/40894",
    "title": "Dynamics and control of electromagnetic satellite formations",
    "abstract": "Satellite formation flying is an enabling technology for many space missions, especially for space-based telescopes. Usually there is a tight formation-keeping requirement that may need constant expenditure of fuel or at least fuel is required for formation reconfiguration. Electromagnetic Formation Flying (EMFF) is a novel concept that uses superconducting electromagnetic coils to provide forces and torques between different satellites in a formation which enables the control of all the relative degrees of freedom. With EMFF, the life-span of the mission becomes independent of the fuel available on board. Also the contamination of optics or sensitive formation instruments, due to thruster plumes, is avoided. This comes at the cost of coupled and nonlinear dynamics of the formation and makes the control problem a challenging one. In this thesis, the dynamics for a general N-satellite electromagnetic formation will be derived for both deep space missions and Low Earth Orbit (LEO) formations. Nonlinear control laws using adaptive techniques will be derived for general formations in LEO. Angular momentum management in LEO is a problem for EMFF due to interaction of the magnetic dipoles with the Earth's magnetic field. A solution of this problem for general Electromagnetic (EM) formations will be presented in the form of a dipole polarity switching control law. For EMFF, the formation reconfiguration problem is a nonlinear and constrained optimal time control problem as fuel cost for EMFF is zero. Two different methods of trajectory generation, namely feedback motion planning using the Artificial Potential Function Method (APFM) and optimal trajectory generation using the Legendre Pseudospectral method, will be derived for general EM Formations.",
    "advisors": ["David W. Miller"],
    "text": "Dynamics and control of electromagnetic satellite formations Satellite formation flying is an enabling technology for many space missions, especially for space-based telescopes. Usually there is a tight formation-keeping requirement that may need constant expenditure of fuel or at least fuel is required for formation reconfiguration. Electromagnetic Formation Flying (EMFF) is a novel concept that uses superconducting electromagnetic coils to provide forces and torques between different satellites in a formation which enables the control of all the relative degrees of freedom. With EMFF, the life-span of the mission becomes independent of the fuel available on board. Also the contamination of optics or sensitive formation instruments, due to thruster plumes, is avoided. This comes at the cost of coupled and nonlinear dynamics of the formation and makes the control problem a challenging one. In this thesis, the dynamics for a general N-satellite electromagnetic formation will be derived for both deep space missions and Low Earth Orbit (LEO) formations. Nonlinear control laws using adaptive techniques will be derived for general formations in LEO. Angular momentum management in LEO is a problem for EMFF due to interaction of the magnetic dipoles with the Earth's magnetic field. A solution of this problem for general Electromagnetic (EM) formations will be presented in the form of a dipole polarity switching control law. For EMFF, the formation reconfiguration problem is a nonlinear and constrained optimal time control problem as fuel cost for EMFF is zero. Two different methods of trajectory generation, namely feedback motion planning using the Artificial Potential Function Method (APFM) and optimal trajectory generation using the Legendre Pseudospectral method, will be derived for general EM Formations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/29901",
    "title": "Multivariable isoperformance methodology for precision opto-mechanical systems",
    "abstract": "Precision opto-mechanical systems, such as space telescopes, combine structures, optics and controls in order to meet stringent pointing and phasing requirements. In this context a novel approach to the design of complex, multi-disciplinary systems is presented in the form of a multivariable isoperformance methodology. The isoperformance approach first finds a point design within a given topology, which meets the performance requirements with sufficient margins. The performance outputs are then treated as equality constraints and the non-uniqueness of the design space is exploited by trading key disturbance, plant, optics and controls parameters with respect to each other. Three algorithms (branch-and-bound, tangential front following and vector spline approximation) are developed for the bivariate and multivariable problem. The challenges of large order models are addressed by presenting a fast diagonal Lyapunov solver, apriori error bounds for model reduction as well as a governing sensitivity equation for similarity transformed state space realizations. Specific applications developed with this technique are error budgeting and multiobjective design optimization. The goal of the multiobjective design optimization is to achieve a design which is pareto optimal, such that multiple competing objectives can be satisfied within the performance invariant set. Thus, situations are avoided where very costly and hard-to-meet requirements are levied onto one subsystem, while other subsystems hold substantial margins.",
    "advisors": ["David W. Miller"],
    "text": "Multivariable isoperformance methodology for precision opto-mechanical systems Precision opto-mechanical systems, such as space telescopes, combine structures, optics and controls in order to meet stringent pointing and phasing requirements. In this context a novel approach to the design of complex, multi-disciplinary systems is presented in the form of a multivariable isoperformance methodology. The isoperformance approach first finds a point design within a given topology, which meets the performance requirements with sufficient margins. The performance outputs are then treated as equality constraints and the non-uniqueness of the design space is exploited by trading key disturbance, plant, optics and controls parameters with respect to each other. Three algorithms (branch-and-bound, tangential front following and vector spline approximation) are developed for the bivariate and multivariable problem. The challenges of large order models are addressed by presenting a fast diagonal Lyapunov solver, apriori error bounds for model reduction as well as a governing sensitivity equation for similarity transformed state space realizations. Specific applications developed with this technique are error budgeting and multiobjective design optimization. The goal of the multiobjective design optimization is to achieve a design which is pareto optimal, such that multiple competing objectives can be satisfied within the performance invariant set. Thus, situations are avoided where very costly and hard-to-meet requirements are levied onto one subsystem, while other subsystems hold substantial margins."
}]