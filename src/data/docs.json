[{
    "id": "oai:dspace.mit.edu:1721.1/91832",
    "title": "Modelling signal interactions with application to financial time series",
    "abstract": "In this thesis, we concern ourselves with the problem of reasoning over a set of objects evolving over time that are coupled through interaction structures that are themselves changing over time. We focus on inferring time-varying interaction structures among a set of objects from sequences of noisy time series observations with the caveat that the number of interaction structures is not known a priori. Furthermore, we aim to develop an inference procedure that operates online, meaning that it is capable of incorporating observations as they arrive. We develop an online nonparametric inference algorithm called Online Nonparametric Switching Temporal Interaction Model inference (ONSTIM). ONSTIM is an extension of the work of Dzunic and Fisher [1], who employ a linear Gaussian model with time-varying transition dynamics as the generative graphical model for observed time series. Like Dzunic and Fisher, we employ sampling approaches to perform inference. Instead of presupposing a fixed number of interaction structures, however, we allow for proposal of new interaction structures sampled from a prior distribution as new observations are incorporated into our inference. We then demonstrate the viability of ONSTIM on synthetic and financial datasets. Synthetic datasets are sampled from a generative model, and financial datasets are constructed from the price data of various US stocks and ETFs.",
    "advisors": ["John W. Fisher III"],
    "text": "Modelling signal interactions with application to financial time series In this thesis, we concern ourselves with the problem of reasoning over a set of objects evolving over time that are coupled through interaction structures that are themselves changing over time. We focus on inferring time-varying interaction structures among a set of objects from sequences of noisy time series observations with the caveat that the number of interaction structures is not known a priori. Furthermore, we aim to develop an inference procedure that operates online, meaning that it is capable of incorporating observations as they arrive. We develop an online nonparametric inference algorithm called Online Nonparametric Switching Temporal Interaction Model inference (ONSTIM). ONSTIM is an extension of the work of Dzunic and Fisher [1], who employ a linear Gaussian model with time-varying transition dynamics as the generative graphical model for observed time series. Like Dzunic and Fisher, we employ sampling approaches to perform inference. Instead of presupposing a fixed number of interaction structures, however, we allow for proposal of new interaction structures sampled from a prior distribution as new observations are incorporated into our inference. We then demonstrate the viability of ONSTIM on synthetic and financial datasets. Synthetic datasets are sampled from a generative model, and financial datasets are constructed from the price data of various US stocks and ETFs."
}, {
    "id": "oai:dspace.mit.edu:1721.1/9080",
    "title": "Attack development for intrusion detector evaluation",
    "abstract": "An important goal of the 1999 DARPA Intrusion Detection Evaluation was to promote the development of intrusion detection systems that can detect new attacks. This thesis describes UNIX attacks developed for the 1999 DARPA Evaluation. Some attacks were new in 1999 and others were stealthy versions of 1998 User-to-Root attacks designed to evade network-based intrusion detection systems. In addition, new and old attacks were fragmented at the packet level to evade network-based intrusion detection systems. Results demonstrated that new and stealthy attacks were not detected well. New attacks that were never seen before were not detected by any network-based systems. Stealthy attacks, modified to be difficult to detect by network intrusion detection systems, were detected less accurately than clear versions. The best network-based system detected 42% of clear attacks and only 11% of stealthy attacks at 10 false alarms per day. A few attacks and background sessions modified with packet modifications eluded network intrusion detection systems causing them to generate false negatives and false positives due to improper TCP/IP reassembly.",
    "advisors": ["Richard Lippmann"],
    "text": "Attack development for intrusion detector evaluation An important goal of the 1999 DARPA Intrusion Detection Evaluation was to promote the development of intrusion detection systems that can detect new attacks. This thesis describes UNIX attacks developed for the 1999 DARPA Evaluation. Some attacks were new in 1999 and others were stealthy versions of 1998 User-to-Root attacks designed to evade network-based intrusion detection systems. In addition, new and old attacks were fragmented at the packet level to evade network-based intrusion detection systems. Results demonstrated that new and stealthy attacks were not detected well. New attacks that were never seen before were not detected by any network-based systems. Stealthy attacks, modified to be difficult to detect by network intrusion detection systems, were detected less accurately than clear versions. The best network-based system detected 42% of clear attacks and only 11% of stealthy attacks at 10 false alarms per day. A few attacks and background sessions modified with packet modifications eluded network intrusion detection systems causing them to generate false negatives and false positives due to improper TCP/IP reassembly."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34099",
    "title": "A VLSI systolic array processor for complex singular value decomposition",
    "abstract": "undefined The singular value decomposition is one example of a variety of more complex routines that are finding use in modern high performance signal processing systems. In the interest of achieving the maximum possible performance, a systolic array processor for computing the singular value decomposition of an arbitrary complex matrix was designed using a silicon compiler system. This system allows for ease of design by specification of the processor architecture in a high level language, utilizing parts from a variety of cell libraries, while still benefiting from the power of custom VLSI. The level of abstraction provided by this system allowed more complex functional units to be built up from existing simple library parts. A novel fast interpolation cell for computation of square roots and inverse square roots was designed, allowing for a new algebraic approach to the singular value decomposition problem. The processors connect together in a systolic array to maximize computational efficiency while minimizing overhead due to high communication requirements. ",
    "advisors": ["Srinivas Devadas", "Steven R. Broadstone"],
    "text": "A VLSI systolic array processor for complex singular value decomposition undefined The singular value decomposition is one example of a variety of more complex routines that are finding use in modern high performance signal processing systems. In the interest of achieving the maximum possible performance, a systolic array processor for computing the singular value decomposition of an arbitrary complex matrix was designed using a silicon compiler system. This system allows for ease of design by specification of the processor architecture in a high level language, utilizing parts from a variety of cell libraries, while still benefiting from the power of custom VLSI. The level of abstraction provided by this system allowed more complex functional units to be built up from existing simple library parts. A novel fast interpolation cell for computation of square roots and inverse square roots was designed, allowing for a new algebraic approach to the singular value decomposition problem. The processors connect together in a systolic array to maximize computational efficiency while minimizing overhead due to high communication requirements. "
}, {
    "id": "oai:dspace.mit.edu:1721.1/9082",
    "title": "Recovery of 3D articulated motion from 2D correspondences",
    "abstract": "Recovering the 3D motion of the human body is an important problem in computer vision. Applications that would benefit from 3D motion include physical therapy, computer user interfaces, and 3D animation. Unfortunately, recovering 3D position from one 2D camera is an inherently ill-posed problem. This thesis focuses on recovery of 3D motion of an articulated model using 2D correspondences from an existing 2D tracker. A number of constraints are used to aid in reconstruction: (i) kinematic constraints from a 3D kinematic model, (ii) joint angle limits, (iii) dynamic smoothing, and (iv) key frames. These methods are used successfully to recover 3D motion from video sequences. Also presented is a method for recovering 3D motion from motion capture data, as well as a method for recovering kinematic model connectivity from 2D tracks.",
    "advisors": ["W. Eric L. Grimson"],
    "text": "Recovery of 3D articulated motion from 2D correspondences Recovering the 3D motion of the human body is an important problem in computer vision. Applications that would benefit from 3D motion include physical therapy, computer user interfaces, and 3D animation. Unfortunately, recovering 3D position from one 2D camera is an inherently ill-posed problem. This thesis focuses on recovery of 3D motion of an articulated model using 2D correspondences from an existing 2D tracker. A number of constraints are used to aid in reconstruction: (i) kinematic constraints from a 3D kinematic model, (ii) joint angle limits, (iii) dynamic smoothing, and (iv) key frames. These methods are used successfully to recover 3D motion from video sequences. Also presented is a method for recovering 3D motion from motion capture data, as well as a method for recovering kinematic model connectivity from 2D tracks."
}, {
    "id": "oai:dspace.mit.edu:1721.1/41030",
    "title": "Stepped-frequency pulse train waveforms for improved radar range resolution",
    "abstract": "The traditional approach of improving radar range resolution using a linear frequency modulated chirp signal requires the full width of the frequency spectrum, which is not feasible in the UHF band due to interference or frequency allocation for other purposes. In this study a linear frequency modulated chirp signal is approximated using two stepped-frequency pulse train waveforms, a continuous wave pulse train and a linear frequency modulated pulse train. The continuous wave pulse train consists of a series of single frequency pulses, each at a different frequency. It is found to be susceptible to corruption due to target motion. The linear frequency modulated pulse train consists of linear frequency modulation within pulses, each at a different center frequency. Simulations are used to demonstrate that both approaches approximate a linear frequency modulated chirp signal, and performance is degraded when there is a gap in the frequency band or if there is phase distortion due to target motion. However, it is shown that a linear frequency modulated pulse train with frequency overlaps between pulses can be used to reduce or eliminate phase distortions resulting from target motion provided the target is moving with constant velocity. The validity of the technique is demonstrated by non-coherently processing radar data from an internal moving target simulator and data from actual planes to resolve targets from their reflected image in order to estimate target height.",
    "advisors": ["Herbert M. Aumann", "James K. Roberge"],
    "text": "Stepped-frequency pulse train waveforms for improved radar range resolution The traditional approach of improving radar range resolution using a linear frequency modulated chirp signal requires the full width of the frequency spectrum, which is not feasible in the UHF band due to interference or frequency allocation for other purposes. In this study a linear frequency modulated chirp signal is approximated using two stepped-frequency pulse train waveforms, a continuous wave pulse train and a linear frequency modulated pulse train. The continuous wave pulse train consists of a series of single frequency pulses, each at a different frequency. It is found to be susceptible to corruption due to target motion. The linear frequency modulated pulse train consists of linear frequency modulation within pulses, each at a different center frequency. Simulations are used to demonstrate that both approaches approximate a linear frequency modulated chirp signal, and performance is degraded when there is a gap in the frequency band or if there is phase distortion due to target motion. However, it is shown that a linear frequency modulated pulse train with frequency overlaps between pulses can be used to reduce or eliminate phase distortions resulting from target motion provided the target is moving with constant velocity. The validity of the technique is demonstrated by non-coherently processing radar data from an internal moving target simulator and data from actual planes to resolve targets from their reflected image in order to estimate target height."
}, {
    "id": "oai:dspace.mit.edu:1721.1/91815",
    "title": "PhysioMiner : a scalable cloud based framework for physiological waveform mining",
    "abstract": "This work presents PhysioMiner, a large scale machine learning and analytics framework for physiological waveform mining. It is a scalable and flexible solution for researchers and practitioners to build predictive models from physiological time series data. It allows users to specify arbitrary features and conditions to train the model, computing everything in parallel in the cloud. PhysioMiner is tested on a large dataset of electrocardiography (ECG) from 6000 patients in the MIMIC database. Signals are cleaned and processed, and features are extracted per period. A total of 1.2 billion heart beats were processed and 26 billion features were extracted resulting in half a terabyte database. These features were aggregated for windows corresponding to patient events. These aggregated features were fed into DELPHI, a multi algorithm multi parameter cloud based system to build a predictive model. An area under the curve of 0.693 was achieved for an acute hypotensive event prediction from the ECG waveform alone. The results demonstrate the scalability and flexibility of PhysioMiner on real world data. PhysioMiner will be an important tool for researchers to spend less time building systems, and more time building predictive models.",
    "advisors": ["Kalyan Veeramachaneni", "Una-May O'Reilly"],
    "text": "PhysioMiner : a scalable cloud based framework for physiological waveform mining This work presents PhysioMiner, a large scale machine learning and analytics framework for physiological waveform mining. It is a scalable and flexible solution for researchers and practitioners to build predictive models from physiological time series data. It allows users to specify arbitrary features and conditions to train the model, computing everything in parallel in the cloud. PhysioMiner is tested on a large dataset of electrocardiography (ECG) from 6000 patients in the MIMIC database. Signals are cleaned and processed, and features are extracted per period. A total of 1.2 billion heart beats were processed and 26 billion features were extracted resulting in half a terabyte database. These features were aggregated for windows corresponding to patient events. These aggregated features were fed into DELPHI, a multi algorithm multi parameter cloud based system to build a predictive model. An area under the curve of 0.693 was achieved for an acute hypotensive event prediction from the ECG waveform alone. The results demonstrate the scalability and flexibility of PhysioMiner on real world data. PhysioMiner will be an important tool for researchers to spend less time building systems, and more time building predictive models."
}, {
    "id": "oai:dspace.mit.edu:1721.1/36678",
    "title": "Store Buffers : implementing single cycle store instructions in write-through, write-back and set associative caches",
    "abstract": "This thesis proposes a new mechanism, called Store Buffers, for implementing single cycle store instructions in a pipelined processor. Single cycle store instructions are difficult to implement because in most cases the tag check must be performed before the data can be written into the data cache. Store buffers allow a store instruction to read the cache tag as it. passes through the pipe while keeping the store instruction data buffered in a backup register until the data cache is free. This strategy guarantees single cycle store execution without increasing the hit access time or degrading the performance of the data cache for simple direct-mapped caches, as well as for more complex set associative and write-back caches. As larger caches are incorporated on-chip, the speed of store instructions becomes an increasingly important part of the overall performance. The first part of the thesis describes the design and implementation of store buffers in write through, write-back, direct-mapped and set associative caches. The second part describes the implementation and simulation of store buffers in a 6-stage pipeline with a direct mapped write-through pipelined cache. The performance of this method is compared to other cache write techniques. Preliminary results show that store buffers perform better than other store strategies under high IO latencies and cache thrashing. With as few as three buffers, they significantly reduce the number of cycles per instruction.",
    "advisors": ["Anant Agarwal"],
    "text": "Store Buffers : implementing single cycle store instructions in write-through, write-back and set associative caches This thesis proposes a new mechanism, called Store Buffers, for implementing single cycle store instructions in a pipelined processor. Single cycle store instructions are difficult to implement because in most cases the tag check must be performed before the data can be written into the data cache. Store buffers allow a store instruction to read the cache tag as it. passes through the pipe while keeping the store instruction data buffered in a backup register until the data cache is free. This strategy guarantees single cycle store execution without increasing the hit access time or degrading the performance of the data cache for simple direct-mapped caches, as well as for more complex set associative and write-back caches. As larger caches are incorporated on-chip, the speed of store instructions becomes an increasingly important part of the overall performance. The first part of the thesis describes the design and implementation of store buffers in write through, write-back, direct-mapped and set associative caches. The second part describes the implementation and simulation of store buffers in a 6-stage pipeline with a direct mapped write-through pipelined cache. The performance of this method is compared to other cache write techniques. Preliminary results show that store buffers perform better than other store strategies under high IO latencies and cache thrashing. With as few as three buffers, they significantly reduce the number of cycles per instruction."
}, {
    "id": "oai:dspace.mit.edu:1721.1/47720",
    "title": "A compact windowing system for the Curl environment",
    "abstract": "Curl is a programming language for creating web content. It is capable of running under a Linux operating system using X windows for a graphics interface. There are applications of Curl which do not need such a large and complex graphics system as Xwindows. The compact windowing system eliminates much of the unnecessary functionality of X windows while implementing the necessary components. The system makes use of the video hardware using svgalib for Linux. The system also make use of the Freetype TrueType font library to use TrueType fonts for text rendering. This eliminates one incompatibility between the Linux and MS Windows versions of Curl.",
    "advisors": ["Steve Ward"],
    "text": "A compact windowing system for the Curl environment Curl is a programming language for creating web content. It is capable of running under a Linux operating system using X windows for a graphics interface. There are applications of Curl which do not need such a large and complex graphics system as Xwindows. The compact windowing system eliminates much of the unnecessary functionality of X windows while implementing the necessary components. The system makes use of the video hardware using svgalib for Linux. The system also make use of the Freetype TrueType font library to use TrueType fonts for text rendering. This eliminates one incompatibility between the Linux and MS Windows versions of Curl."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62393",
    "title": "Complete VLSI implementation of improved low complexity chase Reed-Solomon decoders",
    "abstract": "This thesis presents a complete VLSI design of improved low complexity chase (LCC) decoders for Reed-Solomon (RS) codes. This is the first attempt in published research that implements LCC decoders at the circuit level. Based on the joint algorithm research with University of Hawaii, we propose several new techniques for complexity reduction in LCC decoders and apply them in the VLSI design for RS [255, 239,17] (LCC255) and RS [31, 25, 7] (LCC31) codes. The major algorithm improvement is that the interpolation is performed over a subset of test vectors to avoid redundant decoding. Also the factorization formula is reshaped to avoid large computation complexity overlooked in previous research. To maintain the effectiveness of algorithm improvements, we find it necessary to adopt the systematic message encoding, instead of the evaluation-map encoding used in the previous work on interpolation decoders. The LCC255 and LCC31 decoders are both implemented in 90nm CMOS process with the areas of 1.01mm 2 and 0.255mm 2 respectively. Simulations show that with 1.2V supply voltage they can achieve the energy efficiencies of 67pJ/bit and 34pJ/bit at the maximum throughputs of 2.5Gbps and 1.3Gbps respectively. The proposed algorithm changes, combined with optimized macro- and micro-architectures, result in a 70% complexity reduction (measured with gate count). This new LCC design also achieves 17x better energy-efficiency than a standard Chase decoder (projected from the most recent reported Reed Solomon decoder implementation) for equivalent area, latency and throughput. The comparison of the two decoders links the significantly higher decoding energy cost to the better decoding performance. We quantitatively compute the cost of the decoding gain as the adjusted area of LCC255 being 7.5 times more than LCC31.",
    "advisors": ["Vladimir M. Stojanović"],
    "text": "Complete VLSI implementation of improved low complexity chase Reed-Solomon decoders This thesis presents a complete VLSI design of improved low complexity chase (LCC) decoders for Reed-Solomon (RS) codes. This is the first attempt in published research that implements LCC decoders at the circuit level. Based on the joint algorithm research with University of Hawaii, we propose several new techniques for complexity reduction in LCC decoders and apply them in the VLSI design for RS [255, 239,17] (LCC255) and RS [31, 25, 7] (LCC31) codes. The major algorithm improvement is that the interpolation is performed over a subset of test vectors to avoid redundant decoding. Also the factorization formula is reshaped to avoid large computation complexity overlooked in previous research. To maintain the effectiveness of algorithm improvements, we find it necessary to adopt the systematic message encoding, instead of the evaluation-map encoding used in the previous work on interpolation decoders. The LCC255 and LCC31 decoders are both implemented in 90nm CMOS process with the areas of 1.01mm 2 and 0.255mm 2 respectively. Simulations show that with 1.2V supply voltage they can achieve the energy efficiencies of 67pJ/bit and 34pJ/bit at the maximum throughputs of 2.5Gbps and 1.3Gbps respectively. The proposed algorithm changes, combined with optimized macro- and micro-architectures, result in a 70% complexity reduction (measured with gate count). This new LCC design also achieves 17x better energy-efficiency than a standard Chase decoder (projected from the most recent reported Reed Solomon decoder implementation) for equivalent area, latency and throughput. The comparison of the two decoders links the significantly higher decoding energy cost to the better decoding performance. We quantitatively compute the cost of the decoding gain as the adjusted area of LCC255 being 7.5 times more than LCC31."
}, {
    "id": "oai:dspace.mit.edu:1721.1/71275",
    "title": "Structured decomposition of adaptive applications",
    "abstract": "We describe an approach to automate certain high-level implementation decisions in a pervasive application, allowing them to be postponed until run time. Our system enables a model in which an application programmer can specify the behavior of an adaptive application as a set of open-ended decision points. We formalize decision points as Goals, each of which may be satisfied by a set of scripts called Techniques. The set of Techniques vying to satisfy any Goal is additive and may be extended at runtime without needing to modify or remove any existing Techniques. Our system provides a framework in which Techniques may compete and interoperate at runtime in order to maintain an adaptive application. Technique development may be distributed and incremental, providing a path for the decentralized evolution of applications. Benchmarks show that our system imposes reasonable overhead during application startup and adaptation.",
    "advisors": ["Steve Ward"],
    "text": "Structured decomposition of adaptive applications We describe an approach to automate certain high-level implementation decisions in a pervasive application, allowing them to be postponed until run time. Our system enables a model in which an application programmer can specify the behavior of an adaptive application as a set of open-ended decision points. We formalize decision points as Goals, each of which may be satisfied by a set of scripts called Techniques. The set of Techniques vying to satisfy any Goal is additive and may be extended at runtime without needing to modify or remove any existing Techniques. Our system provides a framework in which Techniques may compete and interoperate at runtime in order to maintain an adaptive application. Technique development may be distributed and incremental, providing a path for the decentralized evolution of applications. Benchmarks show that our system imposes reasonable overhead during application startup and adaptation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/38692",
    "title": "Development of the recess mounting with monolithic metallization optoelectronic integrated circuit technology for optical clock distribution applications",
    "abstract": "Recess mounting with monolithic metallization, or RM3 integration, is used to integrate Ino.47Ga0.53As/InP based lattice-matched high quantum efficiency p-i-n photodetectors on silicon chips to build high performance optoelectronic integrated circuits [1]. In RM3 integration, partially processed heterostructure devices are placed in recesses formed in the dielectric layers covering the surface of an integrated circuit chip, the surface is planarized, and monolithic processing is continued to transform the heterostructures into optoelectronic devices monolithically integrated with the underlying electronic circuitry. Two different RM3 techniques have been investigated, Aligned Pillar Bonding (APB) and OptoPill Assembly (OPA). APB integrates lattice mismatched materials using aligned, selective area wafer bonding at reduced temperature (under 3500C), which protects the electronic chips from the adverse effects of high temperatures, and reduces the thermal expansion mismatch concerns. In the OPA technique, optoelectronic heterostructures are processed into circular pills of 8 gm height and 45 gm diameter, the pills are released from the substrate, and collected through a process that involves decanting.",
    "advisors": ["Clifton G. Fonstad"],
    "text": "Development of the recess mounting with monolithic metallization optoelectronic integrated circuit technology for optical clock distribution applications Recess mounting with monolithic metallization, or RM3 integration, is used to integrate Ino.47Ga0.53As/InP based lattice-matched high quantum efficiency p-i-n photodetectors on silicon chips to build high performance optoelectronic integrated circuits [1]. In RM3 integration, partially processed heterostructure devices are placed in recesses formed in the dielectric layers covering the surface of an integrated circuit chip, the surface is planarized, and monolithic processing is continued to transform the heterostructures into optoelectronic devices monolithically integrated with the underlying electronic circuitry. Two different RM3 techniques have been investigated, Aligned Pillar Bonding (APB) and OptoPill Assembly (OPA). APB integrates lattice mismatched materials using aligned, selective area wafer bonding at reduced temperature (under 3500C), which protects the electronic chips from the adverse effects of high temperatures, and reduces the thermal expansion mismatch concerns. In the OPA technique, optoelectronic heterostructures are processed into circular pills of 8 gm height and 45 gm diameter, the pills are released from the substrate, and collected through a process that involves decanting."
}, {
    "id": "oai:dspace.mit.edu:1721.1/115729",
    "title": "Exploration vs. exploitation in coupon personalization",
    "abstract": "Personalized offers aim to maximize profit by taking into account customer preferences inferred from past purchase behavior. For large retailers with extensive product offerings, learning customer preferences can be challenging due to relatively short purchase histories of most customers. To alleviate the dearth of data, we propose exploiting similarities among products and among customers to reduce problem dimensions. We also propose that retailers use personalized offers not only to maximize expected profit, but to actively learn their customers' preferences. An offer that does not maximize expected profit given current information may still provide valuable insights about customer preferences. This information enables more profitable coupon allocation and higher profits in the long run. In this thesis we 1) derive approximate inference algorithms to learn customer preferences from purchase data in real time, 2) formulate the retailers' offer allocation problem as a multi armed bandit and explore solution strategies.",
    "advisors": ["Devavrat Shah"],
    "text": "Exploration vs. exploitation in coupon personalization Personalized offers aim to maximize profit by taking into account customer preferences inferred from past purchase behavior. For large retailers with extensive product offerings, learning customer preferences can be challenging due to relatively short purchase histories of most customers. To alleviate the dearth of data, we propose exploiting similarities among products and among customers to reduce problem dimensions. We also propose that retailers use personalized offers not only to maximize expected profit, but to actively learn their customers' preferences. An offer that does not maximize expected profit given current information may still provide valuable insights about customer preferences. This information enables more profitable coupon allocation and higher profits in the long run. In this thesis we 1) derive approximate inference algorithms to learn customer preferences from purchase data in real time, 2) formulate the retailers' offer allocation problem as a multi armed bandit and explore solution strategies."
}, {
    "id": "oai:dspace.mit.edu:1721.1/75454",
    "title": "Design and fabrication of a MEMS-array pressure sensor system for passive underwater navigation inspired by the lateral line",
    "abstract": "An object within a fluid flow generates local pressure variations that are unique and characteristic to the object's shape and size. For example, a three-dimensional object or a wall-like obstacle obstructs flow and creates sharp pressure gradients nearby. Similarly, unsteady flow contains vortical patterns with associated unique pressure signatures. Detection of obstacles, as well as identification of unsteady flow features, is required for autonomous undersea vehicle (AUV) navigation. An array of passive underwater pressure sensors, with their ability to An object within a fluid flow generates local pressure variations that are unique and characteristic to the object's shape and size. For example, a three-dimensional object or a wall-like obstacle obstructs flow and creates sharp pressure gradients nearby. Similarly, unsteady flow contains vortical patterns with associated unique pressure signatures. Detection of obstacles, as well as identification of unsteady flow features, is required for autonomous undersea vehicle (AUV) navigation. An array of passive underwater pressure sensors, with their ability to \"touch at a distance\" with minimal power consumption, would be able to resolve the pressure signatures of obstacles in the near field and the wake of objects in the intermediate field. As an additional benefit, with proper design, pressure sensors can also be used to sample acoustic signals as well. Fish already have a biological version of such a pressure sensor system, namely the lateral line organ, a spatially-distributed set of sensors over a fish's body that allows the fish to monitor its hydrodynamic environment, influenced by the external disturbances. Through its ability to resolve the pressure signature of objects, the fish obtains \"hydrodynamic pictures\". Inspired by the fish lateral line, this thesis describes the development of a high-density array of microelectromechanical systems (MEMS) pressure sensors built in KOH-etched silicon and HF-etched Pyrex wafers. A novel strain-gauge resistor design is discussed, and standard CMOS/MEMS fabrication techniques were used to build sensors based on the strain-gauge resistors and thin silicon diphragms. Measurements of the diaphragm deflection and strain-gauge resistance changes in response to changes in applied external pressure confirm that the devices can be reliably calibrated for use as pressure sensors to enable passive navigation by AUVs. A set of sensors with millimeter-scale spacing, 2.1 to 2.5 [mu]V/Pa sensitivity, sub-pascal pressure resolution, and -2000 Pa to 2000 Pa pressure range has been demonstrated. Finally, an integrated circuit for array processing and signal amplification and to be fabricated with the pressure sensors is proposed.",
    "advisors": ["Jeffrey H. Lang"],
    "text": "Design and fabrication of a MEMS-array pressure sensor system for passive underwater navigation inspired by the lateral line An object within a fluid flow generates local pressure variations that are unique and characteristic to the object's shape and size. For example, a three-dimensional object or a wall-like obstacle obstructs flow and creates sharp pressure gradients nearby. Similarly, unsteady flow contains vortical patterns with associated unique pressure signatures. Detection of obstacles, as well as identification of unsteady flow features, is required for autonomous undersea vehicle (AUV) navigation. An array of passive underwater pressure sensors, with their ability to An object within a fluid flow generates local pressure variations that are unique and characteristic to the object's shape and size. For example, a three-dimensional object or a wall-like obstacle obstructs flow and creates sharp pressure gradients nearby. Similarly, unsteady flow contains vortical patterns with associated unique pressure signatures. Detection of obstacles, as well as identification of unsteady flow features, is required for autonomous undersea vehicle (AUV) navigation. An array of passive underwater pressure sensors, with their ability to \"touch at a distance\" with minimal power consumption, would be able to resolve the pressure signatures of obstacles in the near field and the wake of objects in the intermediate field. As an additional benefit, with proper design, pressure sensors can also be used to sample acoustic signals as well. Fish already have a biological version of such a pressure sensor system, namely the lateral line organ, a spatially-distributed set of sensors over a fish's body that allows the fish to monitor its hydrodynamic environment, influenced by the external disturbances. Through its ability to resolve the pressure signature of objects, the fish obtains \"hydrodynamic pictures\". Inspired by the fish lateral line, this thesis describes the development of a high-density array of microelectromechanical systems (MEMS) pressure sensors built in KOH-etched silicon and HF-etched Pyrex wafers. A novel strain-gauge resistor design is discussed, and standard CMOS/MEMS fabrication techniques were used to build sensors based on the strain-gauge resistors and thin silicon diphragms. Measurements of the diaphragm deflection and strain-gauge resistance changes in response to changes in applied external pressure confirm that the devices can be reliably calibrated for use as pressure sensors to enable passive navigation by AUVs. A set of sensors with millimeter-scale spacing, 2.1 to 2.5 [mu]V/Pa sensitivity, sub-pascal pressure resolution, and -2000 Pa to 2000 Pa pressure range has been demonstrated. Finally, an integrated circuit for array processing and signal amplification and to be fabricated with the pressure sensors is proposed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/84718",
    "title": "Dynamic application of problem solving strategies : dependency-based flow control",
    "abstract": "While humans may solve problems by applying any one of a number of different problem solving strategies, computerized problem solving is typically brittle, limited in the number of available strategies and ways of combining them to solve a problem. In this thesis, I present a method to flexibly select and combine problem solving strategies by using a constraint-propagation network, informed by higher-order knowledge about goals and what is known, to selectively control the activity of underlying problem solvers. Knowledge within each problem solver as well as the constraint-propagation network are represented as a network of explicit propositions, each described with respect to five interrelated axes of concrete and abstract knowledge about each proposition. Knowledge within each axis is supported by a set of dependencies that allow for both the adjustment of belief based on modifying supports for solutions and the production of justifications of that belief. I show that this method may be used to solve a variety of real-world problems and provide meaningful justifications for solutions to these problems, including decision-making based on numerical evaluation of risk and the evaluation of whether or not a document may be legally sent to a recipient in accordance with a policy controlling its dissemination.",
    "advisors": ["Gerald Jay Sussman"],
    "text": "Dynamic application of problem solving strategies : dependency-based flow control While humans may solve problems by applying any one of a number of different problem solving strategies, computerized problem solving is typically brittle, limited in the number of available strategies and ways of combining them to solve a problem. In this thesis, I present a method to flexibly select and combine problem solving strategies by using a constraint-propagation network, informed by higher-order knowledge about goals and what is known, to selectively control the activity of underlying problem solvers. Knowledge within each problem solver as well as the constraint-propagation network are represented as a network of explicit propositions, each described with respect to five interrelated axes of concrete and abstract knowledge about each proposition. Knowledge within each axis is supported by a set of dependencies that allow for both the adjustment of belief based on modifying supports for solutions and the production of justifications of that belief. I show that this method may be used to solve a variety of real-world problems and provide meaningful justifications for solutions to these problems, including decision-making based on numerical evaluation of risk and the evaluation of whether or not a document may be legally sent to a recipient in accordance with a policy controlling its dissemination."
}, {
    "id": "oai:dspace.mit.edu:1721.1/93839",
    "title": "User-controlled privacy for personal mobile data",
    "abstract": "Smartphones collect a wide range of sensor data, ranging from the basic, such as location, accelerometer, and Bluetooth, to the more advanced, such as heart rate. Mobile apps on the Android and iOS platforms provide users with \"all-or-nothing\" controls during installation to get permission for data collection and use. Users have to either agree to have the app collect and use all the requested data or not use the app at all. This is slowly changing with the iOS framework, which now allows users to turn off location sharing with specific apps even after installation. MIT Living Lab platform is a mobile app development platform that uses openPDS to provide MIT users with personal data stores but currently lacks user controls for privacy. This thesis presents PrivacyMate, a suite of tools for MIT Living Labs that provide user-controllable privacy mechanisms for mobile apps. PrivacyMate aims to enable users to maintain better control over their mobile personal data. It extends the model of iOS and allows users to select or deselect various types of data (more than just location information) for collection and use by apps. Users can also provide temporal and spatial specifications to indicate a context in which they are comfortable sharing their data with certain apps. We incorporate the privacy mechanisms offered by PrivacyMate into two mobile apps built on the MIT Living Lab platform: ScheduleME and MIT-FIT. ScheduleME enables users to schedule meetings without disclosing either their locations or points of interest. MIT-FIT enables users to track personal and aggregate high-activity regions and times, as well as view personalized fitness-related event recommendations. The MIT Living Lab team is planning to eventually deploy PrivacyMate and MIT-FIT to the entire MIT community.",
    "advisors": ["Lalana Kagal"],
    "text": "User-controlled privacy for personal mobile data Smartphones collect a wide range of sensor data, ranging from the basic, such as location, accelerometer, and Bluetooth, to the more advanced, such as heart rate. Mobile apps on the Android and iOS platforms provide users with \"all-or-nothing\" controls during installation to get permission for data collection and use. Users have to either agree to have the app collect and use all the requested data or not use the app at all. This is slowly changing with the iOS framework, which now allows users to turn off location sharing with specific apps even after installation. MIT Living Lab platform is a mobile app development platform that uses openPDS to provide MIT users with personal data stores but currently lacks user controls for privacy. This thesis presents PrivacyMate, a suite of tools for MIT Living Labs that provide user-controllable privacy mechanisms for mobile apps. PrivacyMate aims to enable users to maintain better control over their mobile personal data. It extends the model of iOS and allows users to select or deselect various types of data (more than just location information) for collection and use by apps. Users can also provide temporal and spatial specifications to indicate a context in which they are comfortable sharing their data with certain apps. We incorporate the privacy mechanisms offered by PrivacyMate into two mobile apps built on the MIT Living Lab platform: ScheduleME and MIT-FIT. ScheduleME enables users to schedule meetings without disclosing either their locations or points of interest. MIT-FIT enables users to track personal and aggregate high-activity regions and times, as well as view personalized fitness-related event recommendations. The MIT Living Lab team is planning to eventually deploy PrivacyMate and MIT-FIT to the entire MIT community."
}, {
    "id": "oai:dspace.mit.edu:1721.1/71512",
    "title": "Defeating eavesdropping with quantum illumination",
    "abstract": "Quantum illumination is a paradigm for using entanglement to gain a performance advantage-in comparison with classical-state systems of the same optical power-over lossy, noisy channels that destroy entanglement. Previous work has shown how it can be used to defeat passive eavesdropping on a two-way Alice-to-Bob-to-Alice communication protocol, in which the eavesdropper, Eve, merely listens to Alice and Bob's transmissions. This thesis extends that work in several ways. First, it derives a lower bound on information advantage that Alice enjoys over Eve in the passive eavesdropping scenario. Next, it explores the performance of alternative practical receivers for Alice, as well as various high-order modulation formats for the passive eavesdropping case. Finally, this thesis extends previous analysis to consider how Alice and Bob can minimize their vulnerability to Eve's doing active eavesdropping, i.e., when she injects her own light into the channel.",
    "advisors": ["Jeffrey H. Shapiro"],
    "text": "Defeating eavesdropping with quantum illumination Quantum illumination is a paradigm for using entanglement to gain a performance advantage-in comparison with classical-state systems of the same optical power-over lossy, noisy channels that destroy entanglement. Previous work has shown how it can be used to defeat passive eavesdropping on a two-way Alice-to-Bob-to-Alice communication protocol, in which the eavesdropper, Eve, merely listens to Alice and Bob's transmissions. This thesis extends that work in several ways. First, it derives a lower bound on information advantage that Alice enjoys over Eve in the passive eavesdropping scenario. Next, it explores the performance of alternative practical receivers for Alice, as well as various high-order modulation formats for the passive eavesdropping case. Finally, this thesis extends previous analysis to consider how Alice and Bob can minimize their vulnerability to Eve's doing active eavesdropping, i.e., when she injects her own light into the channel."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69772",
    "title": "Efficient silicon micro-reactors for thermophotovoltaic applications",
    "abstract": "Thermophotovoltaic (TPV) systems passively generate electricity from the combustion of fuel. Although TPV conversion systems have advantages, they suffer from low efficiency. This thesis investigates different ways to increase the efficiency of TPV systems. In particular the thesis details micro-fabrication of silicon micro-reactors, and twodimensional tungsten photonic crystals (2D W PhC) for high-temperature applications such as selective thermal emitters for TPV energy conversion. Interference lithography and reactive ion etching are used to produce large-area single-crystal tungsten 2D PhC's. The fabricated PhC consists of an array of cylindrical cavities with 800nm diameter, 1.2 pm depth, and 1.2 pm period. Extensive characterization and calibration of all micro-fabrication steps for both micro-reactors and 2D PhC's are presented. Experimentally-obtained thermal emission spectra of the 2D PhC structures match well with numerical predictions.",
    "advisors": ["Leslie Kolodziejski"],
    "text": "Efficient silicon micro-reactors for thermophotovoltaic applications Thermophotovoltaic (TPV) systems passively generate electricity from the combustion of fuel. Although TPV conversion systems have advantages, they suffer from low efficiency. This thesis investigates different ways to increase the efficiency of TPV systems. In particular the thesis details micro-fabrication of silicon micro-reactors, and twodimensional tungsten photonic crystals (2D W PhC) for high-temperature applications such as selective thermal emitters for TPV energy conversion. Interference lithography and reactive ion etching are used to produce large-area single-crystal tungsten 2D PhC's. The fabricated PhC consists of an array of cylindrical cavities with 800nm diameter, 1.2 pm depth, and 1.2 pm period. Extensive characterization and calibration of all micro-fabrication steps for both micro-reactors and 2D PhC's are presented. Experimentally-obtained thermal emission spectra of the 2D PhC structures match well with numerical predictions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/118078",
    "title": "Alternating conditional expectation (ACE) applied to classification and recommendation problems",
    "abstract": "In this thesis, a geometric framework for describing relevant information in a collection of data is applied for the general problems of selecting informative features (dimension reduction) from high dimensional data. The framework can be used in an unsupervised manner, extracting universal features that can be used later for general classification of data. This framework is derived by applying local approximations on the space of probability distributions and a small perturbation approach. With this approach, different information theoretic results can be interpreted as linear algebra optimizations based on the norms of vectors in a linear space, which are in general, easier to carry out. Fundamentally, using known procedures such as Singular Value Decomposition (SVD) and Principal Component Analysis (PCA), dimension reduction for maximizing power can be achieved in a straight forward manner. Using the geometric framework, we relate calculation of SVD of a particular matrix related to a probabilistic channel to the application of Alternating Conditional Expectation (ACE) in the problem of optimal regression. The key takeaway of this method is that such problems can be studied in the space of distributions of the data and not the space of outcomes. This geometric framework allows to give an operational meaning to information metrics in the context of data analysis and feature selection. Additionally, it provides a method to obtain universal classification functions without knowledge of the important feature of the problem. This framework is the applied to the problem of data classification and analysis with satisfactory results.",
    "advisors": ["Lizhong Zheng"],
    "text": "Alternating conditional expectation (ACE) applied to classification and recommendation problems In this thesis, a geometric framework for describing relevant information in a collection of data is applied for the general problems of selecting informative features (dimension reduction) from high dimensional data. The framework can be used in an unsupervised manner, extracting universal features that can be used later for general classification of data. This framework is derived by applying local approximations on the space of probability distributions and a small perturbation approach. With this approach, different information theoretic results can be interpreted as linear algebra optimizations based on the norms of vectors in a linear space, which are in general, easier to carry out. Fundamentally, using known procedures such as Singular Value Decomposition (SVD) and Principal Component Analysis (PCA), dimension reduction for maximizing power can be achieved in a straight forward manner. Using the geometric framework, we relate calculation of SVD of a particular matrix related to a probabilistic channel to the application of Alternating Conditional Expectation (ACE) in the problem of optimal regression. The key takeaway of this method is that such problems can be studied in the space of distributions of the data and not the space of outcomes. This geometric framework allows to give an operational meaning to information metrics in the context of data analysis and feature selection. Additionally, it provides a method to obtain universal classification functions without knowledge of the important feature of the problem. This framework is the applied to the problem of data classification and analysis with satisfactory results."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117814",
    "title": "Systemic risk in the interbank lending market",
    "abstract": "Our goal is to understand the functioning of the interbank lending market in times of market stress. Working towards this goal, we conduct theoretical analysis and simulation to study the effects of network structure and shock scenarios on systemic risk in the market. We consider shocks of various sizes at both global and local scales. In terms of risk measures, we study relative systemic loss and the default rate, separating the latter quantity into fundamental default and contagion. Our simulations suggest that all systemic risk measures are similar on the well-studied directed Erdős-Rényi model and the more complex fitness model if we match the mean density and the mean edge weight of these two models. We show through both derivations and simulations that the network size has little effect on systemic risk when the network is sufficiently large. Moreover, as the mean degree grows, the different default rates considered all increase, while relative systemic loss decreases. Furthermore, simulations suggest that local shocks tend to cause more harm than global shocks of the same total size. We also derive upper and lower bounds on a bank's probability of default, only using its neighbors' information. For implementation, we build a method for real-time, automatic, interpretable assessment of financial systemic risk, which only requires temporal snapshots of observable data. Our algorithm takes in partial data, inferring a random graph model, and then generates empirical distributions for risk measures. The first part relies on inferring a fitness model that is compatible with observed information. For the second part, we use simulations to obtain empirical distributions for systemic risk that arises from interbank clearing. We test our method on synthetic data and apply it to the federal funds market using empirical data. Our method is fast enough to be incorporated into algorithms that produce intraday time trajectories of risk prediction. The data requirement is practical for investors as well as regulators, policy-makers, and financial institutions.",
    "advisors": ["John N. Tsitsiklis", "Munther A. Dahleh"],
    "text": "Systemic risk in the interbank lending market Our goal is to understand the functioning of the interbank lending market in times of market stress. Working towards this goal, we conduct theoretical analysis and simulation to study the effects of network structure and shock scenarios on systemic risk in the market. We consider shocks of various sizes at both global and local scales. In terms of risk measures, we study relative systemic loss and the default rate, separating the latter quantity into fundamental default and contagion. Our simulations suggest that all systemic risk measures are similar on the well-studied directed Erdős-Rényi model and the more complex fitness model if we match the mean density and the mean edge weight of these two models. We show through both derivations and simulations that the network size has little effect on systemic risk when the network is sufficiently large. Moreover, as the mean degree grows, the different default rates considered all increase, while relative systemic loss decreases. Furthermore, simulations suggest that local shocks tend to cause more harm than global shocks of the same total size. We also derive upper and lower bounds on a bank's probability of default, only using its neighbors' information. For implementation, we build a method for real-time, automatic, interpretable assessment of financial systemic risk, which only requires temporal snapshots of observable data. Our algorithm takes in partial data, inferring a random graph model, and then generates empirical distributions for risk measures. The first part relies on inferring a fitness model that is compatible with observed information. For the second part, we use simulations to obtain empirical distributions for systemic risk that arises from interbank clearing. We test our method on synthetic data and apply it to the federal funds market using empirical data. Our method is fast enough to be incorporated into algorithms that produce intraday time trajectories of risk prediction. The data requirement is practical for investors as well as regulators, policy-makers, and financial institutions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/38556",
    "title": "Optical frequency domain imaging of human retina and choroid",
    "abstract": "Optical coherence tomography (OCT) has emerged as a practical noninvasive technology for imaging the microstructure of the human eye in vivo. Using optical interferometry to spatially-resolve backreflections from within tissue, this high-resolution technique provides cross-sectional images of the anterior and posterior eye segments that had previously only been possible with histology. Current commercially-available OCT systems suffer limitations in speed and sensitivity, preventing them from effective screening of the retina and having a larger impact on the clinical environment. While other technological advances have addressed this problem, they are inadequate for imaging the choroid, which can be useful for evaluating choroidal disorders as well as early stages of retinal diseases. The objective of this thesis was to develop a new ophthalmic imaging method, termed optical frequency domain imaging (OFDI), to overcome these limitations. Preliminary imaging of the posterior segment of human eyes in vivo was performed to evaluate the utility of this instrument for comprehensive ophthalmic examination.",
    "advisors": ["Seok-Hyun Yun", "Brett E. Bouma"],
    "text": "Optical frequency domain imaging of human retina and choroid Optical coherence tomography (OCT) has emerged as a practical noninvasive technology for imaging the microstructure of the human eye in vivo. Using optical interferometry to spatially-resolve backreflections from within tissue, this high-resolution technique provides cross-sectional images of the anterior and posterior eye segments that had previously only been possible with histology. Current commercially-available OCT systems suffer limitations in speed and sensitivity, preventing them from effective screening of the retina and having a larger impact on the clinical environment. While other technological advances have addressed this problem, they are inadequate for imaging the choroid, which can be useful for evaluating choroidal disorders as well as early stages of retinal diseases. The objective of this thesis was to develop a new ophthalmic imaging method, termed optical frequency domain imaging (OFDI), to overcome these limitations. Preliminary imaging of the posterior segment of human eyes in vivo was performed to evaluate the utility of this instrument for comprehensive ophthalmic examination."
}, {
    "id": "oai:dspace.mit.edu:1721.1/53145",
    "title": "File system unification using LatticeFS",
    "abstract": "LatticeFS is a namespace unification system designed to merge multiple source file systems into a single working file system. atticeFS can be used to merge multiple software package directories, work with multiple file systems as if they are one, and share a single storage medium among multiple machines. On a high level, LatticeFS takes as input an arbitrary number of file system paths, and mounts a new virtual drive that will appear to the user as a union of the input file systems. Of course, attempting to combine multiple file systems will inevitably be met with conflicts. Situations in which multiple input file systems contain files/directories with the same name will be common in large systems; which file/directory should the user be exposed to in this case? Previous work such as UnionFS solved the problem by giving each input file system a strict priority value, and when a conflict occurred, the file/directory with the highest priority was the one shown to the user. In LatticeFS, we have introduced a plug-in system in which different strategies for resolving conflicts can be easily swapped in and out; additionally, handlers for special file types can also be \"plugged\" into the system. This paper describes and evaluates all aspects of LatticeFS in detail.",
    "advisors": ["Stephen A. Ward"],
    "text": "File system unification using LatticeFS LatticeFS is a namespace unification system designed to merge multiple source file systems into a single working file system. atticeFS can be used to merge multiple software package directories, work with multiple file systems as if they are one, and share a single storage medium among multiple machines. On a high level, LatticeFS takes as input an arbitrary number of file system paths, and mounts a new virtual drive that will appear to the user as a union of the input file systems. Of course, attempting to combine multiple file systems will inevitably be met with conflicts. Situations in which multiple input file systems contain files/directories with the same name will be common in large systems; which file/directory should the user be exposed to in this case? Previous work such as UnionFS solved the problem by giving each input file system a strict priority value, and when a conflict occurred, the file/directory with the highest priority was the one shown to the user. In LatticeFS, we have introduced a plug-in system in which different strategies for resolving conflicts can be easily swapped in and out; additionally, handlers for special file types can also be \"plugged\" into the system. This paper describes and evaluates all aspects of LatticeFS in detail."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34360",
    "title": "An implementation of a 5.25 GHz transceiver for high data rate wireless applications",
    "abstract": "The desire for transmission of high data rate information across wireless channels has grown immensely over the past decade. Wireless devices available today including mobile phones, wireless local area networks (WLANs) and Bluetooth radios have realized a wide variety of applications at data rates ranging from 10s of kbit/s to 10s of Mbit/s. Mobile telephone design strives for large transmit distances, Bluetooth technology enables communication between two close range devices, and wireless LAN strives to achieve a high data rate wireless link within an office or home environment. This link is traditionally implemented through a central access point that communicates with one or more workstations. Due to the large number of applications demanding high speed wireless links, the aspiration for even higher data rates is prevalent.",
    "advisors": ["Charles G. Sodini"],
    "text": "An implementation of a 5.25 GHz transceiver for high data rate wireless applications The desire for transmission of high data rate information across wireless channels has grown immensely over the past decade. Wireless devices available today including mobile phones, wireless local area networks (WLANs) and Bluetooth radios have realized a wide variety of applications at data rates ranging from 10s of kbit/s to 10s of Mbit/s. Mobile telephone design strives for large transmit distances, Bluetooth technology enables communication between two close range devices, and wireless LAN strives to achieve a high data rate wireless link within an office or home environment. This link is traditionally implemented through a central access point that communicates with one or more workstations. Due to the large number of applications demanding high speed wireless links, the aspiration for even higher data rates is prevalent."
}, {
    "id": "oai:dspace.mit.edu:1721.1/36782",
    "title": "Dielectric Resonator Antennas : theory and design",
    "abstract": "Theoretical models for the analysis of Dielectric Resonator Antenna (DRA) are developed. There are no exact solutions to many of the problems in analytical form, therefore a strong focus on the physical interpretation of the numerical results is presented alongside theoretical models. I have used the physical interpretation of the numerical results to lay down some important design rules. A few new inventions associated with the DRA are also included. These are the elliptical DRA, the DRA with a rectangular slot, the adjustable reactance feed, the triangular DRA and the dual band DRA-patch antenna.",
    "advisors": ["Ali Tassoudji", "Jin Au Kong"],
    "text": "Dielectric Resonator Antennas : theory and design Theoretical models for the analysis of Dielectric Resonator Antenna (DRA) are developed. There are no exact solutions to many of the problems in analytical form, therefore a strong focus on the physical interpretation of the numerical results is presented alongside theoretical models. I have used the physical interpretation of the numerical results to lay down some important design rules. A few new inventions associated with the DRA are also included. These are the elliptical DRA, the DRA with a rectangular slot, the adjustable reactance feed, the triangular DRA and the dual band DRA-patch antenna."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101580",
    "title": "Optimization of electron optics in a resonator cavity using Nelder-Mead simplex search for the quantum electron microscope",
    "abstract": "The Quantum Electron Microscope (QEM) is a proposed imaging modality that aims to reduce or eliminate the effects of radiation on living cells compared to traditional electron microscopy techniques. In recent years, an interaction free measurement scheme was proposed by Putnam and Yanik [1], and an implementation of this idea is being developed by an international collaboration. The current implementation foresees an electron cavity, which can be installed into a regular scanning electron microscope, to allow multiple passes of two electron wavefunctions over the specimen. In order to implement this idea, multiple different electron optical designs were proposed. Extensive simulation work is required to test and validate these designs. This work outlines the simulation work done for QEM, and proposes a general framework for optimizing electron trajectory simulations using Nelder-Mead search. It also provides a library of MATLAB wrapper functions and optimization methods to be used with the Integrated Lorentz-2E software.",
    "advisors": ["Mehmet Fatih Yanik"],
    "text": "Optimization of electron optics in a resonator cavity using Nelder-Mead simplex search for the quantum electron microscope The Quantum Electron Microscope (QEM) is a proposed imaging modality that aims to reduce or eliminate the effects of radiation on living cells compared to traditional electron microscopy techniques. In recent years, an interaction free measurement scheme was proposed by Putnam and Yanik [1], and an implementation of this idea is being developed by an international collaboration. The current implementation foresees an electron cavity, which can be installed into a regular scanning electron microscope, to allow multiple passes of two electron wavefunctions over the specimen. In order to implement this idea, multiple different electron optical designs were proposed. Extensive simulation work is required to test and validate these designs. This work outlines the simulation work done for QEM, and proposes a general framework for optimizing electron trajectory simulations using Nelder-Mead search. It also provides a library of MATLAB wrapper functions and optimization methods to be used with the Integrated Lorentz-2E software."
}, {
    "id": "oai:dspace.mit.edu:1721.1/29695",
    "title": "Biologically-plausible six-legged running : control and simulation",
    "abstract": "This thesis presents a controller which produces a stable, dynamic 1.4 meter per second run in a simulated twelve degree of freedom six-legged robot. The algorithm is relatively simple; it consists of only a few hand-tuned feedback loops and is defined by a total of 13 parameters. The control utilizes no vestibular-type inputs to actively control orientation. Evidence from perturbation, robustness, motion analysis, and parameter sensitivity tests indicate a high degree of stability in the simulated gait. The control approach generates a run with an aerial phase, utilizes force information to signal aerial phase leg retraction, has a forward running velocity determined by a single parameter, and couples stance and swing legs using angular momentum information. Both the hypotheses behind the control and the resulting gait are argued to be plausible models of biological locomotion.",
    "advisors": ["Hugh M. Herr"],
    "text": "Biologically-plausible six-legged running : control and simulation This thesis presents a controller which produces a stable, dynamic 1.4 meter per second run in a simulated twelve degree of freedom six-legged robot. The algorithm is relatively simple; it consists of only a few hand-tuned feedback loops and is defined by a total of 13 parameters. The control utilizes no vestibular-type inputs to actively control orientation. Evidence from perturbation, robustness, motion analysis, and parameter sensitivity tests indicate a high degree of stability in the simulated gait. The control approach generates a run with an aerial phase, utilizes force information to signal aerial phase leg retraction, has a forward running velocity determined by a single parameter, and couples stance and swing legs using angular momentum information. Both the hypotheses behind the control and the resulting gait are argued to be plausible models of biological locomotion."
}, {
    "id": "oai:dspace.mit.edu:1721.1/30099",
    "title": "Double-gated field emission arrays",
    "abstract": "There is a need for massively parallel, individually addressed and focused electron sources for applications such as flat panel displays, mass storage and multi-beam electron beam lithography. This project fabricates and characterizes double-gated field emission devices with high aspect ratio. One of the gates extracts the electrons while the second gate focuses the electrons into small spots. High aspect ratio silicon field emitters were defined by reactive ion etching of silicon followed by multiple depositions of polycrystalline oxide insulators and silicon gates. The layers were defined by a combination of lithography, chemical mechanical polishing and micromachining. We obtained devices with gate and focus apertures of 0.4[mu]m and 1.2[mu]m diameter. The anode current has very little dependence on the focus voltage and the ratio of the focus field factor to the gate field factor βF / βG is 0.015. Scanning electron micrographs of the devices, numerical simulation and spot size measurements on a phosphor screen confirmed these results. An e-beam resist, PMMA, was successfully exposed using the FEA device as an electron source.",
    "advisors": ["Akintunde Ibitayo (Tayo) Akinwande"],
    "text": "Double-gated field emission arrays There is a need for massively parallel, individually addressed and focused electron sources for applications such as flat panel displays, mass storage and multi-beam electron beam lithography. This project fabricates and characterizes double-gated field emission devices with high aspect ratio. One of the gates extracts the electrons while the second gate focuses the electrons into small spots. High aspect ratio silicon field emitters were defined by reactive ion etching of silicon followed by multiple depositions of polycrystalline oxide insulators and silicon gates. The layers were defined by a combination of lithography, chemical mechanical polishing and micromachining. We obtained devices with gate and focus apertures of 0.4[mu]m and 1.2[mu]m diameter. The anode current has very little dependence on the focus voltage and the ratio of the focus field factor to the gate field factor βF / βG is 0.015. Scanning electron micrographs of the devices, numerical simulation and spot size measurements on a phosphor screen confirmed these results. An e-beam resist, PMMA, was successfully exposed using the FEA device as an electron source."
}, {
    "id": "oai:dspace.mit.edu:1721.1/17966",
    "title": "Delivering real-time holographic video content with off-the-shelf PC hardware",
    "abstract": "We present a PC based system to simultaneously compute real-time holographic video content and to serve as a framebuffer to drive a holographic video display. Our system uses only 3 PCs each equipped with an nVidia Quadro FX 3000G video card. It replaces a SGI Onyx and the custom built Cheops Image Processing System that previously served as the platform driving the MIT second-generation Holovideo display. With a prototype content generation implementation, we compute holographic stereograms and update the display at a rate of roughly 2 frames per second.",
    "advisors": ["V. Michael Bove"],
    "text": "Delivering real-time holographic video content with off-the-shelf PC hardware We present a PC based system to simultaneously compute real-time holographic video content and to serve as a framebuffer to drive a holographic video display. Our system uses only 3 PCs each equipped with an nVidia Quadro FX 3000G video card. It replaces a SGI Onyx and the custom built Cheops Image Processing System that previously served as the platform driving the MIT second-generation Holovideo display. With a prototype content generation implementation, we compute holographic stereograms and update the display at a rate of roughly 2 frames per second."
}, {
    "id": "oai:dspace.mit.edu:1721.1/108959",
    "title": "Optimal sizing of solar and battery assets in decentralized micro-grids with demand-side management",
    "abstract": "Solar-based community micro-grids and individual home systems have been recognized as key enablers of electricity provision to the over one billion people living without energy access to-date. Despite significant cost reductions in solar panels, these options can still be cost-prohibitive mainly due over-sizing of generation assets corresponding with a lack of ability to actively manage electricity demand. The main contribution shared is the methodology and optimization approach of least-cost combinations of generation asset sizes, in solar panels and batteries, subject to meeting reliability constraints; these results are based on a techno-economic modeling approach constructed for assessing decentralized micro-grids with demand-side management capabilities. The software model constructed is implemented to represent the technical characteristics of a low-voltage, direct current network architecture and computational capabilities of a power management device. The main use-case of the model presented is based on serving representative, aggregated, household-level load profiles combined with simulated power output from solar photovoltaic modules and the kinetic operating constraints of lead-acid batteries at hourly timesteps over year-long simulations. The state-space for solutions is based on available solar module and battery capacities from distributors in Jharkhand, India. Additional work presented also extends to real-time operation of such isolated micro-grids with requisite local computation. First, for load disaggregation and forecasting purposes, clustering algorithms and statistical learning techniques are applied on quantitative results from inferred load profiles based on data logged from off-grid solar home systems. Second, results from an optimization approach to accurately parametrize a lead-acid battery model for potential usage in real-time field implementation are also shared. Economic results, sensitivity analyses around key technical and financial input assumptions, and comparisons in cost reductions due to the optimization of solar and battery assets for decentralized micro-grids with demand-side management capabilities are subsequently presented. The work concludes with insights and policy implications on establishing differentiated willingness-to-pay, tiers of service, and dynamic price-setting in advanced micro-grids.",
    "advisors": ["Rajeev Ram", "Munther Dahleh"],
    "text": "Optimal sizing of solar and battery assets in decentralized micro-grids with demand-side management Solar-based community micro-grids and individual home systems have been recognized as key enablers of electricity provision to the over one billion people living without energy access to-date. Despite significant cost reductions in solar panels, these options can still be cost-prohibitive mainly due over-sizing of generation assets corresponding with a lack of ability to actively manage electricity demand. The main contribution shared is the methodology and optimization approach of least-cost combinations of generation asset sizes, in solar panels and batteries, subject to meeting reliability constraints; these results are based on a techno-economic modeling approach constructed for assessing decentralized micro-grids with demand-side management capabilities. The software model constructed is implemented to represent the technical characteristics of a low-voltage, direct current network architecture and computational capabilities of a power management device. The main use-case of the model presented is based on serving representative, aggregated, household-level load profiles combined with simulated power output from solar photovoltaic modules and the kinetic operating constraints of lead-acid batteries at hourly timesteps over year-long simulations. The state-space for solutions is based on available solar module and battery capacities from distributors in Jharkhand, India. Additional work presented also extends to real-time operation of such isolated micro-grids with requisite local computation. First, for load disaggregation and forecasting purposes, clustering algorithms and statistical learning techniques are applied on quantitative results from inferred load profiles based on data logged from off-grid solar home systems. Second, results from an optimization approach to accurately parametrize a lead-acid battery model for potential usage in real-time field implementation are also shared. Economic results, sensitivity analyses around key technical and financial input assumptions, and comparisons in cost reductions due to the optimization of solar and battery assets for decentralized micro-grids with demand-side management capabilities are subsequently presented. The work concludes with insights and policy implications on establishing differentiated willingness-to-pay, tiers of service, and dynamic price-setting in advanced micro-grids."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112821",
    "title": "Tenant-level network performance isolation in Flowtune",
    "abstract": "Performance isolation is a major concern for multi-tenant datacenters. Many service level agreements include a specification on the allotment of resources. For tenants, these resource guarantees are critical to the availability and efficiency of their services. While CPU, disk, and memory isolation are well-understood, network performance isolation is less straightforward. In this thesis, I investigate methods for enforcing bandwidth fairness guarantees for logical networks in a datacenter and implement network performance isolation in Flowtune. Flowtune is a datacenter network architecture which introduces a centralized arbiter to enforce congestion control at the flowlet level. Flowtune achieves rapid convergence to a desired allocation of network resources in addition to reducing tail latencies in various settings. However, Flowtune currently does not provide tenant-level network performance isolation.",
    "advisors": ["Jonathan Perry", "Hari Balakrishnan"],
    "text": "Tenant-level network performance isolation in Flowtune Performance isolation is a major concern for multi-tenant datacenters. Many service level agreements include a specification on the allotment of resources. For tenants, these resource guarantees are critical to the availability and efficiency of their services. While CPU, disk, and memory isolation are well-understood, network performance isolation is less straightforward. In this thesis, I investigate methods for enforcing bandwidth fairness guarantees for logical networks in a datacenter and implement network performance isolation in Flowtune. Flowtune is a datacenter network architecture which introduces a centralized arbiter to enforce congestion control at the flowlet level. Flowtune achieves rapid convergence to a desired allocation of network resources in addition to reducing tail latencies in various settings. However, Flowtune currently does not provide tenant-level network performance isolation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37066",
    "title": "Algorithms for simulating human pre-mRNA splicing decisions",
    "abstract": "In this thesis, I developed a program, ExonScan, to simulate constitutive human pre-mRNA splicing. ExonScan includes several models for splicing components, including splice sites, exonic splicing enhancers, exonic splicing silencers, and intronic splicing enhancers. I used ExonScan to test various aspects of human splicing, including correlation of splicing signal strength with tissue expression levels, the effectiveness of experimentally determined exonic splicing silencers, and splice site identification.",
    "advisors": ["Christopher B. Burge"],
    "text": "Algorithms for simulating human pre-mRNA splicing decisions In this thesis, I developed a program, ExonScan, to simulate constitutive human pre-mRNA splicing. ExonScan includes several models for splicing components, including splice sites, exonic splicing enhancers, exonic splicing silencers, and intronic splicing enhancers. I used ExonScan to test various aspects of human splicing, including correlation of splicing signal strength with tissue expression levels, the effectiveness of experimentally determined exonic splicing silencers, and splice site identification."
}, {
    "id": "oai:dspace.mit.edu:1721.1/85805",
    "title": "A stacked full-bridge microinverter topology for photovoltaic applications",
    "abstract": "Previous work has been done to develop a microinverter for solar photovoltaic applications consisting of a high-frequency series resonant inverter and transformer section connected to a a cycloconverter that modulates the resonant current into a single-phase 240 VRMS utility line. This thesis presents a new stacked full-bridge topology that improves upon the previous high-frequency inverter section. By utilizing new operating modes to reduce the reliance on frequency control and allowing for the use of lower blocking voltage transistors, the operating frequency range of the HF inverter is reduced and efficiency is increased, especially at low output powers and lower portions of the line cycle. The design of an experimental prototype to test the stacked full-bridge HF inverter topology is presented along with test results that demonstrate the success of the topology. Future improvements to increase performance are also suggested.",
    "advisors": ["David J. Perreault"],
    "text": "A stacked full-bridge microinverter topology for photovoltaic applications Previous work has been done to develop a microinverter for solar photovoltaic applications consisting of a high-frequency series resonant inverter and transformer section connected to a a cycloconverter that modulates the resonant current into a single-phase 240 VRMS utility line. This thesis presents a new stacked full-bridge topology that improves upon the previous high-frequency inverter section. By utilizing new operating modes to reduce the reliance on frequency control and allowing for the use of lower blocking voltage transistors, the operating frequency range of the HF inverter is reduced and efficiency is increased, especially at low output powers and lower portions of the line cycle. The design of an experimental prototype to test the stacked full-bridge HF inverter topology is presented along with test results that demonstrate the success of the topology. Future improvements to increase performance are also suggested."
}, {
    "id": "oai:dspace.mit.edu:1721.1/85510",
    "title": "Teaching computer science principles using StarLogoTNG",
    "abstract": "This thesis outlines the development of a 3-module set of lesson plans implemented using StarLogoTNG. The purpose of these lesson plans are to serve as a vehicle for teaching and reinforcing specific learning objectives of the CollegeBoard's Advanced Placement Computer Science Principles course, which has 7 main themes. Each lesson plan has as its focus a subset of learning objectives from one of the themes of Creativity, Data, or Internet, while simultaneously incorporating additional learning goals from the themes of Abstraction, Programming, Algorithms, and Impact. These interactive lesson plans go beyond the use of StarLogoTNG to complete specific tasks by integrating meaningful class discussions and occasional peer instruction and peer review activities. Such activities become catalysts for students to develop a deeper understanding of the course materials. By connecting learning goals from different themes of the course and packaging them in cohesive lesson plans that utilize methods of teaching for understanding, this thesis aims to provide a useful and effective set of a materials for the instruction of computer science principles.",
    "advisors": ["Eric Klopfer"],
    "text": "Teaching computer science principles using StarLogoTNG This thesis outlines the development of a 3-module set of lesson plans implemented using StarLogoTNG. The purpose of these lesson plans are to serve as a vehicle for teaching and reinforcing specific learning objectives of the CollegeBoard's Advanced Placement Computer Science Principles course, which has 7 main themes. Each lesson plan has as its focus a subset of learning objectives from one of the themes of Creativity, Data, or Internet, while simultaneously incorporating additional learning goals from the themes of Abstraction, Programming, Algorithms, and Impact. These interactive lesson plans go beyond the use of StarLogoTNG to complete specific tasks by integrating meaningful class discussions and occasional peer instruction and peer review activities. Such activities become catalysts for students to develop a deeper understanding of the course materials. By connecting learning goals from different themes of the course and packaging them in cohesive lesson plans that utilize methods of teaching for understanding, this thesis aims to provide a useful and effective set of a materials for the instruction of computer science principles."
}, {
    "id": "oai:dspace.mit.edu:1721.1/77016",
    "title": "Frequency domain model-based intracranial pressure estimation",
    "abstract": "Elevation of intracranial pressure (ICP), the pressure of the fluid surrounding the brain, can require urgent medical attention. Current methods for determining ICP are invasive, require neurosurgical expertise, and can lead to infection. ICP measurement is therefore limited to the sickest patients, though many others could potentially benefit from availability of this vital sign. We present a frequency-domain approach to ICP estimation using a simple lumped, linear time-invariant model of cerebrovascular dynamics. Preliminary results from 28 records of patients with severe traumatic brain injury are presented and discussed. Suggestions for future work to improve the estimation algorithm are proposed.",
    "advisors": ["George C. Verghese", "Faisal M. Kashif"],
    "text": "Frequency domain model-based intracranial pressure estimation Elevation of intracranial pressure (ICP), the pressure of the fluid surrounding the brain, can require urgent medical attention. Current methods for determining ICP are invasive, require neurosurgical expertise, and can lead to infection. ICP measurement is therefore limited to the sickest patients, though many others could potentially benefit from availability of this vital sign. We present a frequency-domain approach to ICP estimation using a simple lumped, linear time-invariant model of cerebrovascular dynamics. Preliminary results from 28 records of patients with severe traumatic brain injury are presented and discussed. Suggestions for future work to improve the estimation algorithm are proposed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/82391",
    "title": "Model-based compressive sensing with Earth Mover's Distance constraints",
    "abstract": "In compressive sensing, we want to recover ... from linear measurements of the form ... describes the measurement process. Standard results in compressive sensing show that it is possible to exactly recover the signal x from only m ... measurements for certain types of matrices. Model-based compressive sensing reduces the number of measurements even further by limiting the supports of x to a subset of the ... possible supports. Such a family of supports is called a structured sparsity model. In this thesis, we introduce a structured sparsity model for two-dimensional signals that have similar support in neighboring columns. We quantify the change in support between neighboring columns with the Earth Mover's Distance (EMD), which measures both how many elements of the support change and how far the supported elements move. We prove that for a reasonable limit on the EMD between adjacent columns, we can recover signals in our model from only ... measurements, where w is the width of the signal. This is an asymptotic improvement over the ... bound in standard compressive sensing. While developing the algorithmic tools for our proposed structured sparsity model, we also extend the model-based compressed sensing framework. In order to use a structured sparsity model in compressive sensing, we need a model projection algorithm that, given an arbitrary signal x, returns the best approximation in the model. We relax this constraint and develop a variant of IHT, an existing sparse recovery algorithm, that works with approximate model projection algorithms.",
    "advisors": ["Piotr Indyk"],
    "text": "Model-based compressive sensing with Earth Mover's Distance constraints In compressive sensing, we want to recover ... from linear measurements of the form ... describes the measurement process. Standard results in compressive sensing show that it is possible to exactly recover the signal x from only m ... measurements for certain types of matrices. Model-based compressive sensing reduces the number of measurements even further by limiting the supports of x to a subset of the ... possible supports. Such a family of supports is called a structured sparsity model. In this thesis, we introduce a structured sparsity model for two-dimensional signals that have similar support in neighboring columns. We quantify the change in support between neighboring columns with the Earth Mover's Distance (EMD), which measures both how many elements of the support change and how far the supported elements move. We prove that for a reasonable limit on the EMD between adjacent columns, we can recover signals in our model from only ... measurements, where w is the width of the signal. This is an asymptotic improvement over the ... bound in standard compressive sensing. While developing the algorithmic tools for our proposed structured sparsity model, we also extend the model-based compressed sensing framework. In order to use a structured sparsity model in compressive sensing, we need a model projection algorithm that, given an arbitrary signal x, returns the best approximation in the model. We relax this constraint and develop a variant of IHT, an existing sparse recovery algorithm, that works with approximate model projection algorithms."
}, {
    "id": "oai:dspace.mit.edu:1721.1/18007",
    "title": "Graph-based privacy preference expression for the semantic web",
    "abstract": "The Web is changing. Originally a medium for human-readable documents, the next generation Semantic Web is opening up toolkit that provides vast opportunities for sharing information that is encoded in a machine-readable form. The opportunities for sharing data also are opportunities for encroaching on people's privacy. While some technologies already exist for expressing privacy preferences, they do not integrate closely with the Semantic Web. In addition, previous approaches to privacy expression do not easily expand to data shared over multiple hops, with different privacy preferences at each hop. The Private Information Management Agent is an attempt to mitigate these concerns.",
    "advisors": ["Daniel J. Weitzner", "Ronald L. Rivest"],
    "text": "Graph-based privacy preference expression for the semantic web The Web is changing. Originally a medium for human-readable documents, the next generation Semantic Web is opening up toolkit that provides vast opportunities for sharing information that is encoded in a machine-readable form. The opportunities for sharing data also are opportunities for encroaching on people's privacy. While some technologies already exist for expressing privacy preferences, they do not integrate closely with the Semantic Web. In addition, previous approaches to privacy expression do not easily expand to data shared over multiple hops, with different privacy preferences at each hop. The Private Information Management Agent is an attempt to mitigate these concerns."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37099",
    "title": "A Web application to improve emotional awareness in high-functioning autistics",
    "abstract": "The web application built here is based on the idea of presenting scenarios to users, using text, and having the users choose likely emotions that match the scenarios. Taken for granted by most neurotypical people, high-functioning autistics are often lacking in this area of social-skill development. This idea of emotion to scenario matching is accomplished using a series of different games that take different approaches to exercise these skills. The application relies on the two main Artificial Intelligence (AI) approaches. The first AI approach is classical, relying on computer-based algorithms developed by others to judge text and put out the correct affect or emotion. The other part of the AI relies on users of the system contributing via regular usage or explicit correction to train the system in a type of feedback loop.",
    "advisors": ["Henry Lieberman"],
    "text": "A Web application to improve emotional awareness in high-functioning autistics The web application built here is based on the idea of presenting scenarios to users, using text, and having the users choose likely emotions that match the scenarios. Taken for granted by most neurotypical people, high-functioning autistics are often lacking in this area of social-skill development. This idea of emotion to scenario matching is accomplished using a series of different games that take different approaches to exercise these skills. The application relies on the two main Artificial Intelligence (AI) approaches. The first AI approach is classical, relying on computer-based algorithms developed by others to judge text and put out the correct affect or emotion. The other part of the AI relies on users of the system contributing via regular usage or explicit correction to train the system in a type of feedback loop."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33201",
    "title": "Optimal control of controllable switched systems",
    "abstract": "Many of the existing techniques for controlling switched systems either require the solution to a complex optimization problem or significant sacrifices to either stability or performance to offer practical controllers. In [13], it is shown that stabilizing, practical controllers with meaningful performance guarantees can be constructed for a specific class of hybrid systems by parameterizing the controller actions by a finite set. We extend this approach to the control of controllable switched systems by constraining the switching portion of the control input and fixing the feedback controller for each subsystem. We show that, under reasonable assumptions, the resulting system is guaranteed to converge to the target while providing meaningful performance. We apply our approach to the direct-injection stratified charge (DISC) engine and compare the results to that of a model predictive controller designed for the same application.",
    "advisors": ["Munther A. Dahleh"],
    "text": "Optimal control of controllable switched systems Many of the existing techniques for controlling switched systems either require the solution to a complex optimization problem or significant sacrifices to either stability or performance to offer practical controllers. In [13], it is shown that stabilizing, practical controllers with meaningful performance guarantees can be constructed for a specific class of hybrid systems by parameterizing the controller actions by a finite set. We extend this approach to the control of controllable switched systems by constraining the switching portion of the control input and fixing the feedback controller for each subsystem. We show that, under reasonable assumptions, the resulting system is guaranteed to converge to the target while providing meaningful performance. We apply our approach to the direct-injection stratified charge (DISC) engine and compare the results to that of a model predictive controller designed for the same application."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16825",
    "title": "A framework for multi-modal input in a pervasive computing environment",
    "abstract": "In this thesis, we propose a framework that uses multiple-domains and multi-modal techniques to disambiguate a variety of natural human input modes. This system is based on the input needs of pervasive computing users. The work extends the Galaxy architecture developed by the Spoken Language Systems group at MIT. Just as speech recognition disambiguates an input wave form by using a grammar to find the best matching phrase, we use the same mechanism to disambiguate other input forms, T9 in particular. A skeleton version of the framework was implemented to show this framework is possible and to explore some of the issues that might arise. The system currently works for both T9 and Speech modes. The framework also includes potential for any other type of input for which a recognizer can be built such as graffiti input.",
    "advisors": ["Larry Rudolph"],
    "text": "A framework for multi-modal input in a pervasive computing environment In this thesis, we propose a framework that uses multiple-domains and multi-modal techniques to disambiguate a variety of natural human input modes. This system is based on the input needs of pervasive computing users. The work extends the Galaxy architecture developed by the Spoken Language Systems group at MIT. Just as speech recognition disambiguates an input wave form by using a grammar to find the best matching phrase, we use the same mechanism to disambiguate other input forms, T9 in particular. A skeleton version of the framework was implemented to show this framework is possible and to explore some of the issues that might arise. The system currently works for both T9 and Speech modes. The framework also includes potential for any other type of input for which a recognizer can be built such as graffiti input."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34370",
    "title": "Active pixel sensors for X-ray astronomy",
    "abstract": "An active pixel sensor array, APS-1, has been fabricated for the purpose of scientific x-ray detection. This thesis presents the results of testing the device. Alternate design architectures are explored. Recommendations are made for a next-generation sensor. CCDs have been the dominant x-ray sensor in astronomy for over ten years. Limitations inherent to CCDs are starting to become important. Active pixel sensors (APS) provide an alternate architecture that may solve these problems. APS-1 is a first-generation sensor designed by Lincoln Laboratory's Advanced Silicon Technology Group. APS-1 is fabricated in a fully depleted silicon-on-insulator (FDSOI) technology. FDSOI is especially well-suited to produce a scientific x-ray imager. The device includes sixteen different pixel variations to determine the processing parameters that can produce the best imager. Dark current, noise, and responsivity of the various pixel designs was measured using an electronics system adapted from a CCD test system. X-rays were detected at room temperature. Ordinary active pixels have high noise levels ( 70 electrons). Many pixel designs capable of lower noise have been presented in the literature. Active reset, pixel-level CDS, and CTIA pixel designs are discussed in detail and simulated. A second-generation sensor from Lincoln Laboratory, using pixel-level CDS, is discussed. This device, APS-2, will be available for testing in 2006. APS-2 simulation results are presented. It is expected to have an input-referred noise of less than five electrons, near the performance of modern CCDs.",
    "advisors": ["Mark W. Bautz", "Kent H. Lundberg"],
    "text": "Active pixel sensors for X-ray astronomy An active pixel sensor array, APS-1, has been fabricated for the purpose of scientific x-ray detection. This thesis presents the results of testing the device. Alternate design architectures are explored. Recommendations are made for a next-generation sensor. CCDs have been the dominant x-ray sensor in astronomy for over ten years. Limitations inherent to CCDs are starting to become important. Active pixel sensors (APS) provide an alternate architecture that may solve these problems. APS-1 is a first-generation sensor designed by Lincoln Laboratory's Advanced Silicon Technology Group. APS-1 is fabricated in a fully depleted silicon-on-insulator (FDSOI) technology. FDSOI is especially well-suited to produce a scientific x-ray imager. The device includes sixteen different pixel variations to determine the processing parameters that can produce the best imager. Dark current, noise, and responsivity of the various pixel designs was measured using an electronics system adapted from a CCD test system. X-rays were detected at room temperature. Ordinary active pixels have high noise levels ( 70 electrons). Many pixel designs capable of lower noise have been presented in the literature. Active reset, pixel-level CDS, and CTIA pixel designs are discussed in detail and simulated. A second-generation sensor from Lincoln Laboratory, using pixel-level CDS, is discussed. This device, APS-2, will be available for testing in 2006. APS-2 simulation results are presented. It is expected to have an input-referred noise of less than five electrons, near the performance of modern CCDs."
}, {
    "id": "oai:dspace.mit.edu:1721.1/110868",
    "title": "Integrated LIDAR with optical phased arrays in silicon photonics",
    "abstract": "Light detection and ranging (LIDAR) has become an ubiquitous ranging technology. LIDAR systems are integral to almost all autonomous vehicles and robotics. Most LIDAR systems today use discrete free-space optical components and utilize a mechanical apparatus for beam steering. Apart from the relative high cost of the system, this mechanical apparatus limits the scan rate of the LIDAR system while increasing both size and complexity. This leads to concerns about long-term reliability, especially in harsh environments. In this thesis, the design and experimental results of an integrated chip-scale frequency-modulated continuous-wave LIDAR system are presented. This system has the capability of measuring both distance and velocity simultaneously with a 20mm resolution and a 2m range. Its functionality is then extended by utilizing optical phased arrays as a transmitter and receiver for solid-state beam steering. The phased array utilized has a grouped cascaded phase shifter architecture and is shown to have a steering range of 46°x36°. This is the first integrated coherent LIDAR system based on optical phased arrays. In order to have a viable LIDAR system with optical phased arrays, high beam powers and large aperture sizes are needed. A silicon nitride distribution network is used to enable high on-chip power because of the low material nonlinearities. An ultra-high main beam power of 520mW is reported. A phased array is demonstrated with an ultra-large aperture size of 4x4mm2, achieving a record-small and near diffraction limited spot size of 0.021°x0.021° with a side lobe suppression of 10 dB. This is the largest optical phased array to date by an order of magnitude and shows the scalability of optical phased arrays. Finally, an optical phased array at a visible wavelength of 635nm is shown with an aperture size of 0.5x0.5mm2 and a spot size of 0.064°x0.074°. This demonstration moves large-scale integrated photonics into the visible spectrum and has potential applications in bathymetric LIDAR.",
    "advisors": ["Michael R. Watts"],
    "text": "Integrated LIDAR with optical phased arrays in silicon photonics Light detection and ranging (LIDAR) has become an ubiquitous ranging technology. LIDAR systems are integral to almost all autonomous vehicles and robotics. Most LIDAR systems today use discrete free-space optical components and utilize a mechanical apparatus for beam steering. Apart from the relative high cost of the system, this mechanical apparatus limits the scan rate of the LIDAR system while increasing both size and complexity. This leads to concerns about long-term reliability, especially in harsh environments. In this thesis, the design and experimental results of an integrated chip-scale frequency-modulated continuous-wave LIDAR system are presented. This system has the capability of measuring both distance and velocity simultaneously with a 20mm resolution and a 2m range. Its functionality is then extended by utilizing optical phased arrays as a transmitter and receiver for solid-state beam steering. The phased array utilized has a grouped cascaded phase shifter architecture and is shown to have a steering range of 46°x36°. This is the first integrated coherent LIDAR system based on optical phased arrays. In order to have a viable LIDAR system with optical phased arrays, high beam powers and large aperture sizes are needed. A silicon nitride distribution network is used to enable high on-chip power because of the low material nonlinearities. An ultra-high main beam power of 520mW is reported. A phased array is demonstrated with an ultra-large aperture size of 4x4mm2, achieving a record-small and near diffraction limited spot size of 0.021°x0.021° with a side lobe suppression of 10 dB. This is the largest optical phased array to date by an order of magnitude and shows the scalability of optical phased arrays. Finally, an optical phased array at a visible wavelength of 635nm is shown with an aperture size of 0.5x0.5mm2 and a spot size of 0.064°x0.074°. This demonstration moves large-scale integrated photonics into the visible spectrum and has potential applications in bathymetric LIDAR."
}, {
    "id": "oai:dspace.mit.edu:1721.1/92973",
    "title": "Power monitoring in integrated circuits",
    "abstract": "Power monitoring is needed in most electrical systems, and is crucial for ensuring reliability in everything from industrial and telecom applications, to automotive and consumer electronics. Power monitoring of integrated circuits (ICs) is also essential, as today ICs exist in most electrical and electronic systems, in a vast range of applications. Many ICs, including power ICs, have functional blocks across the chip that are used for different purposes. Measuring circuit block currents in both analog and digital ICs is important in a wide range of applications, including power management as well as IC testing and fault detection and analysis. For example, the presence of different kinds of faults in IC circuit blocks during IC fabrication causes the currents flowing through these circuit blocks to change from the expected values. There has been general interest in monitoring currents through different circuit blocks in an attempt to identify the location and type of the faults. Previous works on non intrusive load monitoring as well as on power-line communications (PLCs) provide motivation for the work presented here. The techniques are extended and used to develop a new method for power monitoring in ICs. Most solutions to the challenge of measuring currents in different circuit blocks of the IC involve adding circuitry that is both costly and power consuming. In this work, a new method is proposed to enable individual measurement of current consumed in each circuit block within an IC while adding negligible area and power overhead. This method works by encoding the individual current signatures in the main supply current of the IC, which can then be sensed and sampled off-chip, and then disaggregated through signal processing. A demonstration of this power monitoring scheme is given on a modular discrete platform that is implemented based on the UC3842 current-mode controller IC, which can also be used for educational purposes.",
    "advisors": ["Steven B. Leeb", "Al-Thaddeus Avestruz"],
    "text": "Power monitoring in integrated circuits Power monitoring is needed in most electrical systems, and is crucial for ensuring reliability in everything from industrial and telecom applications, to automotive and consumer electronics. Power monitoring of integrated circuits (ICs) is also essential, as today ICs exist in most electrical and electronic systems, in a vast range of applications. Many ICs, including power ICs, have functional blocks across the chip that are used for different purposes. Measuring circuit block currents in both analog and digital ICs is important in a wide range of applications, including power management as well as IC testing and fault detection and analysis. For example, the presence of different kinds of faults in IC circuit blocks during IC fabrication causes the currents flowing through these circuit blocks to change from the expected values. There has been general interest in monitoring currents through different circuit blocks in an attempt to identify the location and type of the faults. Previous works on non intrusive load monitoring as well as on power-line communications (PLCs) provide motivation for the work presented here. The techniques are extended and used to develop a new method for power monitoring in ICs. Most solutions to the challenge of measuring currents in different circuit blocks of the IC involve adding circuitry that is both costly and power consuming. In this work, a new method is proposed to enable individual measurement of current consumed in each circuit block within an IC while adding negligible area and power overhead. This method works by encoding the individual current signatures in the main supply current of the IC, which can then be sensed and sampled off-chip, and then disaggregated through signal processing. A demonstration of this power monitoring scheme is given on a modular discrete platform that is implemented based on the UC3842 current-mode controller IC, which can also be used for educational purposes."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37100",
    "title": "Identifying and modeling unwanted traffic on the Internet",
    "abstract": "Accurate models of Internet traffic are important for successful testing of devices that provide network security. However, with the growth of the Internet. it has become increasingly difficult to develop and maintain accurate traffic models. While much internet traffic is legitimate, productive communications between users and services, a significant portion of Internet traffic is the result of unwanted messages sent to IP addresses without regard as to whether there is an active host at that address. In an effort to analyze unwanted traffic, tools were developed that generate statistics and plots on captured unwanted traffic to unused IP addresses. These tools were used on a four-day period of traffic received on an inactive IPv4 class A network address space. Each class B subnet in this address space received an average of 7 million packets corresponding to 21 packets per second. Analyses were performed on a range of class B and C subnets with the intent of discovering the types of variability that are characteristic of unwanted traffic. Traffic volume over time, number of scans, destinations ports, and traffic sources varied substantially across class B and C subnets.",
    "advisors": ["Richard Lippmann"],
    "text": "Identifying and modeling unwanted traffic on the Internet Accurate models of Internet traffic are important for successful testing of devices that provide network security. However, with the growth of the Internet. it has become increasingly difficult to develop and maintain accurate traffic models. While much internet traffic is legitimate, productive communications between users and services, a significant portion of Internet traffic is the result of unwanted messages sent to IP addresses without regard as to whether there is an active host at that address. In an effort to analyze unwanted traffic, tools were developed that generate statistics and plots on captured unwanted traffic to unused IP addresses. These tools were used on a four-day period of traffic received on an inactive IPv4 class A network address space. Each class B subnet in this address space received an average of 7 million packets corresponding to 21 packets per second. Analyses were performed on a range of class B and C subnets with the intent of discovering the types of variability that are characteristic of unwanted traffic. Traffic volume over time, number of scans, destinations ports, and traffic sources varied substantially across class B and C subnets."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42120",
    "title": "Design and editing 2.5-dimensional terrain in StarLogo TNG",
    "abstract": "StarLogo TNG is \"The Next Generation\" in block-based decentralized programming for modeling and simulation software. Its aim is to make computer programming more appealing for students in middle school and high school. Part of the draw of StarLogo TNG is its 3-D rendered world called Spaceland where \"agents\" live on a terrain made of a grid of \"patches\". This thesis evaluates and outlines the redesign of Spaceland and its associated terrain editor based on user-task analysis, and discusses the design of new data structures to support the desired features.",
    "advisors": ["Eric Klopfer"],
    "text": "Design and editing 2.5-dimensional terrain in StarLogo TNG StarLogo TNG is \"The Next Generation\" in block-based decentralized programming for modeling and simulation software. Its aim is to make computer programming more appealing for students in middle school and high school. Part of the draw of StarLogo TNG is its 3-D rendered world called Spaceland where \"agents\" live on a terrain made of a grid of \"patches\". This thesis evaluates and outlines the redesign of Spaceland and its associated terrain editor based on user-task analysis, and discusses the design of new data structures to support the desired features."
}, {
    "id": "oai:dspace.mit.edu:1721.1/53142",
    "title": "Enabling diagnostics in user interfaces for CAD applications",
    "abstract": "Computer aided design (CAD) applications such as Autodesk Civil 3D allow the user to specify design constraints for a number of common geometries. These applications typically prompt the user for the required constraints and then attempt to find a feasible solution. When there is no feasible solution, however, there is little or no explanation given to the user. Furthermore, given the number of degrees of freedom, it is unreasonable to expect the user to be able to analyze the solution space of the problem in order to correct his input. In this thesis I describe an extension to the geometric solvers in Civil 3D that will enable new user interfaces to assist the user in correcting his input. Furthermore I present several example user interfaces that demonstrate these new capabilities.",
    "advisors": ["Daniel Philbrick", "Srini Devadas"],
    "text": "Enabling diagnostics in user interfaces for CAD applications Computer aided design (CAD) applications such as Autodesk Civil 3D allow the user to specify design constraints for a number of common geometries. These applications typically prompt the user for the required constraints and then attempt to find a feasible solution. When there is no feasible solution, however, there is little or no explanation given to the user. Furthermore, given the number of degrees of freedom, it is unreasonable to expect the user to be able to analyze the solution space of the problem in order to correct his input. In this thesis I describe an extension to the geometric solvers in Civil 3D that will enable new user interfaces to assist the user in correcting his input. Furthermore I present several example user interfaces that demonstrate these new capabilities."
}, {
    "id": "oai:dspace.mit.edu:1721.1/41656",
    "title": "Implementation of H.264 Decoder in Bluespec SystemVerilog",
    "abstract": "In this thesis, I present a implementation of a H.264 decoder designed in Bluespec SystemVerilog, a high level hardware description language. This design is intended to serve both as a more understandable reference code, as well as a starting point for efficient hardware implementations. I illustrate this by modifying this initial design to meet a performance requirement of 720p at 60 frames per second.",
    "advisors": ["Arvind"],
    "text": "Implementation of H.264 Decoder in Bluespec SystemVerilog In this thesis, I present a implementation of a H.264 decoder designed in Bluespec SystemVerilog, a high level hardware description language. This design is intended to serve both as a more understandable reference code, as well as a starting point for efficient hardware implementations. I illustrate this by modifying this initial design to meet a performance requirement of 720p at 60 frames per second."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120368",
    "title": "Dielectric reliability in GaN metal-insulator-semiconductor high electron mobility transistors",
    "abstract": "GaN Metal Insulator Semiconductor High Electron Mobility Transistors (GaN MIS-HEMTs) show excellent promise as high voltage power transistors that can operate efficiently at high temperatures and frequencies. However, current GaN technology faces several obstacles, one of which is Time-Dependent Dielectric Breakdown (TDDB) of the gate dielectric. Under prolonged electrical stress, the gate dielectric suffers a catastrophic breakdown that renders the transistor useless. Understanding the physics behind gate dielectric breakdown and accurately estimating the average time to failure of the dielectric are of critical importance. TDDB is conventionally studied under DC conditions. However, as actual device operation in power circuits involves rapid switching between on and off states, it is important to determine if estimations done from DC stress results are accurate. Due to the rich dynamics of the GaN MIS-HEMT system such as electron trapping and carrier accumulation at the dielectric/AlGaN interface, unaccounted physics might be introduced under AC stress that may cause error in DC estimation. To this end, we characterize TDDB behavior of GaN MIS-HEMTs at both DC stress conditions and more accurate AC stress conditions. We find that TDDB behavior is improved for AC stress compared to DC stress conditions at high stress frequencies. At 100 kHz, the average dielectric breakdown time is twice the average dielectric breakdown time under DC stress conditions. Furthermore, the impact of tensile mechanical stress on TDDB under DC stress is investigated. This is an important concern because of the piezoelectric nature of GaN and the substantial lattice mismatch between Si, GaN and AlGaN that results in high mechanical strain in the active portion of the device. If mechanical stress significantly impacts TDDB, designers will have to work with further constraints to ensure minimal stress across the dielectric. To address this, we have carried out measurements of TDDB under [epsilon] = 0.29% tensile strain. We find that TDDB in both the On-state and Off-state stress conditions are unaffected by this mechanical stress. Through measurements done in this thesis, we gather further insight towards understanding the physics behind TDDB. Through AC stress we find that the dynamics of the GaN MIS-HEMTs prolong dielectric breakdown times. Through mechanical stress we find that modulation of the 2-Dimensional Electron Gas and dielectric bond straining have minimal impact on TDDB.",
    "advisors": ["Jesús A. del Alamo"],
    "text": "Dielectric reliability in GaN metal-insulator-semiconductor high electron mobility transistors GaN Metal Insulator Semiconductor High Electron Mobility Transistors (GaN MIS-HEMTs) show excellent promise as high voltage power transistors that can operate efficiently at high temperatures and frequencies. However, current GaN technology faces several obstacles, one of which is Time-Dependent Dielectric Breakdown (TDDB) of the gate dielectric. Under prolonged electrical stress, the gate dielectric suffers a catastrophic breakdown that renders the transistor useless. Understanding the physics behind gate dielectric breakdown and accurately estimating the average time to failure of the dielectric are of critical importance. TDDB is conventionally studied under DC conditions. However, as actual device operation in power circuits involves rapid switching between on and off states, it is important to determine if estimations done from DC stress results are accurate. Due to the rich dynamics of the GaN MIS-HEMT system such as electron trapping and carrier accumulation at the dielectric/AlGaN interface, unaccounted physics might be introduced under AC stress that may cause error in DC estimation. To this end, we characterize TDDB behavior of GaN MIS-HEMTs at both DC stress conditions and more accurate AC stress conditions. We find that TDDB behavior is improved for AC stress compared to DC stress conditions at high stress frequencies. At 100 kHz, the average dielectric breakdown time is twice the average dielectric breakdown time under DC stress conditions. Furthermore, the impact of tensile mechanical stress on TDDB under DC stress is investigated. This is an important concern because of the piezoelectric nature of GaN and the substantial lattice mismatch between Si, GaN and AlGaN that results in high mechanical strain in the active portion of the device. If mechanical stress significantly impacts TDDB, designers will have to work with further constraints to ensure minimal stress across the dielectric. To address this, we have carried out measurements of TDDB under [epsilon] = 0.29% tensile strain. We find that TDDB in both the On-state and Off-state stress conditions are unaffected by this mechanical stress. Through measurements done in this thesis, we gather further insight towards understanding the physics behind TDDB. Through AC stress we find that the dynamics of the GaN MIS-HEMTs prolong dielectric breakdown times. Through mechanical stress we find that modulation of the 2-Dimensional Electron Gas and dielectric bond straining have minimal impact on TDDB."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33381",
    "title": "Graphical real-time simulation tool for passive UHF RFID environments",
    "abstract": "In this thesis, I present the design and implementation of a real-time simulation tool, RFID Vis, that is used to simulate a UHF RFID environment. The simulation tool simulates environments containing to pallets of cases as is common in parts of the supply chain. The simulation tool consists of two parts, a graphical front end which interfaces with the user as well as displays the electromagnetic power present in a given volume of space in an intuitive manner and an electromagnetics simulation engine which takes care of all the electromagnetic calculations and approximations. The simulation tool is written in C++ using Microsoft DirectX 9.0 to interface with the graphics hardware. RFID Vis enables users to quickly simulate a real world operating scenario providing insights and building intuition.",
    "advisors": ["Daniel W. Engels"],
    "text": "Graphical real-time simulation tool for passive UHF RFID environments In this thesis, I present the design and implementation of a real-time simulation tool, RFID Vis, that is used to simulate a UHF RFID environment. The simulation tool simulates environments containing to pallets of cases as is common in parts of the supply chain. The simulation tool consists of two parts, a graphical front end which interfaces with the user as well as displays the electromagnetic power present in a given volume of space in an intuitive manner and an electromagnetics simulation engine which takes care of all the electromagnetic calculations and approximations. The simulation tool is written in C++ using Microsoft DirectX 9.0 to interface with the graphics hardware. RFID Vis enables users to quickly simulate a real world operating scenario providing insights and building intuition."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34148",
    "title": "A device user interface for the guided ablative therapy of cardiac arrhythmias",
    "abstract": "Radio Frequency Ablation (RFA) of cardiac arrhythmias involves the guidance of an ablation catheter to the site of the arrhythmia and the administration of a high-intensity radio-frequency current to the tissue. The current technique used to locate the arrhythmic site suffers from a number of drawbacks. Ablation is a trial-and-error procedure and may require many hours, during which the arrhythmia is ongoing. Patients with hemodynamically unstable VT are therefore excluded, as are those with more complex arrhythmias, accounting for an estimated 90% of patients. Furthermore, the technique is only successful in 71% to 76% of the cases to which it is applied. A new algorithm was recently identified that allows the non-invasive and rapid detection of the origin of an arrhythmia from body-surface ECG signals, making the RFA procedure accessible to many patients hitherto excluded. Software implementing this algorithm, and providing a multi-layer graphical user-interface to operate in conjunction with an RFA device, has been designed and implemented. If used in tandem with commercially available ECG and ablation catheter devices, this software will allow cardiologists to deliver ablating currents much more precisely and more quickly than is currently possible, and reach a far wider group of patients.",
    "advisors": ["Richard J. Cohen"],
    "text": "A device user interface for the guided ablative therapy of cardiac arrhythmias Radio Frequency Ablation (RFA) of cardiac arrhythmias involves the guidance of an ablation catheter to the site of the arrhythmia and the administration of a high-intensity radio-frequency current to the tissue. The current technique used to locate the arrhythmic site suffers from a number of drawbacks. Ablation is a trial-and-error procedure and may require many hours, during which the arrhythmia is ongoing. Patients with hemodynamically unstable VT are therefore excluded, as are those with more complex arrhythmias, accounting for an estimated 90% of patients. Furthermore, the technique is only successful in 71% to 76% of the cases to which it is applied. A new algorithm was recently identified that allows the non-invasive and rapid detection of the origin of an arrhythmia from body-surface ECG signals, making the RFA procedure accessible to many patients hitherto excluded. Software implementing this algorithm, and providing a multi-layer graphical user-interface to operate in conjunction with an RFA device, has been designed and implemented. If used in tandem with commercially available ECG and ablation catheter devices, this software will allow cardiologists to deliver ablating currents much more precisely and more quickly than is currently possible, and reach a far wider group of patients."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34677",
    "title": "Optical studies of super-collimation in photonic crystals",
    "abstract": "Recent developments in material science and engineering have made possible the fabrication of photonic crystals for optical wavelengths. These periodic structures of alternating high-to-low index of refraction materials allow the observation of peculiar effects, in particular, the propagation of optical beams without spatial spreading. This effect, called super-collimation (also known as self-collimation), allows diffraction-free propagation of micron-sized beams over centimeter-scale distances. This linear effect is a natural result of the unique dispersive properties of photonic crystals. In this thesis, these dispersive properties are studied in a two-dimensional photonic crystal slab. Both qualitative and quantitative descriptions are presented. The beam propagation method was used to simulate the evolution of a Gaussian beam inside such structures. The wavelength dependence of the super-collimation effect was studied, and it was observed that the optimum wavelength for this device was around 1500 nm. A precise contact-mode near-field optical microscopy technique was used to obtain high-resolution images of the beam profile at different positions along the photonic crystal, and showed that a 2 [micro]m beam width was conserved over 3 mm. In addition, high-resolution confocal measurements confirmed the size of the beam after 5 mm of propagation.",
    "advisors": ["Erich P. Ippen"],
    "text": "Optical studies of super-collimation in photonic crystals Recent developments in material science and engineering have made possible the fabrication of photonic crystals for optical wavelengths. These periodic structures of alternating high-to-low index of refraction materials allow the observation of peculiar effects, in particular, the propagation of optical beams without spatial spreading. This effect, called super-collimation (also known as self-collimation), allows diffraction-free propagation of micron-sized beams over centimeter-scale distances. This linear effect is a natural result of the unique dispersive properties of photonic crystals. In this thesis, these dispersive properties are studied in a two-dimensional photonic crystal slab. Both qualitative and quantitative descriptions are presented. The beam propagation method was used to simulate the evolution of a Gaussian beam inside such structures. The wavelength dependence of the super-collimation effect was studied, and it was observed that the optimum wavelength for this device was around 1500 nm. A precise contact-mode near-field optical microscopy technique was used to obtain high-resolution images of the beam profile at different positions along the photonic crystal, and showed that a 2 [micro]m beam width was conserved over 3 mm. In addition, high-resolution confocal measurements confirmed the size of the beam after 5 mm of propagation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8570",
    "title": "Adaptive delivery of real-time streaming video",
    "abstract": "While there is an increasing demand for streaming video applications on the Internet, various network characteristics make the deployment of these applications more challenging than traditional Internet applications like email and the Web. The applications that transmit data over the Internet must cope with the time-varying bandwidth and delay characteristics of the Internet and must be resilient to packet loss. This thesis examines these challenges and presents a system design and implementation that ameliorates some of the important problems with video streaming over the Internet. Video sequences are typically compressed in a format such as MPEG-4 to achieve bandwidth efficiency. Video compression exploits redundancy between frames to achieve higher compression. However, packet loss can be detrimental to compressed video with interdependent frames because errors potentially propagate across many frames. While the need for low latency prevents the retransmission of all lost data, we leverage the characteristics of MPEG-4 to selectively retransmit only the most important data in order to limit the propagation of errors. We quantify the effects of packet loss on the quality of MPEG-4 video, develop an analytical model to explain these effects, and present an RTP-compatible protocol-which we call SR-RTP--to adaptively deliver higher quality video in the face of packet loss. The Internet's variable bandwidth and delay make it difficult to achieve high utilization, Tcp friendliness, and a high-quality constant playout rate; a video streaming system should adapt to these changing conditions and tailor the quality of the transmitted bitstream to available bandwidth. Traditional congestion avoidance schemes such as TCP's additive-increase/multiplicative/decrease (AIMD) cause large oscillations in transmission rates that degrade the perceptual quality of the video stream. To combat bandwidth variation, we design a scheme for performing quality adaptation of layered video for a general family of congestion control algorithms called binomial congestion control and show that a combination of smooth congestion control and clever receiver-buffered quality adaptation can reduce oscillations, increase interactivity, and deliver higher quality video for a given amount of buffering. We have integrated this selective reliability and quality adaptation into a publicly available software library. Using this system as a testbed, we show that the use of selective reliability can greatly increase the quality of received video, and that the use of binomial congestion control and receiver quality adaptation allow for increased user interactivity and better video quality.",
    "advisors": ["Hari Balakrishnan"],
    "text": "Adaptive delivery of real-time streaming video While there is an increasing demand for streaming video applications on the Internet, various network characteristics make the deployment of these applications more challenging than traditional Internet applications like email and the Web. The applications that transmit data over the Internet must cope with the time-varying bandwidth and delay characteristics of the Internet and must be resilient to packet loss. This thesis examines these challenges and presents a system design and implementation that ameliorates some of the important problems with video streaming over the Internet. Video sequences are typically compressed in a format such as MPEG-4 to achieve bandwidth efficiency. Video compression exploits redundancy between frames to achieve higher compression. However, packet loss can be detrimental to compressed video with interdependent frames because errors potentially propagate across many frames. While the need for low latency prevents the retransmission of all lost data, we leverage the characteristics of MPEG-4 to selectively retransmit only the most important data in order to limit the propagation of errors. We quantify the effects of packet loss on the quality of MPEG-4 video, develop an analytical model to explain these effects, and present an RTP-compatible protocol-which we call SR-RTP--to adaptively deliver higher quality video in the face of packet loss. The Internet's variable bandwidth and delay make it difficult to achieve high utilization, Tcp friendliness, and a high-quality constant playout rate; a video streaming system should adapt to these changing conditions and tailor the quality of the transmitted bitstream to available bandwidth. Traditional congestion avoidance schemes such as TCP's additive-increase/multiplicative/decrease (AIMD) cause large oscillations in transmission rates that degrade the perceptual quality of the video stream. To combat bandwidth variation, we design a scheme for performing quality adaptation of layered video for a general family of congestion control algorithms called binomial congestion control and show that a combination of smooth congestion control and clever receiver-buffered quality adaptation can reduce oscillations, increase interactivity, and deliver higher quality video for a given amount of buffering. We have integrated this selective reliability and quality adaptation into a publicly available software library. Using this system as a testbed, we show that the use of selective reliability can greatly increase the quality of received video, and that the use of binomial congestion control and receiver quality adaptation allow for increased user interactivity and better video quality."
}, {
    "id": "oai:dspace.mit.edu:1721.1/105961",
    "title": "An adaptive partitioning scheme for ad-hoc and time-varying database analytics",
    "abstract": "Data partitioning significantly improves query performance in distributed database systems. A large number of techniques have been proposed to efficiently partition a dataset, often focusing on finding the best partitioning for a particular query workload. However, many modern analytic applications involve ad-hoc or exploratory analysis where users do not have a representative query workload. Furthermore, workloads change over time as businesses evolve or as analysts gain better understanding of their data. Static workload-based data partitioning techniques are therefore not suitable for such settings. In this thesis, we present Amoeba, an adaptive distributed storage system for data skipping. It does not require an upfront query workload and adapts the data partitioning according to the queries posed by users over time. We present the data structures, partitioning algorithms, and an efficient implementation on top of Apache Spark and HDFS. Our experimental results show that the Amoeba storage system provides improved query performance for ad-hoc workloads, adapts to changes in the query workloads, and converges to a steady state in case of recurring workloads. On a real world workload, Amoeba reduces the total workload runtime by 1.8x compared to Spark with data partitioned and 3.4x compared to unmodified Spark.",
    "advisors": ["Samuel Madden"],
    "text": "An adaptive partitioning scheme for ad-hoc and time-varying database analytics Data partitioning significantly improves query performance in distributed database systems. A large number of techniques have been proposed to efficiently partition a dataset, often focusing on finding the best partitioning for a particular query workload. However, many modern analytic applications involve ad-hoc or exploratory analysis where users do not have a representative query workload. Furthermore, workloads change over time as businesses evolve or as analysts gain better understanding of their data. Static workload-based data partitioning techniques are therefore not suitable for such settings. In this thesis, we present Amoeba, an adaptive distributed storage system for data skipping. It does not require an upfront query workload and adapts the data partitioning according to the queries posed by users over time. We present the data structures, partitioning algorithms, and an efficient implementation on top of Apache Spark and HDFS. Our experimental results show that the Amoeba storage system provides improved query performance for ad-hoc workloads, adapts to changes in the query workloads, and converges to a steady state in case of recurring workloads. On a real world workload, Amoeba reduces the total workload runtime by 1.8x compared to Spark with data partitioned and 3.4x compared to unmodified Spark."
}, {
    "id": "oai:dspace.mit.edu:1721.1/26710",
    "title": "Cooperative routing in wireless networks",
    "abstract": "In this thesis, we study the problem of energy efficiency and reliability in wireless ad-hoc networks. First, we introduce the idea of wireless cooperation advantage. We formulate the problem of finding the minimum energy cooperative route for a wireless network under idealized channel and receiver models. Fundamental to the understanding of the routing problem is the understanding of the optimal power allocation for a single message transmission between two sets of nodes. We present the solution to this problem, and use that as the basis for solving the minimum energy cooperative routing problem. We analytically obtain the energy savings in regular line and regular grid networks. We propose heuristics for selecting the cooperative route in random networks and give simulation results confirming significant energy savings achieved through cooperation. In the second part, we study the problem of route reliability in a multi-hop network. We look at the reliability issue at the link level and extend those result to a wireless network setting. In the network setting, we first define and analyze the reliability for a fixed route and then propose algorithms for finding the optimal route between a source-destination pair of nodes. The relationship between the route reliability and consumed power is studied. The idea of route diversity is introduced as a way to improve the reliability by taking advantage of the broadcast property, the independence of fading state between different pairs of nodes, and space diversity created by multiple intermediate relay nodes along the route. We give analytical results on improvements due to route diversity in some simple network topologies.",
    "advisors": ["Eytan Modiano", " Lizhong Zheng", "Jinane Abounadi"],
    "text": "Cooperative routing in wireless networks In this thesis, we study the problem of energy efficiency and reliability in wireless ad-hoc networks. First, we introduce the idea of wireless cooperation advantage. We formulate the problem of finding the minimum energy cooperative route for a wireless network under idealized channel and receiver models. Fundamental to the understanding of the routing problem is the understanding of the optimal power allocation for a single message transmission between two sets of nodes. We present the solution to this problem, and use that as the basis for solving the minimum energy cooperative routing problem. We analytically obtain the energy savings in regular line and regular grid networks. We propose heuristics for selecting the cooperative route in random networks and give simulation results confirming significant energy savings achieved through cooperation. In the second part, we study the problem of route reliability in a multi-hop network. We look at the reliability issue at the link level and extend those result to a wireless network setting. In the network setting, we first define and analyze the reliability for a fixed route and then propose algorithms for finding the optimal route between a source-destination pair of nodes. The relationship between the route reliability and consumed power is studied. The idea of route diversity is introduced as a way to improve the reliability by taking advantage of the broadcast property, the independence of fading state between different pairs of nodes, and space diversity created by multiple intermediate relay nodes along the route. We give analytical results on improvements due to route diversity in some simple network topologies."
}, {
    "id": "oai:dspace.mit.edu:1721.1/100670",
    "title": "Graph analytics on relational databases",
    "abstract": "Graph analytics has become increasing popular in the recent years. Conventionally, data is stored in relational databases that have been refined over decades, resulting in highly optimized data processing engines. However, the awkwardness of expressing iterative queries in SQL makes the relational query-processing model inadequate for graph analytics, leading to many alternative solutions. Our research explores the possibility of combining a more natural query model with relational databases for graph analytics. In particular, we bring together a graph-natural vertex-centric query interface to highly optimized column-oriented relational databases, thus providing the efficiency of relational engines and ease-of-use of new graph systems. Throughout the thesis, we used stochastic gradient descent, a loss-minimization algorithm applied in many machine learning and graph analytics queries, as the example iterative algorithm. We implemented two different approaches for emulating a vertex-centric interface on a leading column-oriented database, Vertica: disk-based and main-memory based. The disk-based solution stores data for each iteration in relational tables and allows for interleaving SQL queries with graph algorithms. The main-memory approach stores data in memory, allowing faster updates. We applied optimizations to both implementations, which included refining logical and physical query plans, applying algorithm-level improvements and performing system-specific optimizations. The experiments and results show that the two implementations provide reasonable performance in comparison with popular graph processing systems. We present a detailed cost analysis of the two implementations and study the effect of each individual optimization on the query performance.",
    "advisors": ["Samuel Madden"],
    "text": "Graph analytics on relational databases Graph analytics has become increasing popular in the recent years. Conventionally, data is stored in relational databases that have been refined over decades, resulting in highly optimized data processing engines. However, the awkwardness of expressing iterative queries in SQL makes the relational query-processing model inadequate for graph analytics, leading to many alternative solutions. Our research explores the possibility of combining a more natural query model with relational databases for graph analytics. In particular, we bring together a graph-natural vertex-centric query interface to highly optimized column-oriented relational databases, thus providing the efficiency of relational engines and ease-of-use of new graph systems. Throughout the thesis, we used stochastic gradient descent, a loss-minimization algorithm applied in many machine learning and graph analytics queries, as the example iterative algorithm. We implemented two different approaches for emulating a vertex-centric interface on a leading column-oriented database, Vertica: disk-based and main-memory based. The disk-based solution stores data for each iteration in relational tables and allows for interleaving SQL queries with graph algorithms. The main-memory approach stores data in memory, allowing faster updates. We applied optimizations to both implementations, which included refining logical and physical query plans, applying algorithm-level improvements and performing system-specific optimizations. The experiments and results show that the two implementations provide reasonable performance in comparison with popular graph processing systems. We present a detailed cost analysis of the two implementations and study the effect of each individual optimization on the query performance."
}, {
    "id": "oai:dspace.mit.edu:1721.1/66454",
    "title": "Development of gallium nitride power transistors",
    "abstract": "GaN-based high-voltage transistors have outstanding properties for the development of ultra-high efficiency and compact power electronics. This thesis describes a new process technology for the fabrication of GaN power devices optimized for their use in efficient power distribution systems in computer micro-processors. An existing process flow was used to fabricate the baseline single-finger transistors and additional process steps were developed and optimized to fabricate multi-finger devices with total gate widths up to 12mm. These transistors offer the current and on-resistance levels required by future GaN-based power converters. Transistors with various gate widths were fabricated and characterized by DC and capacitancevoltage measurements to study how the main transistor metrics scale with gate width.",
    "advisors": ["Tomás Palacios"],
    "text": "Development of gallium nitride power transistors GaN-based high-voltage transistors have outstanding properties for the development of ultra-high efficiency and compact power electronics. This thesis describes a new process technology for the fabrication of GaN power devices optimized for their use in efficient power distribution systems in computer micro-processors. An existing process flow was used to fabricate the baseline single-finger transistors and additional process steps were developed and optimized to fabricate multi-finger devices with total gate widths up to 12mm. These transistors offer the current and on-resistance levels required by future GaN-based power converters. Transistors with various gate widths were fabricated and characterized by DC and capacitancevoltage measurements to study how the main transistor metrics scale with gate width."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28449",
    "title": "VISTA : a visualization tool for computer architects",
    "abstract": "As computer architectures continue to grow in complexity, software developers and hardware engineers cope with the increasing complexity by developing proprietary applications, simulations and tool sets to understand the behavior of these complex systems. Although the field of information visualization is leading to powerful applications in many areas, information visualization applications for computer architecture development are either tightly coupled with a specific architecture or target a wide range of computer system data. This thesis introduces the Visualization Tool for Computer Architects (VISTA) Environment. The VISTA Environment is an extensible and modular information visualization environment for hardware engineers, software developers and educators to visualize data from a variety of computer architecture simulations at different levels of abstraction. The VISTA Environment leverages common attributes in simulation data, computer architecture visualizations, and computer architecture development methods to create a powerful information visualization environment to aid in designing, understanding and communicating complex computer architectures.",
    "advisors": ["Krste AsanoviÄ"],
    "text": "VISTA : a visualization tool for computer architects As computer architectures continue to grow in complexity, software developers and hardware engineers cope with the increasing complexity by developing proprietary applications, simulations and tool sets to understand the behavior of these complex systems. Although the field of information visualization is leading to powerful applications in many areas, information visualization applications for computer architecture development are either tightly coupled with a specific architecture or target a wide range of computer system data. This thesis introduces the Visualization Tool for Computer Architects (VISTA) Environment. The VISTA Environment is an extensible and modular information visualization environment for hardware engineers, software developers and educators to visualize data from a variety of computer architecture simulations at different levels of abstraction. The VISTA Environment leverages common attributes in simulation data, computer architecture visualizations, and computer architecture development methods to create a powerful information visualization environment to aid in designing, understanding and communicating complex computer architectures."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33331",
    "title": "On symbolic analysis of cryptographic protocols",
    "abstract": "The universally composable symbolic analysis (UCSA) framework layers Dolev-Yao style symbolic analysis on top of the universally composable (UC) secure framework to construct computationally sound proofs of cryptographic protocol security. The original proposal of the UCSA framework by Canetti and Herzog (2004) focused on protocols that only use public key encryption to achieve 2-party mutual authentication or key exchange. This thesis expands the framework to include protocols that use digital signatures as well. In the process of expanding the framework, we identify a flaw in the framework's use of UC ideal functionality FKE. We also identify issues that arise when combining FKE with the current formulation of ideal signature functionality FSI,. Motivated by these discoveries, we redefine the FPKE and FsIG functionalities appropriately.",
    "advisors": ["Ronald L. Rivest", "Ran Canetti"],
    "text": "On symbolic analysis of cryptographic protocols The universally composable symbolic analysis (UCSA) framework layers Dolev-Yao style symbolic analysis on top of the universally composable (UC) secure framework to construct computationally sound proofs of cryptographic protocol security. The original proposal of the UCSA framework by Canetti and Herzog (2004) focused on protocols that only use public key encryption to achieve 2-party mutual authentication or key exchange. This thesis expands the framework to include protocols that use digital signatures as well. In the process of expanding the framework, we identify a flaw in the framework's use of UC ideal functionality FKE. We also identify issues that arise when combining FKE with the current formulation of ideal signature functionality FSI,. Motivated by these discoveries, we redefine the FPKE and FsIG functionalities appropriately."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62448",
    "title": "High fidelity pattern transfer in InP photonic device fabrication",
    "abstract": "The photonic industry is driven by the information ages demand for higher bandwidth. To meet the future demands of 10 Tbit networks, photonic integrated circuits (PIC) are required. Device performance is affected by everything from component coupling to electrical connectivity of the active components. However the most fundamental and often challenging aspect of photonic device fabrication is dimensional control. At 1550 nm, line width tolerance range between 1 pm to 0.05 pm.[1] Although these tolerances are easily achieved using lithography technology such as electron beam lithography (EBL) or 193 nm projection, neither are viable optical options for InP production.[2] The purpose of this thesis is to develop a fabrication process for InP Faraday rotators using standard, high throughput lithographic and etching techniques. The Faraday rotator is a 1.4 Jim InP-InGaAsP-InP waveguide with a line width tolerance of ± 0.07 pm.",
    "advisors": ["Rajeev R. Ram"],
    "text": "High fidelity pattern transfer in InP photonic device fabrication The photonic industry is driven by the information ages demand for higher bandwidth. To meet the future demands of 10 Tbit networks, photonic integrated circuits (PIC) are required. Device performance is affected by everything from component coupling to electrical connectivity of the active components. However the most fundamental and often challenging aspect of photonic device fabrication is dimensional control. At 1550 nm, line width tolerance range between 1 pm to 0.05 pm.[1] Although these tolerances are easily achieved using lithography technology such as electron beam lithography (EBL) or 193 nm projection, neither are viable optical options for InP production.[2] The purpose of this thesis is to develop a fabrication process for InP Faraday rotators using standard, high throughput lithographic and etching techniques. The Faraday rotator is a 1.4 Jim InP-InGaAsP-InP waveguide with a line width tolerance of ± 0.07 pm."
}, {
    "id": "oai:dspace.mit.edu:1721.1/76997",
    "title": "Can silhouette execution mitigate VM boot storms?",
    "abstract": "Server virtualization enables data centers to run many VMs on individual hosts - this reduces costs, simplifies administration and facilitates management. Improvement in hardware and virtualization technology, coupled with the use of virtualization for desktop machines with modest steady-state resource utilization, is expected to allow individual hosts to run thousands of VMs at the same time. Such high VM densities per host would allow data centers to reap unprecedented cost-savings in the future. Unfortunately, unusually high CPU and memory pressure generated when many VMs boot up concurrently can cripple hosts that can otherwise run many VMs. Over provisioning hardware to avoid prohibitively high boot latencies that result from these - often daily - boot storms is clearly expensive. The aim of this thesis is to investigate whether a hypervisor could theoretically exploit the overlap in the instruction streams of concurrently booting VMs to reduce CPU pressure in boot storms. This idea, which we name silhouette execution, would allow hypervisors to use the CPU in a scalable way, much like transparent page sharing allows a hypervisor to use its limited memory in a scalable fashion. To evaluate silhouette execution, we studied user-space instruction streams from a few Linux services using dynamic instrumentation. We statistically profiled the extent of nondeterminism in program execution, and compiled the reasons behind any execution differences. Though there is significant overlap in the user-mode instruction streams of Linux services, our simple simulations show that silhouette execution would increase CPU pressure by 13% for 100 VMs and 6% for 1000 VMs. To remedy this, we present a few strategies for reducing synthetic differences in execution in user-space programs. Our simulations show that silhouette execution can reduce CPU pressure on a host by a factor of 8x for 100 VMs and a factor of 19x for 1000 VMs once these strategies are used. We believe that the insights provided in this thesis on controlling execution differences in concurrently booting VMs via dynamic instrumentation are a prelude to a successful future implementation of silhouette execution.",
    "advisors": ["Saman P. Amarasinghe"],
    "text": "Can silhouette execution mitigate VM boot storms? Server virtualization enables data centers to run many VMs on individual hosts - this reduces costs, simplifies administration and facilitates management. Improvement in hardware and virtualization technology, coupled with the use of virtualization for desktop machines with modest steady-state resource utilization, is expected to allow individual hosts to run thousands of VMs at the same time. Such high VM densities per host would allow data centers to reap unprecedented cost-savings in the future. Unfortunately, unusually high CPU and memory pressure generated when many VMs boot up concurrently can cripple hosts that can otherwise run many VMs. Over provisioning hardware to avoid prohibitively high boot latencies that result from these - often daily - boot storms is clearly expensive. The aim of this thesis is to investigate whether a hypervisor could theoretically exploit the overlap in the instruction streams of concurrently booting VMs to reduce CPU pressure in boot storms. This idea, which we name silhouette execution, would allow hypervisors to use the CPU in a scalable way, much like transparent page sharing allows a hypervisor to use its limited memory in a scalable fashion. To evaluate silhouette execution, we studied user-space instruction streams from a few Linux services using dynamic instrumentation. We statistically profiled the extent of nondeterminism in program execution, and compiled the reasons behind any execution differences. Though there is significant overlap in the user-mode instruction streams of Linux services, our simple simulations show that silhouette execution would increase CPU pressure by 13% for 100 VMs and 6% for 1000 VMs. To remedy this, we present a few strategies for reducing synthetic differences in execution in user-space programs. Our simulations show that silhouette execution can reduce CPU pressure on a host by a factor of 8x for 100 VMs and a factor of 19x for 1000 VMs once these strategies are used. We believe that the insights provided in this thesis on controlling execution differences in concurrently booting VMs via dynamic instrumentation are a prelude to a successful future implementation of silhouette execution."
}, {
    "id": "oai:dspace.mit.edu:1721.1/78535",
    "title": "Flying between obstacles with an autonomous knife-edge maneuver",
    "abstract": "We develop an aircraft and control system that is capable of repeatedly performing a high speed (7m/s or 16 MPH) \"knife-edge\" maneuver through a gap that is smaller than the aircraft's wingspan. The maneuver consists of flying towards a gap, rolling to a significant angle, accurately navigating between the obstacles, and rolling back to horizontal. The speed and roll-rate required demand a control system capable of highly precise, repeatable maneuvers. We address the necessary control theory, path planning, and hardware requirements for such a maneuver, and give a proposal for a new system that may improve upon the existing techniques.",
    "advisors": ["Russ Tedrake"],
    "text": "Flying between obstacles with an autonomous knife-edge maneuver We develop an aircraft and control system that is capable of repeatedly performing a high speed (7m/s or 16 MPH) \"knife-edge\" maneuver through a gap that is smaller than the aircraft's wingspan. The maneuver consists of flying towards a gap, rolling to a significant angle, accurately navigating between the obstacles, and rolling back to horizontal. The speed and roll-rate required demand a control system capable of highly precise, repeatable maneuvers. We address the necessary control theory, path planning, and hardware requirements for such a maneuver, and give a proposal for a new system that may improve upon the existing techniques."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28427",
    "title": "The StreamIt development tool : a programming environment for StreamIt",
    "abstract": "StreamIt [28] is a high-level programming language intended for the development of large-scale and high-performance streaming applications that are characterized by the processing of data streams by modular structures. The StreamIt Development Tool (SDT) [25] is designed to aid the coding and simultaneous code- and graph-based debugging and visualizing of programs written in StreamIt. The goal is to provide a graphical programming environment that simply and intuitively conveys the hierarchical and structured nature of the StreamIt language by visually interpreting the dynamic behavior and graph representation of a StreamIt application. Consequently, the SDT provides utilities for program creation and code editing, compilation and launch support, breakpoints and code stepping, general debugging infrastructure, help support, stream graph examination and navigation, and stream data display, modification, and tracking. A user study evaluating the SDT uncovered several problems and areas of improvement that need to be addressed before this tool can approach its goals. Assessment of the SDT's efficacy in its current state is inconclusive--the SDT demonstrates both the ability to improve and hinder a user's debugging ability. Facilitating effective coding and debugging techniques and developing for scalability are critical elements in improving the SDT's effectiveness.",
    "advisors": ["Saman Amarasinghe"],
    "text": "The StreamIt development tool : a programming environment for StreamIt StreamIt [28] is a high-level programming language intended for the development of large-scale and high-performance streaming applications that are characterized by the processing of data streams by modular structures. The StreamIt Development Tool (SDT) [25] is designed to aid the coding and simultaneous code- and graph-based debugging and visualizing of programs written in StreamIt. The goal is to provide a graphical programming environment that simply and intuitively conveys the hierarchical and structured nature of the StreamIt language by visually interpreting the dynamic behavior and graph representation of a StreamIt application. Consequently, the SDT provides utilities for program creation and code editing, compilation and launch support, breakpoints and code stepping, general debugging infrastructure, help support, stream graph examination and navigation, and stream data display, modification, and tracking. A user study evaluating the SDT uncovered several problems and areas of improvement that need to be addressed before this tool can approach its goals. Assessment of the SDT's efficacy in its current state is inconclusive--the SDT demonstrates both the ability to improve and hinder a user's debugging ability. Facilitating effective coding and debugging techniques and developing for scalability are critical elements in improving the SDT's effectiveness."
}, {
    "id": "oai:dspace.mit.edu:1721.1/66313",
    "title": "Evolutionary algorithms for compiler-enabled program autotuning",
    "abstract": "PetaBricks [4, 21, 7, 3, 5] is an implicitly parallel programming language which, through the process of autotuning, can automatically optimize programs for fast QoS-aware execution on any hardware. In this thesis we develop and evaluate two PetaBricks autotuners: INCREA and SiblingRivalry. INCREA, based on a novel bottom-up evolutionary algorithm, optimizes programs offline at compile time. SiblingRivalry improves on INCREA by optimizing online during a program's execution, dynamically adapting to changes in hardware and the operating system. Continuous adaptation is achieved through racing, where half of available resources are devoted to always-on learning. We evaluate INCREA and SiblingRivalry on a large number of real-world benchmarks, and show that our autotuners can significantly speed up PetaBricks programs with respect to many non-tuned and mis-tuned baselines. Our results indicate the need for a continuous learning loop that can optimize efficiently by exploiting online knowledge of a program's performance. The results leave open the question of how to solve the online optimization problem on all cores, i.e. without racing.",
    "advisors": ["Una-May O'Reilly"],
    "text": "Evolutionary algorithms for compiler-enabled program autotuning PetaBricks [4, 21, 7, 3, 5] is an implicitly parallel programming language which, through the process of autotuning, can automatically optimize programs for fast QoS-aware execution on any hardware. In this thesis we develop and evaluate two PetaBricks autotuners: INCREA and SiblingRivalry. INCREA, based on a novel bottom-up evolutionary algorithm, optimizes programs offline at compile time. SiblingRivalry improves on INCREA by optimizing online during a program's execution, dynamically adapting to changes in hardware and the operating system. Continuous adaptation is achieved through racing, where half of available resources are devoted to always-on learning. We evaluate INCREA and SiblingRivalry on a large number of real-world benchmarks, and show that our autotuners can significantly speed up PetaBricks programs with respect to many non-tuned and mis-tuned baselines. Our results indicate the need for a continuous learning loop that can optimize efficiently by exploiting online knowledge of a program's performance. The results leave open the question of how to solve the online optimization problem on all cores, i.e. without racing."
}, {
    "id": "oai:dspace.mit.edu:1721.1/17978",
    "title": "Vision based robot navigation",
    "abstract": "In this thesis we propose a vision-based robot navigation system that constructs a high level topological representation of the world. A robot using this system learns to recognize rooms and spaces by building a hidden Markov model of the environment. Motion planning is performed by doing bidirectional heuristic search with a discrete set of actions that account for the robot's nonholonomic constraints. The intent of this project is to create a system that allows a robot to be able to explore and to navigate in a wide variety of environments in a way that facilitates goal-oriented tasks.",
    "advisors": ["Leslie P. Kaelbling"],
    "text": "Vision based robot navigation In this thesis we propose a vision-based robot navigation system that constructs a high level topological representation of the world. A robot using this system learns to recognize rooms and spaces by building a hidden Markov model of the environment. Motion planning is performed by doing bidirectional heuristic search with a discrete set of actions that account for the robot's nonholonomic constraints. The intent of this project is to create a system that allows a robot to be able to explore and to navigate in a wide variety of environments in a way that facilitates goal-oriented tasks."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33761",
    "title": "Imitation learning of whole-body grasps",
    "abstract": "Humans often learn to manipulate objects by observing other people. In much the same way, robots can use imitation learning to pick up useful skills. A system is demonstrated here for using imitation learning to teach a robot to grasp objects using both hand and whole-body grasps, which use the arms and torso as well as hands. Demonstration grasp trajectories are created by teleoperating a simulated robot to pick up simulated objects, and stored as sequences of keyframes in which contacts with the object are gained or lost. When presented with a new object, the system compares it against the objects in a stored database to pick a demonstrated grasp used on a similar object. Both objects are modeled as a combination of primitives-boxes, cylinders, and spheres-and the primitives for each object are grouped into 'functional groups' that geometrically match parts of the new object with similar parts of the demonstration object. These functional groups are then used to map contact points from the demonstration object to the new object, and the resulting adapted keyframes are adjusted and checked for feasibility. Finally, a trajectory is found that moves among the keyframes in the adapted grasp sequence, and the full trajectory is tested for feasibility by executing it in the simulation. The system successfully uses this method to pick up 92 out of 100 randomly generated test objects in simulation.",
    "advisors": ["Tomász Lozano-Pérez"],
    "text": "Imitation learning of whole-body grasps Humans often learn to manipulate objects by observing other people. In much the same way, robots can use imitation learning to pick up useful skills. A system is demonstrated here for using imitation learning to teach a robot to grasp objects using both hand and whole-body grasps, which use the arms and torso as well as hands. Demonstration grasp trajectories are created by teleoperating a simulated robot to pick up simulated objects, and stored as sequences of keyframes in which contacts with the object are gained or lost. When presented with a new object, the system compares it against the objects in a stored database to pick a demonstrated grasp used on a similar object. Both objects are modeled as a combination of primitives-boxes, cylinders, and spheres-and the primitives for each object are grouped into 'functional groups' that geometrically match parts of the new object with similar parts of the demonstration object. These functional groups are then used to map contact points from the demonstration object to the new object, and the resulting adapted keyframes are adjusted and checked for feasibility. Finally, a trajectory is found that moves among the keyframes in the adapted grasp sequence, and the full trajectory is tested for feasibility by executing it in the simulation. The system successfully uses this method to pick up 92 out of 100 randomly generated test objects in simulation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32095",
    "title": "Predicate dispatching in the Common Lisp Object",
    "abstract": "I have added support for predicate dispatching, a powerful generalization of other dispatching mechanisms, to the Common Lisp Object System (CLOS). To demonstrate its utility, I used predicate dispatching to enhance Weyl, a computer algebra system which doubles as a CLOS library. My result is Dispatching-Enhanced Weyl (DEW), a computer algebra system that I have demonstrated to be well suited for both users and programmers.",
    "advisors": ["Howard E. Shrobe"],
    "text": "Predicate dispatching in the Common Lisp Object I have added support for predicate dispatching, a powerful generalization of other dispatching mechanisms, to the Common Lisp Object System (CLOS). To demonstrate its utility, I used predicate dispatching to enhance Weyl, a computer algebra system which doubles as a CLOS library. My result is Dispatching-Enhanced Weyl (DEW), a computer algebra system that I have demonstrated to be well suited for both users and programmers."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112843",
    "title": "Crafting certified elliptic curve cryptography implementations in Coq",
    "abstract": "Elliptic curve cryptography has become a de-facto standard for protecting the privacy and integrity of internet communications. To minimize the operational cost and enable near-universal adoption, increasingly sophisticated implementation techniques have been developed. While the complete specification of an elliptic curve cryptosystem (in terms of middle school mathematics) fits on the back of a napkin, the fast implementations span thousands of lines of low-level code and are only intelligible to a small group of experts. However, the complexity of the code makes it prone to bugs, which have rendered well-designed security systems completely ineffective. I describe a principled approach for writing crypto code simultaneously with machine-checkable functional correctness proofs that compose into an end-to-end certificate tying highly optimized C code to the simplest specification used for verification so far. Despite using template-based synthesis for creating low-level code, this workflow offers good control over performance: I was able to match the fastest C implementation of X25519 to within 1% of arithmetic instructions per inner loop and 7% of overall execution time. While the development method itself relies heavily on a proof assistant such as Coq and most techniques are explained through code snippets, every Coq feature is introduced and motivated when it is first used to accommodate a non-Coq-savvy reader.",
    "advisors": ["Adam Chlipala"],
    "text": "Crafting certified elliptic curve cryptography implementations in Coq Elliptic curve cryptography has become a de-facto standard for protecting the privacy and integrity of internet communications. To minimize the operational cost and enable near-universal adoption, increasingly sophisticated implementation techniques have been developed. While the complete specification of an elliptic curve cryptosystem (in terms of middle school mathematics) fits on the back of a napkin, the fast implementations span thousands of lines of low-level code and are only intelligible to a small group of experts. However, the complexity of the code makes it prone to bugs, which have rendered well-designed security systems completely ineffective. I describe a principled approach for writing crypto code simultaneously with machine-checkable functional correctness proofs that compose into an end-to-end certificate tying highly optimized C code to the simplest specification used for verification so far. Despite using template-based synthesis for creating low-level code, this workflow offers good control over performance: I was able to match the fastest C implementation of X25519 to within 1% of arithmetic instructions per inner loop and 7% of overall execution time. While the development method itself relies heavily on a proof assistant such as Coq and most techniques are explained through code snippets, every Coq feature is introduced and motivated when it is first used to accommodate a non-Coq-savvy reader."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28456",
    "title": "Versioning for the Haystack system",
    "abstract": "In this thesis, the design and implementation of a repository for electronic information is presented. While designed with the Haystack system in mind, the repository can easily be used by anyone familiar with RDF. This repository performs the basic task of storing the information that the user collects, in addition to automatically performing several other tasks in an effort to make retrieving the information simple and efficient. The additional tasks involve the storing of when and why information enters the repository. The hope is that this additional data will help the user when searching their repository. A person's sense of time and data dependence is a strong organizing princple that can help them locate a file simply because they remember when or why they archived it, or perhaps when they used it last. This repository allows a user to narrow the search of their information space in regards to time and dependence.",
    "advisors": ["David Karger", "Lynn Stein"],
    "text": "Versioning for the Haystack system In this thesis, the design and implementation of a repository for electronic information is presented. While designed with the Haystack system in mind, the repository can easily be used by anyone familiar with RDF. This repository performs the basic task of storing the information that the user collects, in addition to automatically performing several other tasks in an effort to make retrieving the information simple and efficient. The additional tasks involve the storing of when and why information enters the repository. The hope is that this additional data will help the user when searching their repository. A person's sense of time and data dependence is a strong organizing princple that can help them locate a file simply because they remember when or why they archived it, or perhaps when they used it last. This repository allows a user to narrow the search of their information space in regards to time and dependence."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61160",
    "title": "ALA ASIC : a standard cell library for Asynchronous Logic Automata",
    "abstract": "This thesis demonstrates a hardware library with related tools and designs for Asynchronous Logic Automata (ALA) gates in a generic 90nm process development kit that allows a direct one-to-one mapping from software to hardware. Included are basic design tools to enable writing ALA software, the necessary hardware designs for implementation, and simulation techniques for quickly verifying correctness and performance. This thesis also documents many of the hazards and opportunities for improving them including helpful variations to the ALA model, design tool needs, better simulation models, and hardware improvements. To embody software you could compile a hardware description language to an FPGA or synthesize it all the way to transistors. Alternatively, you could use your favorite high level language and run it on a standard processor. However, the widening gap between traditional models of computation and the reality of the underlying hardware has led to massive costs for design and fabrication as well as numerous issues for scalability and portability. Unlike any of these other approaches, ALA aligns computational and physical descriptions making it possible to use a direct one-to-one mapping to convert an ALA program to a circuit or other physical artifact that executes that program. No unpredictable fitters or compilers are needed and no extra expertise is needed for specific technologies. Similar to Mead-Conway design rules ALA designs trade flexibility for portability and ease of design. Unlike Mead- Conway design rules, ALA designs do not require any further verification-the design rule primitives are logical operations suitable for use in analysis at the algorithmic level. ALA separates many of the scaling issues that plague integrated circuit design by cleanly separating algorithm design from hardware engineering-improving design verification, tape-out costs (by reusing masks), yield, portability, and the ability to break designs across multiple chips. ALA designs are not limited to integrated circuits and could just as easily be implemented in microfluidics, magnetic logic, or a lattice of molecular logic gates. Although each of these technologies would require implementing a basic set of gates and tiling rules, hardware (or equivalently software) can be developed using the same deterministic noiseless digital abstraction using the same design in many different technologies.",
    "advisors": ["Neil Gershenfeld"],
    "text": "ALA ASIC : a standard cell library for Asynchronous Logic Automata This thesis demonstrates a hardware library with related tools and designs for Asynchronous Logic Automata (ALA) gates in a generic 90nm process development kit that allows a direct one-to-one mapping from software to hardware. Included are basic design tools to enable writing ALA software, the necessary hardware designs for implementation, and simulation techniques for quickly verifying correctness and performance. This thesis also documents many of the hazards and opportunities for improving them including helpful variations to the ALA model, design tool needs, better simulation models, and hardware improvements. To embody software you could compile a hardware description language to an FPGA or synthesize it all the way to transistors. Alternatively, you could use your favorite high level language and run it on a standard processor. However, the widening gap between traditional models of computation and the reality of the underlying hardware has led to massive costs for design and fabrication as well as numerous issues for scalability and portability. Unlike any of these other approaches, ALA aligns computational and physical descriptions making it possible to use a direct one-to-one mapping to convert an ALA program to a circuit or other physical artifact that executes that program. No unpredictable fitters or compilers are needed and no extra expertise is needed for specific technologies. Similar to Mead-Conway design rules ALA designs trade flexibility for portability and ease of design. Unlike Mead- Conway design rules, ALA designs do not require any further verification-the design rule primitives are logical operations suitable for use in analysis at the algorithmic level. ALA separates many of the scaling issues that plague integrated circuit design by cleanly separating algorithm design from hardware engineering-improving design verification, tape-out costs (by reusing masks), yield, portability, and the ability to break designs across multiple chips. ALA designs are not limited to integrated circuits and could just as easily be implemented in microfluidics, magnetic logic, or a lattice of molecular logic gates. Although each of these technologies would require implementing a basic set of gates and tiling rules, hardware (or equivalently software) can be developed using the same deterministic noiseless digital abstraction using the same design in many different technologies."
}, {
    "id": "oai:dspace.mit.edu:1721.1/53174",
    "title": "Improving search quality of the Google search appliance",
    "abstract": "In this thesis, we describe various experiments on the ranking function of the Google Search Appliance to improve search quality. An evolutionary computation framework is implemented and applied to optimize various parameter settings of the ranking function. We evaluate the importance of IDF in the ranking function and achieve small improvements in performance. We also examine many ways to combining the query-independent and query-dependent scores. Lastly, we perform various experiments with signals based on the positions of the query terms in the document.",
    "advisors": ["David Elworthy", "Regina Barzilay"],
    "text": "Improving search quality of the Google search appliance In this thesis, we describe various experiments on the ranking function of the Google Search Appliance to improve search quality. An evolutionary computation framework is implemented and applied to optimize various parameter settings of the ranking function. We evaluate the importance of IDF in the ranking function and achieve small improvements in performance. We also examine many ways to combining the query-independent and query-dependent scores. Lastly, we perform various experiments with signals based on the positions of the query terms in the document."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33390",
    "title": "EClerk office assistant",
    "abstract": "For decades, people have continued to collect an inordinate amount of paper documents containing important information that should be easily accessible. This paper clutter inhibits indexing this information and easily searching through it. This thesis presents the code architecture and user interface design of the Electronic Clerk, a proof-of-concept electronic office assistant. The Electronic Clerk (EClerk) is a device to assist in reducing paper clutter in the office environment. The device takes paper and speech as input, performs data binding between input streams in order to attach metadata to each document, and structures the data using the Resource Description Framework (RDF) standard. The hardware structure of EClerk consists of a dedicated computer, video camera, scanner, touchscreen, and microphone for capturing input. The software structure consists of the Galaxy speech recognition system, the Haystack information client for retrieval and modification of the collected data, optical character recognition, and a graphical user interface that provides continuous feedback to the user. Primary design principles for this device include providing continuous user feedback and robustness to imperfect input in order to provide a truly usable system.",
    "advisors": ["Seth Teller"],
    "text": "EClerk office assistant For decades, people have continued to collect an inordinate amount of paper documents containing important information that should be easily accessible. This paper clutter inhibits indexing this information and easily searching through it. This thesis presents the code architecture and user interface design of the Electronic Clerk, a proof-of-concept electronic office assistant. The Electronic Clerk (EClerk) is a device to assist in reducing paper clutter in the office environment. The device takes paper and speech as input, performs data binding between input streams in order to attach metadata to each document, and structures the data using the Resource Description Framework (RDF) standard. The hardware structure of EClerk consists of a dedicated computer, video camera, scanner, touchscreen, and microphone for capturing input. The software structure consists of the Galaxy speech recognition system, the Haystack information client for retrieval and modification of the collected data, optical character recognition, and a graphical user interface that provides continuous feedback to the user. Primary design principles for this device include providing continuous user feedback and robustness to imperfect input in order to provide a truly usable system."
}, {
    "id": "oai:dspace.mit.edu:1721.1/41639",
    "title": "Compiling array computations for the Fresh Breeze Parallel Processor",
    "abstract": "Fresh Breeze is a highly parallel architecture currently under development, which strives to provide high performance scientific computing with simple programmability. The architecture provides for multithreaded determinate execution with a write-once shared memory system. In particular, Fresh Breeze data structures must be constructed from directed acyclic graphs of immutable fixed-size chunks of memory, rather than laid out in a mutable linear memory. While this model is well suited for executing functional programs, the goal of this thesis is to see if conventional programs can be efficiently compiled for this novel memory system and parallelization model, focusing specifically on array-based linear algebra computations. We compile a subset of Java, targeting the Fresh Breeze instruction set. The compiler, using a static data-flow graph intermediate representation, performs analysis and transformations which reduce communication with the shared memory and identify opportunities for parallelization.",
    "advisors": ["Jack Dennis"],
    "text": "Compiling array computations for the Fresh Breeze Parallel Processor Fresh Breeze is a highly parallel architecture currently under development, which strives to provide high performance scientific computing with simple programmability. The architecture provides for multithreaded determinate execution with a write-once shared memory system. In particular, Fresh Breeze data structures must be constructed from directed acyclic graphs of immutable fixed-size chunks of memory, rather than laid out in a mutable linear memory. While this model is well suited for executing functional programs, the goal of this thesis is to see if conventional programs can be efficiently compiled for this novel memory system and parallelization model, focusing specifically on array-based linear algebra computations. We compile a subset of Java, targeting the Fresh Breeze instruction set. The compiler, using a static data-flow graph intermediate representation, performs analysis and transformations which reduce communication with the shared memory and identify opportunities for parallelization."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113295",
    "title": "Obstacle detection and tracking in an urban environment Using 3D LiDAR and a Mobileye 560",
    "abstract": "In order to navigate in an urban environment, a vehicle must be able to reliably detect and track dynamic obstacles such as vehicles, pedestrians, bicycles, and motorcycles. This paper presents a sensor fusion algorithm which combines tracking information from a Mobileye 560 and a Velodyne HDL-64E. The Velodyne tracking module first extracts obstacles by removing the ground plane points and then segmenting the remaining points using Euclidean Cluster Extraction. The Velodyne tracking module then uses the Kuhn-Munkres algorithm to associate Velodyne obstacles of the same type between time steps. The sensor fusion module associates and tracks obstacles from both the Velodyne and Mobileye tracking modules. It is able to reliably associate the same Velodyne and Mobileye obstacle between frames, although the Velodyne tracking module only provides robust tracking in simple scenes such as bridges.",
    "advisors": ["Sertac Karaman"],
    "text": "Obstacle detection and tracking in an urban environment Using 3D LiDAR and a Mobileye 560 In order to navigate in an urban environment, a vehicle must be able to reliably detect and track dynamic obstacles such as vehicles, pedestrians, bicycles, and motorcycles. This paper presents a sensor fusion algorithm which combines tracking information from a Mobileye 560 and a Velodyne HDL-64E. The Velodyne tracking module first extracts obstacles by removing the ground plane points and then segmenting the remaining points using Euclidean Cluster Extraction. The Velodyne tracking module then uses the Kuhn-Munkres algorithm to associate Velodyne obstacles of the same type between time steps. The sensor fusion module associates and tracks obstacles from both the Velodyne and Mobileye tracking modules. It is able to reliably associate the same Velodyne and Mobileye obstacle between frames, although the Velodyne tracking module only provides robust tracking in simple scenes such as bridges."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120421",
    "title": "Probabilistic latent variable modeling for predicting future well-being and assessing behavioral influences on mood, stress and health",
    "abstract": "In recent years, there has been a shift in the psychological research literature from an emphasis on dysfunction to a focus on well-being and positive mental health. As a result, enhancing well-being in individuals has become a viable approach to improving health, in addition to treating disorders when present. Also, the availability of rich multi-modal datasets and advances in machine learning methods have spurred an increase in research studies assessing well-being objectively. However, most of these studies tend to primarily focus on using data to estimate or detect the current state of well-being as opposed to the prediction of well-being. In addition, these studies investigate how stand-alone health behaviors and not a combination of health behaviors influence well-being. Furthermore, these studies do not provide data-backed insights and recommendations to individuals seeking to improve their well-being. In this dissertation, we use a real-world dataset from a population of college students and interpretable machine learning methods to (1) predict future mood, stress and health, (2) uncover how combinations of health behaviors work together to influence well-being, and (3) understand how to make evidence-based recommendations to individuals looking to improve their well-being. The use of these methods contributes to the development of objective techniques that can help individuals monitor their wellbeing. In addition, insights from this study contribute to knowledge advancement on how combinations of daily human behaviors can affect well-being.",
    "advisors": ["Rosalind W. Picard"],
    "text": "Probabilistic latent variable modeling for predicting future well-being and assessing behavioral influences on mood, stress and health In recent years, there has been a shift in the psychological research literature from an emphasis on dysfunction to a focus on well-being and positive mental health. As a result, enhancing well-being in individuals has become a viable approach to improving health, in addition to treating disorders when present. Also, the availability of rich multi-modal datasets and advances in machine learning methods have spurred an increase in research studies assessing well-being objectively. However, most of these studies tend to primarily focus on using data to estimate or detect the current state of well-being as opposed to the prediction of well-being. In addition, these studies investigate how stand-alone health behaviors and not a combination of health behaviors influence well-being. Furthermore, these studies do not provide data-backed insights and recommendations to individuals seeking to improve their well-being. In this dissertation, we use a real-world dataset from a population of college students and interpretable machine learning methods to (1) predict future mood, stress and health, (2) uncover how combinations of health behaviors work together to influence well-being, and (3) understand how to make evidence-based recommendations to individuals looking to improve their well-being. The use of these methods contributes to the development of objective techniques that can help individuals monitor their wellbeing. In addition, insights from this study contribute to knowledge advancement on how combinations of daily human behaviors can affect well-being."
}, {
    "id": "oai:dspace.mit.edu:1721.1/55085",
    "title": "Analysis and transfer of photographic viewpoint and appearance",
    "abstract": "To make a compelling photograph, photographers need to carefully choose the subject and composition of a picture, to select the right lens and viewpoint, and to make great efforts with lighting and post-processing to arrange the tones and contrast. Unfortunately, such painstaking work and advanced skill is out of reach for casual photographers. In addition, for professional photographers, it is important to improve workflow efficiency. The goal of our work is to allow users to achieve a faithful viewpoint for rephotography and a particular appearance with ease and speed. To this end, we analyze and transfer properties of a model photo to a new photo. In particular, we transfer the viewpoint of a reference photo to enable rephotography. In addition, we transfer photographic appearance from a model photo to a new input photo. In this thesis,we present two contributions that transfer photographic view and look using model photographs and one contribution that magnifies existing defocus given a single photo. First, we address the challenge of viewpoint matching for rephotography. Our interactive, computer-vision-based technique helps users match the viewpoint of a reference photograph at capture time. Next, we focus on the tonal aspects of photographic look using post-processing. Users just need to provide a pair of photos, an input and a model, and our technique automatically transfers the look from the model to the input. Finally, we magnify defocus given a single image. We analyze the existing defocus in the input image and increase the amount of defocus present in out-of focus regions.",
    "advisors": ["Frédo Durand"],
    "text": "Analysis and transfer of photographic viewpoint and appearance To make a compelling photograph, photographers need to carefully choose the subject and composition of a picture, to select the right lens and viewpoint, and to make great efforts with lighting and post-processing to arrange the tones and contrast. Unfortunately, such painstaking work and advanced skill is out of reach for casual photographers. In addition, for professional photographers, it is important to improve workflow efficiency. The goal of our work is to allow users to achieve a faithful viewpoint for rephotography and a particular appearance with ease and speed. To this end, we analyze and transfer properties of a model photo to a new photo. In particular, we transfer the viewpoint of a reference photo to enable rephotography. In addition, we transfer photographic appearance from a model photo to a new input photo. In this thesis,we present two contributions that transfer photographic view and look using model photographs and one contribution that magnifies existing defocus given a single photo. First, we address the challenge of viewpoint matching for rephotography. Our interactive, computer-vision-based technique helps users match the viewpoint of a reference photograph at capture time. Next, we focus on the tonal aspects of photographic look using post-processing. Users just need to provide a pair of photos, an input and a model, and our technique automatically transfers the look from the model to the input. Finally, we magnify defocus given a single image. We analyze the existing defocus in the input image and increase the amount of defocus present in out-of focus regions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44709",
    "title": "Development of multimodal spectroscopy for the detection of vulnerable atherosclerotic plaques",
    "abstract": "The combination of reflectance, fluorescence, and Raman spectroscopy - which is termed multimodal spectroscopy (MMS) - provides complementary and depth-sensitive information about tissue composition. As such, MMS can provide biochemical and morphological information useful in detecting vulnerable atherosclerotic plaques, that is, plaques most prone to rupture and causing sudden death. Early detection of these vulnerable plaques is critical to reducing patient mortality associated with cardiovascular disease. In developing MMS into a clinical diagnostic modality, several scientific and engineering directions are explored in this work: the physical motivation for MMS, the framework of quantitative extraction of spectral parameters, the spectral probes that enable the efficient collection of data, a clinical instrument able to provide real-time diagnosis, and, finally, a clinical implementation of the entire methodology. The motivation for MMS is shown through a pilot in vitro study using carotid artery specimens, which shows the promise for MMS to detect features of vulnerable plaque. Having established the motivation, the next step describes the mathematical tools used to extract quantitative spectral parameters and, moreover, to assess the uncertainty and confidence of the spectral information. In order to implement MMS, the development of an efficient, specialized MMS probe for data acquisition and a compact and practical clinical MMS instrument are described. Lastly, in vivo and ex vivo results from a relatively large clinical study of vulnerable plaque in humans show excellent agreement between MMS and histopathology. Specifically, MMS is shown to have the ability to detect a thin fibrous cap, necrotic core or superficial foam cells, and thrombus.",
    "advisors": ["Michael S. Feld"],
    "text": "Development of multimodal spectroscopy for the detection of vulnerable atherosclerotic plaques The combination of reflectance, fluorescence, and Raman spectroscopy - which is termed multimodal spectroscopy (MMS) - provides complementary and depth-sensitive information about tissue composition. As such, MMS can provide biochemical and morphological information useful in detecting vulnerable atherosclerotic plaques, that is, plaques most prone to rupture and causing sudden death. Early detection of these vulnerable plaques is critical to reducing patient mortality associated with cardiovascular disease. In developing MMS into a clinical diagnostic modality, several scientific and engineering directions are explored in this work: the physical motivation for MMS, the framework of quantitative extraction of spectral parameters, the spectral probes that enable the efficient collection of data, a clinical instrument able to provide real-time diagnosis, and, finally, a clinical implementation of the entire methodology. The motivation for MMS is shown through a pilot in vitro study using carotid artery specimens, which shows the promise for MMS to detect features of vulnerable plaque. Having established the motivation, the next step describes the mathematical tools used to extract quantitative spectral parameters and, moreover, to assess the uncertainty and confidence of the spectral information. In order to implement MMS, the development of an efficient, specialized MMS probe for data acquisition and a compact and practical clinical MMS instrument are described. Lastly, in vivo and ex vivo results from a relatively large clinical study of vulnerable plaque in humans show excellent agreement between MMS and histopathology. Specifically, MMS is shown to have the ability to detect a thin fibrous cap, necrotic core or superficial foam cells, and thrombus."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39732",
    "title": "Verification of d-wave pairing symmetry by microwave intermodulation distortion measurements in yttrium barium copper oxide",
    "abstract": "We report measurements of the temperature and power dependence of the microwave frequency intermodulation distortion (IMD) in high quality pulsed laser deposition (PLD) Yttrium Barium Copper Oxide (YBCO) on LaAlO3 substrate. A low-temperature (T < 30 K) increase in IMD is the observation of an upturn of the nonlinear coefficient of the quadratic field dependence of the penetration depth. This IMD upturn is limited by the nonlinear Meissner effect that has been predicted for d-wave high-T, superconductors. Various amounts of IMD increase are observed for different films with impurity (Ni, Zn and Ca) doping and other defects. The demonstration of the IMD upturn and the nonlinear Meissner effect were possible because the IMD measurement is an extremely sensitive method to detect the penetration depth change at even less than 0.01 nm. IMDs from various samples tend to merge at a single universal value at 0 K regardless of disorder, defects, and impurities due to the node singularity at 0 K. There is a similar converging trend in IMD towards the transition temperature T, due to the quasiparticle thermal excitation and depletion of superelectrons. It is most likely that IMD has both intrinsic and extrinsic contributions.",
    "advisors": ["Daniel E. Oates", "Terry P. Orlando"],
    "text": "Verification of d-wave pairing symmetry by microwave intermodulation distortion measurements in yttrium barium copper oxide We report measurements of the temperature and power dependence of the microwave frequency intermodulation distortion (IMD) in high quality pulsed laser deposition (PLD) Yttrium Barium Copper Oxide (YBCO) on LaAlO3 substrate. A low-temperature (T < 30 K) increase in IMD is the observation of an upturn of the nonlinear coefficient of the quadratic field dependence of the penetration depth. This IMD upturn is limited by the nonlinear Meissner effect that has been predicted for d-wave high-T, superconductors. Various amounts of IMD increase are observed for different films with impurity (Ni, Zn and Ca) doping and other defects. The demonstration of the IMD upturn and the nonlinear Meissner effect were possible because the IMD measurement is an extremely sensitive method to detect the penetration depth change at even less than 0.01 nm. IMDs from various samples tend to merge at a single universal value at 0 K regardless of disorder, defects, and impurities due to the node singularity at 0 K. There is a similar converging trend in IMD towards the transition temperature T, due to the quasiparticle thermal excitation and depletion of superelectrons. It is most likely that IMD has both intrinsic and extrinsic contributions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/17963",
    "title": "Diffractive optics for maskless lithography and imaging",
    "abstract": "Semiconductor industry has primarily been driven by the capability of lithography to pattern smaller and smaller features. However due to increasing mask costs and complexity, and increasing tool costs, the state-of-the-art technology in lithography is accessible only to a select few. Zone-plate array lithography (ZPAL) is a novel method of maskless lithography that aims to alleviate some of these issues while offering a solution that can be extended to the limits of nanolithography. In ZPAL, an array of diffractive lenses is used to form an array of spots on the substrate. Each spot is modulated independently by means of spatial-light modulators. This essentially creates a \"parallel laserwriter\". In addition, this lithography system can be converted into a parallel-confocal microscope, which enables fast, high-resolution imaging. This thesis addresses the performance of diffractive lenses, particularly high-numerical aperture zone plates for lithography and imaging using a combination of experimental and theoretical studies. A novel proximity-effect correction algorithm that was implemented effectively in a ZPAL system is also described. Variations to another diffractive lens known as the photon sieve are proposed. The first ever lithography results performed using these new elements are presented in this thesis.",
    "advisors": ["Henry I. Smith"],
    "text": "Diffractive optics for maskless lithography and imaging Semiconductor industry has primarily been driven by the capability of lithography to pattern smaller and smaller features. However due to increasing mask costs and complexity, and increasing tool costs, the state-of-the-art technology in lithography is accessible only to a select few. Zone-plate array lithography (ZPAL) is a novel method of maskless lithography that aims to alleviate some of these issues while offering a solution that can be extended to the limits of nanolithography. In ZPAL, an array of diffractive lenses is used to form an array of spots on the substrate. Each spot is modulated independently by means of spatial-light modulators. This essentially creates a \"parallel laserwriter\". In addition, this lithography system can be converted into a parallel-confocal microscope, which enables fast, high-resolution imaging. This thesis addresses the performance of diffractive lenses, particularly high-numerical aperture zone plates for lithography and imaging using a combination of experimental and theoretical studies. A novel proximity-effect correction algorithm that was implemented effectively in a ZPAL system is also described. Variations to another diffractive lens known as the photon sieve are proposed. The first ever lithography results performed using these new elements are presented in this thesis."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107289",
    "title": "Tradeoffs of the use of SiGe buffer layers in tandem GaAsP/Si solar cells",
    "abstract": "III-V multi-junction solar cells currently have the highest reported theoretical and experimental energy conversion efficiency but their cost, mainly attributed to the use of expensive substrates, limits their widespread use for terrestrial applications. Successful integration of III--V's on a Si substrate to enable a III-V/Si tandem cell can lower the cost of energy by combining the high-efficiency of the III--V materials with the low-cost and abundance of the Si substrate. A maximum theoretical efficiency of 44.8% from a tandem cell on Si can be achieved by using a GaAsP (Eg=1.7 eV) as the top cell. Out of several possible integration routes, the use of a linearly graded SiGe buffer as interfacial layer between the two cells potentially yields the highest quality for the epitaxial GaAsP layer, an essential requirement for realization of high-efficiency solar cells. In this thesis, the impact of the SiGe buffer layer on the optical and electrical characteristics of the bottom Si cell of a GaAsP/Si tandem solar cell was assessed via experimental work. The growth of a SiGe buffer layer was shown to increase the threading dislocation density and as a result the leakage current of the bottom Si cell by about 10x. In addition, the low-bandgap SiGe absorbs more than 80% of the light that is intended for the Si sub-cell, reducing the short-circuit current of the Si cell from 33 mA/cm² to only 6 mA/cm². By using a step-cell design, in which the SiGe was partially etched to allow more light to reach the bottom cell, the current was increased to 20 mA/cm². To quantify the merits of the studied approach as well as evaluate other approaches, we have carried out a theoretical study of absorbed irradiance in a Si single-junction cell, a bonded GaAsP/Si tandem cell, a GaAsP/SiGe/Si tandem cell as well as the step-cell design. The GaAsP/Si bonded tandem cell showed 24% relative improvement in light absorption over a single-junction Si cell. The addition of a SiGe graded buffer was shown to reduce the total absorption by 25%, bringing the efficiency of GaAsP/SiGe/Si tandem cell under that of the Si single-junction cell. The step-cell design, even though successful in increasing light absorption, was not found effective in achieving a higher absorbed power density than that of the Si cell. These results suggest that any future work on integrating GaAsP cells on Si towards a high-performance tandem cell should be focused on using a higher-bandgap material as a graded buffer or using a wafer bonding technique.",
    "advisors": ["Judy L. Hoyt", "Jesús A. del Alamo"],
    "text": "Tradeoffs of the use of SiGe buffer layers in tandem GaAsP/Si solar cells III-V multi-junction solar cells currently have the highest reported theoretical and experimental energy conversion efficiency but their cost, mainly attributed to the use of expensive substrates, limits their widespread use for terrestrial applications. Successful integration of III--V's on a Si substrate to enable a III-V/Si tandem cell can lower the cost of energy by combining the high-efficiency of the III--V materials with the low-cost and abundance of the Si substrate. A maximum theoretical efficiency of 44.8% from a tandem cell on Si can be achieved by using a GaAsP (Eg=1.7 eV) as the top cell. Out of several possible integration routes, the use of a linearly graded SiGe buffer as interfacial layer between the two cells potentially yields the highest quality for the epitaxial GaAsP layer, an essential requirement for realization of high-efficiency solar cells. In this thesis, the impact of the SiGe buffer layer on the optical and electrical characteristics of the bottom Si cell of a GaAsP/Si tandem solar cell was assessed via experimental work. The growth of a SiGe buffer layer was shown to increase the threading dislocation density and as a result the leakage current of the bottom Si cell by about 10x. In addition, the low-bandgap SiGe absorbs more than 80% of the light that is intended for the Si sub-cell, reducing the short-circuit current of the Si cell from 33 mA/cm² to only 6 mA/cm². By using a step-cell design, in which the SiGe was partially etched to allow more light to reach the bottom cell, the current was increased to 20 mA/cm². To quantify the merits of the studied approach as well as evaluate other approaches, we have carried out a theoretical study of absorbed irradiance in a Si single-junction cell, a bonded GaAsP/Si tandem cell, a GaAsP/SiGe/Si tandem cell as well as the step-cell design. The GaAsP/Si bonded tandem cell showed 24% relative improvement in light absorption over a single-junction Si cell. The addition of a SiGe graded buffer was shown to reduce the total absorption by 25%, bringing the efficiency of GaAsP/SiGe/Si tandem cell under that of the Si single-junction cell. The step-cell design, even though successful in increasing light absorption, was not found effective in achieving a higher absorbed power density than that of the Si cell. These results suggest that any future work on integrating GaAsP cells on Si towards a high-performance tandem cell should be focused on using a higher-bandgap material as a graded buffer or using a wafer bonding technique."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42056",
    "title": "Learning by learning to communicate",
    "abstract": "Human intelligence is a product of cooperation among many different specialists. Much of this cooperation must be learned, but we do not yet have a mechanism that explains how this might happen for the \"high-level\" agile cooperation that permeates our daily lives. I propose that the various specialists learn to cooperate by learning to communicate, basing this proposal on the phenomenon of communication bootstrapping, in which shared experiences form a basis for agreement on a system of signals. In this dissertation, I lay out a roadmap for investigating this hypothesis, identifying problems that must be overcome in order to understand the capabilities of communication bootstrapping and in order to test whether it is exploited by human intelligence. I then demonstrate progress along the course of investigation laid out in my roadmap: * I establish a measure of developmental cost that allows me to eliminate many possible designs * I develop a method of engineering devices for use in models of intelligence, including characterizing their behavior under a wide variety of conditions and compensating for their misbehavior using failure simplification. * I develop mechanisms that reliably produce communication bootstrapping such that it can be used to connect specialists in an engineered system. * I construct a demonstration system including a simulated world and pair of observers that learn world dynamics via communication bootstrapping.",
    "advisors": ["Gerald Jay Sussman"],
    "text": "Learning by learning to communicate Human intelligence is a product of cooperation among many different specialists. Much of this cooperation must be learned, but we do not yet have a mechanism that explains how this might happen for the \"high-level\" agile cooperation that permeates our daily lives. I propose that the various specialists learn to cooperate by learning to communicate, basing this proposal on the phenomenon of communication bootstrapping, in which shared experiences form a basis for agreement on a system of signals. In this dissertation, I lay out a roadmap for investigating this hypothesis, identifying problems that must be overcome in order to understand the capabilities of communication bootstrapping and in order to test whether it is exploited by human intelligence. I then demonstrate progress along the course of investigation laid out in my roadmap: * I establish a measure of developmental cost that allows me to eliminate many possible designs * I develop a method of engineering devices for use in models of intelligence, including characterizing their behavior under a wide variety of conditions and compensating for their misbehavior using failure simplification. * I develop mechanisms that reliably produce communication bootstrapping such that it can be used to connect specialists in an engineered system. * I construct a demonstration system including a simulated world and pair of observers that learn world dynamics via communication bootstrapping."
}, {
    "id": "oai:dspace.mit.edu:1721.1/97800",
    "title": "Micro-optic elements for a compact opto-electronic integrated neural coprocessor",
    "abstract": "The research done for this thesis was aimed at developing the optical elements needed for the Compact Opto-electronic Integrated Neural coprocessor (COIN coprocessor) project. The COIN coprocessor is an implementation of a feed forward neural network using free-space optical interconnects to communicate between neurons. Prior work on this project had assumed these interconnects would be formed using Holographic Optical Elements (HOEs), so early work for this thesis was directed along these lines. Important limits to the use of HOEs in the COIN system were identified and evaluated. In particular, the problem of changing wavelength between the hologram recording and readout steps was examined and it was shown that there is no general solution to this problem when the hologram to be recorded is constructed with more than two plane waves interfering with each other. Two experimental techniques, the holographic bead lens and holographic liftoff, were developed as partial workarounds to the identified limitations. As an alternative to HOEs, an optical element based on the concept of the Fresnel Zone Plate was developed and experimentally tested. The zone plate based elements offer an easily scalable method for fabricating the COIN optical interconnects using standard lithographic processes and appear to be the best choice for the COIN coprocessor project at this time. In addition to the development of the optical elements for the COIN coprocessor, this thesis also looks at the impact of optical element efficiency on the power consumption of the COIN coprocessor. Finally, a model of the COIN network based on the current COIN design was used to compare the performance and cost of the COIN system with competing implementations of neural networks, with the conclusion that at this time the proposed COIN coprocessor system is still a competitive option for neural network implementations.",
    "advisors": ["Cardinal Warde"],
    "text": "Micro-optic elements for a compact opto-electronic integrated neural coprocessor The research done for this thesis was aimed at developing the optical elements needed for the Compact Opto-electronic Integrated Neural coprocessor (COIN coprocessor) project. The COIN coprocessor is an implementation of a feed forward neural network using free-space optical interconnects to communicate between neurons. Prior work on this project had assumed these interconnects would be formed using Holographic Optical Elements (HOEs), so early work for this thesis was directed along these lines. Important limits to the use of HOEs in the COIN system were identified and evaluated. In particular, the problem of changing wavelength between the hologram recording and readout steps was examined and it was shown that there is no general solution to this problem when the hologram to be recorded is constructed with more than two plane waves interfering with each other. Two experimental techniques, the holographic bead lens and holographic liftoff, were developed as partial workarounds to the identified limitations. As an alternative to HOEs, an optical element based on the concept of the Fresnel Zone Plate was developed and experimentally tested. The zone plate based elements offer an easily scalable method for fabricating the COIN optical interconnects using standard lithographic processes and appear to be the best choice for the COIN coprocessor project at this time. In addition to the development of the optical elements for the COIN coprocessor, this thesis also looks at the impact of optical element efficiency on the power consumption of the COIN coprocessor. Finally, a model of the COIN network based on the current COIN design was used to compare the performance and cost of the COIN system with competing implementations of neural networks, with the conclusion that at this time the proposed COIN coprocessor system is still a competitive option for neural network implementations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44411",
    "title": "Implementation and modeling of a scheduled Optical Flow Switching (OFS) network",
    "abstract": "In this thesis we present analysis of Optical Flow Switching (OFS), an architectural approach for enabling all-optical user to user connections for transmission of Internet traffic. We first describe a demonstration of OFS on the ONRAMP test environment which is a MAN optical network implemented in hardware in the Boston geographic area. This demonstration shows the viability of OFS in an actual implementation, with good performance results and an assessment over OFS overheads. Then, we use stochastic models to quantify the behavior of an OFS network. Strong quantitative evidence leads us to draw the conclusion that scheduling is a necessary component of any architectural approach to implementing OFS in a Metro Area network (MAN).",
    "advisors": ["Vincent Chan"],
    "text": "Implementation and modeling of a scheduled Optical Flow Switching (OFS) network In this thesis we present analysis of Optical Flow Switching (OFS), an architectural approach for enabling all-optical user to user connections for transmission of Internet traffic. We first describe a demonstration of OFS on the ONRAMP test environment which is a MAN optical network implemented in hardware in the Boston geographic area. This demonstration shows the viability of OFS in an actual implementation, with good performance results and an assessment over OFS overheads. Then, we use stochastic models to quantify the behavior of an OFS network. Strong quantitative evidence leads us to draw the conclusion that scheduling is a necessary component of any architectural approach to implementing OFS in a Metro Area network (MAN)."
}, {
    "id": "oai:dspace.mit.edu:1721.1/78547",
    "title": "A theoretical analysis of interstitial hydrogen : pressure-composition-temperature, chemical potential, enthalpy and entropy",
    "abstract": "We provide a first principles analysis of the physics and thermodynamics of interstitial hydrogen in metal. By utilizing recent advances in Density Functional Theory (DFT) to get state energies of the metal-hydrogen system, we are able to model the absorption process fairly accurately. A connection to experiment is made via Pressure-Composition-Temperature (PCT) isotherms, and thermodynamic molar quantities. In the model, we understand the excess entropy of absorbed hydrogen in terms of the change in its accessible microstates. A connection is also made between the entropy and electronic states of interstitial hydrogen. However, our model indicates that this connection is too small to account for experimental results. Therefore, a conclusion is made that the entropy of absorbed hydrogen is mostly (non-ideal) configurational in nature. To model the latter in a manner consistent with experiment, we have explored a new model that posits a weak binding between clusters of hydrogen atoms at neighboring sites. We have developed a formulation and fitted the results to experimental data. We find a least squares fitting of the model to the entropy and enthalpy results in model parameters which seem physically reasonable. The resulting model appears to provide a natural physical explanation for the dependence of the excess entropy on loading.",
    "advisors": ["Peter L. Hagelstein"],
    "text": "A theoretical analysis of interstitial hydrogen : pressure-composition-temperature, chemical potential, enthalpy and entropy We provide a first principles analysis of the physics and thermodynamics of interstitial hydrogen in metal. By utilizing recent advances in Density Functional Theory (DFT) to get state energies of the metal-hydrogen system, we are able to model the absorption process fairly accurately. A connection to experiment is made via Pressure-Composition-Temperature (PCT) isotherms, and thermodynamic molar quantities. In the model, we understand the excess entropy of absorbed hydrogen in terms of the change in its accessible microstates. A connection is also made between the entropy and electronic states of interstitial hydrogen. However, our model indicates that this connection is too small to account for experimental results. Therefore, a conclusion is made that the entropy of absorbed hydrogen is mostly (non-ideal) configurational in nature. To model the latter in a manner consistent with experiment, we have explored a new model that posits a weak binding between clusters of hydrogen atoms at neighboring sites. We have developed a formulation and fitted the results to experimental data. We find a least squares fitting of the model to the entropy and enthalpy results in model parameters which seem physically reasonable. The resulting model appears to provide a natural physical explanation for the dependence of the excess entropy on loading."
}, {
    "id": "oai:dspace.mit.edu:1721.1/121429",
    "title": "High field terahertz radiation : conduits to synchronized hyper spectral systems",
    "abstract": "After the first experiments of nonlinear phenomena in optics, the development of the mode-locked laser has led to rapid proliferation of the study of nonlinear frequency conversion techniques-enabling the conversion of light from the infra-red, all the way to the soft X-Ray region. However, accessing the hard X-ray region remains elusive, and the domain of specialized facilities. The key insight to accessing hard X-rays optically may not be in seeking to convert optical frequencies upward, but rather downward to frequencies spanning a 100 to 10,000 GHz. This would enable unprecedented control of electrons and consequently the generation of hard X-rays. The efficient optical conversion to terahertz radiation would thus open up the possibility of highly synchronized multi-spectral systems to transform the landscape of scientific investigation and medicine among others. In this thesis, the problem of efficient conversion is tackled theoretically. A montage of novel computational techniques, analyses, device proposals and physical mechanisms culminate in record breaking experimental demonstrations with efficiencies, an order of magnitude larger than prior art. The thesis further paves the way for even greater improvements, by another order of magnitude. The underlying science of cascaded difference frequency generation, expounded here would be of significant value to terahertz generation in chip-scale systems for future applications such as Quantum computing, chip-scale accelerators and X-ray sources.",
    "advisors": ["Franz X. Kärtner", "Erich P. Ippen"],
    "text": "High field terahertz radiation : conduits to synchronized hyper spectral systems After the first experiments of nonlinear phenomena in optics, the development of the mode-locked laser has led to rapid proliferation of the study of nonlinear frequency conversion techniques-enabling the conversion of light from the infra-red, all the way to the soft X-Ray region. However, accessing the hard X-ray region remains elusive, and the domain of specialized facilities. The key insight to accessing hard X-rays optically may not be in seeking to convert optical frequencies upward, but rather downward to frequencies spanning a 100 to 10,000 GHz. This would enable unprecedented control of electrons and consequently the generation of hard X-rays. The efficient optical conversion to terahertz radiation would thus open up the possibility of highly synchronized multi-spectral systems to transform the landscape of scientific investigation and medicine among others. In this thesis, the problem of efficient conversion is tackled theoretically. A montage of novel computational techniques, analyses, device proposals and physical mechanisms culminate in record breaking experimental demonstrations with efficiencies, an order of magnitude larger than prior art. The thesis further paves the way for even greater improvements, by another order of magnitude. The underlying science of cascaded difference frequency generation, expounded here would be of significant value to terahertz generation in chip-scale systems for future applications such as Quantum computing, chip-scale accelerators and X-ray sources."
}, {
    "id": "oai:dspace.mit.edu:1721.1/93058",
    "title": "Structural and functional imaging of the human and small animal eyes using ultrahigh speed Fourier domain optical coherence tomography",
    "abstract": "Optical coherence tomography (OCT) is a non-invasive optical imaging technique that allows the three-dimensional structure of biological tissue to be visualized with micrometer resolution. In ophthalmology OCT has the unique advantage that it provides cross-sectional images of the retina and choroid noninvasively and in vivo, which have led OCT to be a clinical standard for the diagnosis of a variety of retinal diseases. Although current commercial Fourier domain OCT systems have high imaging speeds of 20-100kHz A-scan rates, these imaging speeds are not sufficient for more advanced structural and functional imaging techniques. Current state-of-the-art spectral domain and swept source OCT provide ultrahigh imaging speeds of >200kHz A-scan rates. These speeds enable functional imaging of retinal blood flow, OCT angiography of the retinal and choroidal microvasculature, and wide field volumetric structural imaging of the retina and choroid. In this thesis, advances in structural and functional ophthalmic imaging techniques for the human and small animal eyes are investigated using ultrahigh speed Fourier domain OCT. The following topics are discussed: (1) a method for numerically extracting and compensating dispersion mismatch in ultrahigh resolution spectral domain OCT, (2) ultrahigh speed spectral domain imaging in the small animal eye for measuring total retinal blood flow, (3) development of ultrahigh speed phase stable swept source OCT system for human retinal imaging, (4) OCT angiography of the choriocapillaris in the human eye, (5) clinical applications of OCT angiography in retinal diseases, including diabetic retinopathy and age-related macular degeneration, (6) small animal anesthesia protocol for functional hemodynamic imaging, and (7) imaging of neurovascular coupling in small animals using ultrahigh speed OCT.",
    "advisors": ["James G. Fujimoto"],
    "text": "Structural and functional imaging of the human and small animal eyes using ultrahigh speed Fourier domain optical coherence tomography Optical coherence tomography (OCT) is a non-invasive optical imaging technique that allows the three-dimensional structure of biological tissue to be visualized with micrometer resolution. In ophthalmology OCT has the unique advantage that it provides cross-sectional images of the retina and choroid noninvasively and in vivo, which have led OCT to be a clinical standard for the diagnosis of a variety of retinal diseases. Although current commercial Fourier domain OCT systems have high imaging speeds of 20-100kHz A-scan rates, these imaging speeds are not sufficient for more advanced structural and functional imaging techniques. Current state-of-the-art spectral domain and swept source OCT provide ultrahigh imaging speeds of >200kHz A-scan rates. These speeds enable functional imaging of retinal blood flow, OCT angiography of the retinal and choroidal microvasculature, and wide field volumetric structural imaging of the retina and choroid. In this thesis, advances in structural and functional ophthalmic imaging techniques for the human and small animal eyes are investigated using ultrahigh speed Fourier domain OCT. The following topics are discussed: (1) a method for numerically extracting and compensating dispersion mismatch in ultrahigh resolution spectral domain OCT, (2) ultrahigh speed spectral domain imaging in the small animal eye for measuring total retinal blood flow, (3) development of ultrahigh speed phase stable swept source OCT system for human retinal imaging, (4) OCT angiography of the choriocapillaris in the human eye, (5) clinical applications of OCT angiography in retinal diseases, including diabetic retinopathy and age-related macular degeneration, (6) small animal anesthesia protocol for functional hemodynamic imaging, and (7) imaging of neurovascular coupling in small animals using ultrahigh speed OCT."
}, {
    "id": "oai:dspace.mit.edu:1721.1/108849",
    "title": "Integrated optical quantum manipulation and measurement of trapped ions",
    "abstract": "Individual atomic ions confined in designed electromagnetic potentials and manipulated via lasers are strong candidates as physical bases for quantum information processing (QIP). This is in large part due to their long coherence times, in distinguishability, and strong Coulomb interactions. Much work in recent years has utilized these properties to implement increasingly precise quantum operations essential for QIP, as well as to conduct increasingly sophisticated experiments on few-ion systems. Many questions remain however regarding how to implement the significant classical apparatus required to control and measure many ions (and indeed any physical qubit under study) in a scalable way that furthermore does not compromise qubit quality. This work draws on techniques in integrated optics to address this question. Planar-fabricated waveguides and gratings integrated with planar ion traps are demonstrated to allow optical addressing of individual 88Sr+ions 50 [mu]m above the chip surface with distraction-limited focused beams, with advantages in stability and scalability. Motivated by the requirement for low crosstalk in qubit addressing, we show also that intuitively designed devices can generate precisely tailored intensity profiles at the ion locations, with distraction-limited side lobe intensities characterized to the 5x10-6 level in relative intensity up to 25 [mu]m from the focus. Such devices can be implemented alongside complex systems in complementary metal-oxide-semiconductor (CMOS) processes. We show in addition that the multiple patternable metal layers present in CMOS processes can be used to create complex planar ion traps with performance comparable to simple single-layer traps, and that CMOS silicon avalanche photodiodes may be employed for scalable quantum state readout. Finally we show initial results on integrated electro-optic modulators for visible light. These results open possibilities for experiments with trapped ions in the short term, and indicate routes to achieving large-scale systems of thousands or more ions in the future. Though ion qubits may seem isolated from scalable solid-state technologies, it appears this apparent isolation may uniquely allow a cooperation with complex planar-fabricated optical and electronic systems without introducing additional decoherence.",
    "advisors": ["Rajeev J. Ram"],
    "text": "Integrated optical quantum manipulation and measurement of trapped ions Individual atomic ions confined in designed electromagnetic potentials and manipulated via lasers are strong candidates as physical bases for quantum information processing (QIP). This is in large part due to their long coherence times, in distinguishability, and strong Coulomb interactions. Much work in recent years has utilized these properties to implement increasingly precise quantum operations essential for QIP, as well as to conduct increasingly sophisticated experiments on few-ion systems. Many questions remain however regarding how to implement the significant classical apparatus required to control and measure many ions (and indeed any physical qubit under study) in a scalable way that furthermore does not compromise qubit quality. This work draws on techniques in integrated optics to address this question. Planar-fabricated waveguides and gratings integrated with planar ion traps are demonstrated to allow optical addressing of individual 88Sr+ions 50 [mu]m above the chip surface with distraction-limited focused beams, with advantages in stability and scalability. Motivated by the requirement for low crosstalk in qubit addressing, we show also that intuitively designed devices can generate precisely tailored intensity profiles at the ion locations, with distraction-limited side lobe intensities characterized to the 5x10-6 level in relative intensity up to 25 [mu]m from the focus. Such devices can be implemented alongside complex systems in complementary metal-oxide-semiconductor (CMOS) processes. We show in addition that the multiple patternable metal layers present in CMOS processes can be used to create complex planar ion traps with performance comparable to simple single-layer traps, and that CMOS silicon avalanche photodiodes may be employed for scalable quantum state readout. Finally we show initial results on integrated electro-optic modulators for visible light. These results open possibilities for experiments with trapped ions in the short term, and indicate routes to achieving large-scale systems of thousands or more ions in the future. Though ion qubits may seem isolated from scalable solid-state technologies, it appears this apparent isolation may uniquely allow a cooperation with complex planar-fabricated optical and electronic systems without introducing additional decoherence."
}, {
    "id": "oai:dspace.mit.edu:1721.1/111878",
    "title": "Sensing and timekeeping using a light-trapping diamond waveguide",
    "abstract": "Solid-state quantum systems have emerged as promising sensing platforms. In particular, the spin properties of nitrogen vacancy (NV) color centers in diamond make them outstanding sensors of magnetic fields, electric fields, and temperature under ambient conditions. This thesis focuses on spin-based sensing using multimode diamond waveguide structures to efficiently use large ensembles of NV centers (> 10¹⁰). Temperature-stabilized precision magnetometry, thermometry, and electrometry are discussed. In addition, the precision characterization of the NV ground state structure under a transverse magnetic field and the use of NV-diamond for spin-based clocks are reported.",
    "advisors": ["Dirk Englund"],
    "text": "Sensing and timekeeping using a light-trapping diamond waveguide Solid-state quantum systems have emerged as promising sensing platforms. In particular, the spin properties of nitrogen vacancy (NV) color centers in diamond make them outstanding sensors of magnetic fields, electric fields, and temperature under ambient conditions. This thesis focuses on spin-based sensing using multimode diamond waveguide structures to efficiently use large ensembles of NV centers (> 10¹⁰). Temperature-stabilized precision magnetometry, thermometry, and electrometry are discussed. In addition, the precision characterization of the NV ground state structure under a transverse magnetic field and the use of NV-diamond for spin-based clocks are reported."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8012",
    "title": "Minimum description complexity",
    "abstract": "The classical problem of model selection among parametric model sets is considered. The goal is to choose a model set which best represents observed data. The critical task is the choice of a criterion for model set comparison. Pioneer information theoretic based approaches to this problem are Akaike information criterion (AIC) and different forms of minimum description length (MDL). The prior assumption in these methods is that the unknown true model is a member of all the competing sets. We introduce a new method of model selection: minimum description complexity (MDC). The approach is motivated by the Kullback-Leibler information distance. The method suggests choosing the model set for which the model set relative entropy is minimum. We provide a probabilistic method of MDC estimation for a class of parametric model sets. In this calculation the key factor is our prior assumption: unlike the existing methods, no assumption of the true model being a member of the competing model sets is needed. The main strength of the MDC calculation is in its method of extracting information from the observed data.",
    "advisors": ["Munther A. Dahleh"],
    "text": "Minimum description complexity The classical problem of model selection among parametric model sets is considered. The goal is to choose a model set which best represents observed data. The critical task is the choice of a criterion for model set comparison. Pioneer information theoretic based approaches to this problem are Akaike information criterion (AIC) and different forms of minimum description length (MDL). The prior assumption in these methods is that the unknown true model is a member of all the competing sets. We introduce a new method of model selection: minimum description complexity (MDC). The approach is motivated by the Kullback-Leibler information distance. The method suggests choosing the model set for which the model set relative entropy is minimum. We provide a probabilistic method of MDC estimation for a class of parametric model sets. In this calculation the key factor is our prior assumption: unlike the existing methods, no assumption of the true model being a member of the competing model sets is needed. The main strength of the MDC calculation is in its method of extracting information from the observed data."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28709",
    "title": "Information theoretic aspects of the control and the mode estimation of stochastic systems",
    "abstract": "(cont.) parallel with a communication paradigm and deriving an analysis of performance. In our approach, the switching system is viewed as an encoder of the mode, which is interpreted as the message, while a probing signal establishes a random code. Using a distortion function, we define an uncertainty ball where the estimates are guaranteed to lie with probability arbitrarily close to 1. The radius of the uncertainty ball is directly related to the entropy rate of the switching process.",
    "advisors": ["Munther A. Daleh"],
    "text": "Information theoretic aspects of the control and the mode estimation of stochastic systems (cont.) parallel with a communication paradigm and deriving an analysis of performance. In our approach, the switching system is viewed as an encoder of the mode, which is interpreted as the message, while a probing signal establishes a random code. Using a distortion function, we define an uncertainty ball where the estimates are guaranteed to lie with probability arbitrarily close to 1. The radius of the uncertainty ball is directly related to the entropy rate of the switching process."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117843",
    "title": "A proximal atomic coordination algorithm for distributed optimization in distribution grids",
    "abstract": "The control and regulation of power grids has historically relied upon large-scale scheduleable generation and relatively stable load demand profiles. With the advent of extensive local renewable energy generation technologies as well as the incorporation of load responsive demand response (DR) methodologies, it has become imperative that new distributed control strategies are developed to better regulate the increasingly volatile nature of modern generation and load profiles. In this thesis, we introduce a distributed control strategy called Proximal Atomic Coordination (PAC) to solve for optimal control strategies in distributed power grids, a problem called Optimal Power Flow (OPF). Using a convex relaxed variant of OPF, we show that PAC exhibits sub-linear convergence to the optimal ergodic cost, and linear convergence to the OPF solution. We demonstrate our results on various power grid topologies with large levels of renewable energy penetration and DR, and show that PAC converges to optimal control profiles in these scenarios. We further show that in certain regimes PAC outperforms the standard distributed 2-Block ADMM algorithm, and we discuss the benefits of using PAC over 2-Block ADMM and other standard distributed solvers.",
    "advisors": ["Anuradha Annaswamy"],
    "text": "A proximal atomic coordination algorithm for distributed optimization in distribution grids The control and regulation of power grids has historically relied upon large-scale scheduleable generation and relatively stable load demand profiles. With the advent of extensive local renewable energy generation technologies as well as the incorporation of load responsive demand response (DR) methodologies, it has become imperative that new distributed control strategies are developed to better regulate the increasingly volatile nature of modern generation and load profiles. In this thesis, we introduce a distributed control strategy called Proximal Atomic Coordination (PAC) to solve for optimal control strategies in distributed power grids, a problem called Optimal Power Flow (OPF). Using a convex relaxed variant of OPF, we show that PAC exhibits sub-linear convergence to the optimal ergodic cost, and linear convergence to the OPF solution. We demonstrate our results on various power grid topologies with large levels of renewable energy penetration and DR, and show that PAC converges to optimal control profiles in these scenarios. We further show that in certain regimes PAC outperforms the standard distributed 2-Block ADMM algorithm, and we discuss the benefits of using PAC over 2-Block ADMM and other standard distributed solvers."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33935",
    "title": "Hybrid organic/quantum dot thin film structures and devices",
    "abstract": "Organic light emitting diodes have undergone rapid advancement over the course of the past decade. Similarly, quantum dot synthesis has progressed to the point that room temperature highly efficient photoluminescence can be realized. It is the purpose of this work to utilize the beneficial properties of these two material sets in a robust light emitting device. New deposition techniques are necessary to the realization of this goal, enabling QD organic hybrids to be created in a quick and reliable manner compatible with known device fabrication methods. With these techniques, quantum dot light emitting devices are fabricated, measured, and analyzed. The devices are of high efficiency and color saturation, and provide us with a test bed for understanding the interactions between inorganic QDs and organic thin films.",
    "advisors": ["Vladimir BuloviÄ", "Terry Orlando"],
    "text": "Hybrid organic/quantum dot thin film structures and devices Organic light emitting diodes have undergone rapid advancement over the course of the past decade. Similarly, quantum dot synthesis has progressed to the point that room temperature highly efficient photoluminescence can be realized. It is the purpose of this work to utilize the beneficial properties of these two material sets in a robust light emitting device. New deposition techniques are necessary to the realization of this goal, enabling QD organic hybrids to be created in a quick and reliable manner compatible with known device fabrication methods. With these techniques, quantum dot light emitting devices are fabricated, measured, and analyzed. The devices are of high efficiency and color saturation, and provide us with a test bed for understanding the interactions between inorganic QDs and organic thin films."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8178",
    "title": "Synthetic aperture microscopy",
    "abstract": "In the late 1800's, Ernst Abbe, research director of the Carl Zeiss Optical Works, wrote down the rules for a lens to form a sharp image. Advances in communications theory, signal processing, and computers have allowed us.finally to break those rules. Our \"Synthetic Aperture Microscope\" floods a large region with a richly complex, finely structured pattern of light-the interference pattern of a ring of n coherent sources. A target within the volume of the interference fluoresces (or scatters or transmits) an amount of lights that reveals correspondences with this \"probing illumination.\" Modulating'fthe phases and amplitudes of the n beams with carefully chosen modulation signals causes the probe illumination to step through a predetermined or measured family of patterns. A sensor records the target's response in a time-sequence. This time-sequence contains each of order n2 complex Fourier coefficients of the target. Each of these coefficients is encrypted by a unique spread-spectrum key embedded in the amplitude and phase modulation signals. Signal processing picks out these coefficients to reconstruct an image of the target. Low resolution conventional imaging maps an array of \"targets\" (actually portions of a larger target) to a CCD array, thus allowing this sensing process to be done in parallel over a large region. The end result is to boost the resolution of a conventional imager by hundreds to thousands of sub-pixels per physical pixel. Both theoretical and experimental work on the engineering to make the concept practical are reported.",
    "advisors": ["Thomas F. Knight", "Dennis M. Freeman"],
    "text": "Synthetic aperture microscopy In the late 1800's, Ernst Abbe, research director of the Carl Zeiss Optical Works, wrote down the rules for a lens to form a sharp image. Advances in communications theory, signal processing, and computers have allowed us.finally to break those rules. Our \"Synthetic Aperture Microscope\" floods a large region with a richly complex, finely structured pattern of light-the interference pattern of a ring of n coherent sources. A target within the volume of the interference fluoresces (or scatters or transmits) an amount of lights that reveals correspondences with this \"probing illumination.\" Modulating'fthe phases and amplitudes of the n beams with carefully chosen modulation signals causes the probe illumination to step through a predetermined or measured family of patterns. A sensor records the target's response in a time-sequence. This time-sequence contains each of order n2 complex Fourier coefficients of the target. Each of these coefficients is encrypted by a unique spread-spectrum key embedded in the amplitude and phase modulation signals. Signal processing picks out these coefficients to reconstruct an image of the target. Low resolution conventional imaging maps an array of \"targets\" (actually portions of a larger target) to a CCD array, thus allowing this sensing process to be done in parallel over a large region. The end result is to boost the resolution of a conventional imager by hundreds to thousands of sub-pixels per physical pixel. Both theoretical and experimental work on the engineering to make the concept practical are reported."
}, {
    "id": "oai:dspace.mit.edu:1721.1/87449",
    "title": "Procedural authoring of solid models",
    "abstract": "This thesis investigates the creation, representation, and manipulation of volumetric geometry suitable for computer graphics applications. In order to capture and reproduce the appearance and behavior of many objects, it is necessary to model the internal structures and materials, and how they change over time. However, producing real-world effects with standard surface modeling techniques can be extremely challenging. My key contribution is a concise procedural approach for authoring layered, solid models. Using a simple scripting language, a complete volumetric representation of an object, including its internal structure, can be created from one or more input surfaces, such as scanned polygonal meshes, CAD models or implicit surfaces. Furthermore, the resulting model can be easily modified using sculpting and simulation tools, such as the Finite Element Method or particle systems, which are embedded as operators in the language. Simulation is treated as a modeling tool rather than merely a device for animation, which provides a novel level of abstraction for interacting with simulation environments. I present an implementation of the language using a flexible tetrahedral representation, which I chose because of its advantages for simulation tasks. The language and implementation are demonstrated on a variety of complex examples that were inspired by real-world objects.",
    "advisors": ["Julie Dorsey", "Leonard McMillan"],
    "text": "Procedural authoring of solid models This thesis investigates the creation, representation, and manipulation of volumetric geometry suitable for computer graphics applications. In order to capture and reproduce the appearance and behavior of many objects, it is necessary to model the internal structures and materials, and how they change over time. However, producing real-world effects with standard surface modeling techniques can be extremely challenging. My key contribution is a concise procedural approach for authoring layered, solid models. Using a simple scripting language, a complete volumetric representation of an object, including its internal structure, can be created from one or more input surfaces, such as scanned polygonal meshes, CAD models or implicit surfaces. Furthermore, the resulting model can be easily modified using sculpting and simulation tools, such as the Finite Element Method or particle systems, which are embedded as operators in the language. Simulation is treated as a modeling tool rather than merely a device for animation, which provides a novel level of abstraction for interacting with simulation environments. I present an implementation of the language using a flexible tetrahedral representation, which I chose because of its advantages for simulation tasks. The language and implementation are demonstrated on a variety of complex examples that were inspired by real-world objects."
}, {
    "id": "oai:dspace.mit.edu:1721.1/93833",
    "title": "Accountable systems : enabling appropriate use of information on the Web",
    "abstract": ".The Web is plagued by problems of privacy and piracy. In each instance, outdated laws combined with current technology provides little reassurance to information providers, and may have damaging side effects. To meet this challenge, we have designed, built, and tested and present a new architecture for information exchange on the Internet called HTTPA (Hyper Text Transfer Protocol with Accountability). In this 'Accountable' architecture, information use is tracked from its creation through its modification, repurposing and republishing with the help of the 'Provenance Tracking Network', a decentralized network of peers that together record the rules governing resources on the Web, coupled with how these resources are shared and used. We found that the accountable systems framework provides an attractive compromise where the rights and abilities of parties to control access and use is balanced against the burden of restrictions imposed for two prototype applications; one dealing with privacy in healthcare, and the other with rights in photo sharing. Healthcare patients given the ability to be notified of use of their medical records judged that they had sufficient privacy protection, while doctors obtained easier access to the records. Providers of photos could be assured their images were not being misused, without the many drawbacks that digital rights management (DRM) systems impose on those consuming the material. In a similar vein in which the growth of e-commerce Web sites led to the massive adoption of HTTPS, we envision that over time HTTPA will be accepted by the larger Web community to meet the concerns of privacy and copyright violations on the Web.",
    "advisors": ["Tim Berners-Lee", "Lalana Kagal"],
    "text": "Accountable systems : enabling appropriate use of information on the Web .The Web is plagued by problems of privacy and piracy. In each instance, outdated laws combined with current technology provides little reassurance to information providers, and may have damaging side effects. To meet this challenge, we have designed, built, and tested and present a new architecture for information exchange on the Internet called HTTPA (Hyper Text Transfer Protocol with Accountability). In this 'Accountable' architecture, information use is tracked from its creation through its modification, repurposing and republishing with the help of the 'Provenance Tracking Network', a decentralized network of peers that together record the rules governing resources on the Web, coupled with how these resources are shared and used. We found that the accountable systems framework provides an attractive compromise where the rights and abilities of parties to control access and use is balanced against the burden of restrictions imposed for two prototype applications; one dealing with privacy in healthcare, and the other with rights in photo sharing. Healthcare patients given the ability to be notified of use of their medical records judged that they had sufficient privacy protection, while doctors obtained easier access to the records. Providers of photos could be assured their images were not being misused, without the many drawbacks that digital rights management (DRM) systems impose on those consuming the material. In a similar vein in which the growth of e-commerce Web sites led to the massive adoption of HTTPS, we envision that over time HTTPA will be accepted by the larger Web community to meet the concerns of privacy and copyright violations on the Web."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117840",
    "title": "Computation by block copolymer self-assembly",
    "abstract": "Unconventional computation is a paradigm of computation that uses novel information tokens from natural systems to perform information processing. Using the complexity of physical systems, unconventional computing systems can efficiently solve problems that are difficult to solve classically. In this thesis, we use block copolymer self-assembly, a well-studied phenomenon in polymer science, to develop a new approach to computing by applying directed self-assembly to implement Ising-model-based computing systems in materials. In the first part of the thesis, we investigate directed self-assembly of block copolymer thin films within templates of different polygonal shapes. We define a two-state system based on the two degenerate alignment orientations of the ladder-shaped block copolymer structures formed inside square confinements, and study properties of the two-state system. In the second part of the thesis, we demonstrate an Ising lattice setup for directed self-assembly of block copolymers defined on two-dimensional arrays of posts. We develop an Ising-model-based simulation method that can perform block copolymer pattern prediction and template design. Finally, we design simple Boolean logic gates as a proof-of-concept demonstration of computation.",
    "advisors": ["Karl K. Berggren"],
    "text": "Computation by block copolymer self-assembly Unconventional computation is a paradigm of computation that uses novel information tokens from natural systems to perform information processing. Using the complexity of physical systems, unconventional computing systems can efficiently solve problems that are difficult to solve classically. In this thesis, we use block copolymer self-assembly, a well-studied phenomenon in polymer science, to develop a new approach to computing by applying directed self-assembly to implement Ising-model-based computing systems in materials. In the first part of the thesis, we investigate directed self-assembly of block copolymer thin films within templates of different polygonal shapes. We define a two-state system based on the two degenerate alignment orientations of the ladder-shaped block copolymer structures formed inside square confinements, and study properties of the two-state system. In the second part of the thesis, we demonstrate an Ising lattice setup for directed self-assembly of block copolymers defined on two-dimensional arrays of posts. We develop an Ising-model-based simulation method that can perform block copolymer pattern prediction and template design. Finally, we design simple Boolean logic gates as a proof-of-concept demonstration of computation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69773",
    "title": "Real-time brain-machine interface architectures : neural decoding from plan to movement",
    "abstract": "Brain-machine interfaces (BMI) aim to enable motor function in individuals with neurological injury or disease, by recording the neural activity, mapping or 'decoding' it into a motor command, and then controlling a device such as a computer interface or robotic arm. BMI research has largely focused on the problem of restoring the original motor function. The goal therefore has been to achieve a performance close to that of the healthy individual. There have been compelling proof of concept demonstrations of the utility of such BMIs in the past decade. However, performance of these systems needs to be significantly improved before they become clinically viable. Moreover, while developing high-performance BMIs with the goal of matching the original motor function is indeed valuable, a compelling goal is that of designing BMIs that can surpass original motor function. In this thesis, we first develop a novel real-time BMI for restoration of natural motor function. We then introduce a BMI architecture aimed at enhancing original motor function. We implement both our designs in rhesus monkeys. To facilitate the restoration of lost motor function, BMIs have focused on either estimating the continuous movement trajectory or target intent. However, natural movement often incorporates both. Moreover, both target and trajectory information are encoded in the motor cortical areas. These suggest that BMIs should be designed to combine these principal aspects of movement. We develop a novel two-stage BMI to decode jointly the target and trajectory of a reaching movement. First, we decode the intended target from neural spiking activity before movement initiation. Second, we combine the decoded target with the spiking activity during movement to estimate the trajectory. To do so, we use an optimal feedback-control design that aims to emulate the sensorimotor processing underlying actual motor control and directly processes the spiking activity using point process modeling in real time. We show that the two-stage BMI performs more accurately than either stage alone. Correct target prediction can compensate for inaccurate trajectory estimation and vice versa. This BMI also performs significantly better than linear regression approaches demonstrating the advantage of a design that more closely mimics the sensorimotor system.",
    "advisors": ["Gregory W. Wornell", "Emery N. Brown"],
    "text": "Real-time brain-machine interface architectures : neural decoding from plan to movement Brain-machine interfaces (BMI) aim to enable motor function in individuals with neurological injury or disease, by recording the neural activity, mapping or 'decoding' it into a motor command, and then controlling a device such as a computer interface or robotic arm. BMI research has largely focused on the problem of restoring the original motor function. The goal therefore has been to achieve a performance close to that of the healthy individual. There have been compelling proof of concept demonstrations of the utility of such BMIs in the past decade. However, performance of these systems needs to be significantly improved before they become clinically viable. Moreover, while developing high-performance BMIs with the goal of matching the original motor function is indeed valuable, a compelling goal is that of designing BMIs that can surpass original motor function. In this thesis, we first develop a novel real-time BMI for restoration of natural motor function. We then introduce a BMI architecture aimed at enhancing original motor function. We implement both our designs in rhesus monkeys. To facilitate the restoration of lost motor function, BMIs have focused on either estimating the continuous movement trajectory or target intent. However, natural movement often incorporates both. Moreover, both target and trajectory information are encoded in the motor cortical areas. These suggest that BMIs should be designed to combine these principal aspects of movement. We develop a novel two-stage BMI to decode jointly the target and trajectory of a reaching movement. First, we decode the intended target from neural spiking activity before movement initiation. Second, we combine the decoded target with the spiking activity during movement to estimate the trajectory. To do so, we use an optimal feedback-control design that aims to emulate the sensorimotor processing underlying actual motor control and directly processes the spiking activity using point process modeling in real time. We show that the two-stage BMI performs more accurately than either stage alone. Correct target prediction can compensate for inaccurate trajectory estimation and vice versa. This BMI also performs significantly better than linear regression approaches demonstrating the advantage of a design that more closely mimics the sensorimotor system."
}, {
    "id": "oai:dspace.mit.edu:1721.1/60099",
    "title": "Ultrastructure and nanomechanical properties of aggrecan from native cartilage and engineered tissue",
    "abstract": "Electrostatic interactions associated with aggrecan, one of the major components of the cartilage extracellular matrix, are responsible for ~50% of the equilibrium compressive elastic modulus of the tissue. The bottle-brush-shaped aggrecan consists of a core protein to which ~100 sulfated glycosaminoglycan (sGAG) chains are attached. Loss of sGAG is one early events in the pathogenesis of osteoarthritis and the resulting degradation of cartilage is irreversible due to its limited capacity for self-repair. Tissue engineering is one of the techniques which holds great potential for cartilage repair. In order to achieve successful repair, a clear understanding of native and engineered cartilage aggrecan is essential. With atomic force microscopy and high resolution force microscopy, the structure of aggrecan single molecules and the nanomechanical properties of an end-grafted aggrecan monolayer were quantified. Adult human aggrecan showed significantly shorter GAG chains and core proteins as well as lower molecular stiffness compared to that of newborn aggrecan. After enzymatic digestion of chondroitin sulfate (CS) GAGs, keratan sulfate GAG chains were visualized near the N-terminal domain of a less extended core protein. Direct visualization of aggrecan aggregates confirmed the structure of the constituent hyaluronic acid, aggrecan G1 domain, and link protein. Increased flexibility of the core protein was found near the G1 domain, which may facilitate the aggregate self-assembly process. Aggregated and non-aggregated aggrecan both showed remarked flexibility (i.e., decreased extension ratio) when the aggrecan areal density increased. These findings on intra- and inter-molecular structure provide insights into the structure-property relationships of aggrecan in vivo. Aggrecan produced by animal-matched bone marrow stromal cells (BMSCs) and chondrocytes seeded in peptide hydrogel were evaluated for their age-associated structure and nanomechanical properties. Independent of age, BMSCs produced longer core proteins and GAG chains than the chondrocytes, suggesting that the BMSC-produced aggrecan was characteristic of that from young cartilage. Comparison of the adult BMSC-produced aggrecan with adult cartilage-extracted aggrecan revealed that adult BMSC-aggrecan has a phenotype characteristic of young growth cartilage: primarily full-length aggrecan core, longer GAG chains and a higher content of chondroitin-4-sulfate in the CS-GAG chains, the latter identified via fluorescence assisted carbohydrate electrophoresis. The nanomechanical stiffness of BMSC-aggrecan was demonstrably greater than that of cartilage-aggrecan at the same total sGAG (fixed charge) density. These results support the use of adult BMSCs for cell-based cartilage repair.",
    "advisors": ["Alan J. Grodzinsky", "Christine Ortiz"],
    "text": "Ultrastructure and nanomechanical properties of aggrecan from native cartilage and engineered tissue Electrostatic interactions associated with aggrecan, one of the major components of the cartilage extracellular matrix, are responsible for ~50% of the equilibrium compressive elastic modulus of the tissue. The bottle-brush-shaped aggrecan consists of a core protein to which ~100 sulfated glycosaminoglycan (sGAG) chains are attached. Loss of sGAG is one early events in the pathogenesis of osteoarthritis and the resulting degradation of cartilage is irreversible due to its limited capacity for self-repair. Tissue engineering is one of the techniques which holds great potential for cartilage repair. In order to achieve successful repair, a clear understanding of native and engineered cartilage aggrecan is essential. With atomic force microscopy and high resolution force microscopy, the structure of aggrecan single molecules and the nanomechanical properties of an end-grafted aggrecan monolayer were quantified. Adult human aggrecan showed significantly shorter GAG chains and core proteins as well as lower molecular stiffness compared to that of newborn aggrecan. After enzymatic digestion of chondroitin sulfate (CS) GAGs, keratan sulfate GAG chains were visualized near the N-terminal domain of a less extended core protein. Direct visualization of aggrecan aggregates confirmed the structure of the constituent hyaluronic acid, aggrecan G1 domain, and link protein. Increased flexibility of the core protein was found near the G1 domain, which may facilitate the aggregate self-assembly process. Aggregated and non-aggregated aggrecan both showed remarked flexibility (i.e., decreased extension ratio) when the aggrecan areal density increased. These findings on intra- and inter-molecular structure provide insights into the structure-property relationships of aggrecan in vivo. Aggrecan produced by animal-matched bone marrow stromal cells (BMSCs) and chondrocytes seeded in peptide hydrogel were evaluated for their age-associated structure and nanomechanical properties. Independent of age, BMSCs produced longer core proteins and GAG chains than the chondrocytes, suggesting that the BMSC-produced aggrecan was characteristic of that from young cartilage. Comparison of the adult BMSC-produced aggrecan with adult cartilage-extracted aggrecan revealed that adult BMSC-aggrecan has a phenotype characteristic of young growth cartilage: primarily full-length aggrecan core, longer GAG chains and a higher content of chondroitin-4-sulfate in the CS-GAG chains, the latter identified via fluorescence assisted carbohydrate electrophoresis. The nanomechanical stiffness of BMSC-aggrecan was demonstrably greater than that of cartilage-aggrecan at the same total sGAG (fixed charge) density. These results support the use of adult BMSCs for cell-based cartilage repair."
}, {
    "id": "oai:dspace.mit.edu:1721.1/38201",
    "title": "An electromechanical valve drive incorporating a nonlinear mechanical transformer",
    "abstract": "In traditional internal combustion engines, a camshaft acts on the valve stems to open and close the valves. Valve timing is fixed relative to piston position. On the other hand, if a valve is flexibly controlled by a variable valve actuation (VVA) system, we can achieve significant improvements in fuel efficiency, engine performance, and emissions. One of the most advanced variable valve actuation systems is the VVA operated by an electromechanical actuator without a camshaft, the so-called bi-positional electromechanical valve drive (EMVD). Existing EMVDs characteristically use a spring to provide the required mechanical power for operating a valve. The use of a spring provides many benefits to the design of the system, but it also results in difficult design challenges. The large holding force against the spring at the ends of the stroke suggests the use of a normal-force electromagnetic actuator, which, from a servomechanical point of view, is considerably inferior to a shear-force actuator. Furthermore, the large holding force generates a large jerk at the beginning and the end of a stroke and makes it difficult to achieve soft valve landing. An innovative electromechanical valve drive (EMVD) design is proposed, which incorporates a nonlinear mechanical transformer and a shear-force actuator. This allows not only fast but also smooth valve motion, almost zero seating velocity, zero holding power, and improved control with acceptable electric power. This proposed concept is modeled, analyzed, simulated, designed, and implemented. Experimental results show the beneficial features of the promising proposed concept.",
    "advisors": ["John G. Kassakian"],
    "text": "An electromechanical valve drive incorporating a nonlinear mechanical transformer In traditional internal combustion engines, a camshaft acts on the valve stems to open and close the valves. Valve timing is fixed relative to piston position. On the other hand, if a valve is flexibly controlled by a variable valve actuation (VVA) system, we can achieve significant improvements in fuel efficiency, engine performance, and emissions. One of the most advanced variable valve actuation systems is the VVA operated by an electromechanical actuator without a camshaft, the so-called bi-positional electromechanical valve drive (EMVD). Existing EMVDs characteristically use a spring to provide the required mechanical power for operating a valve. The use of a spring provides many benefits to the design of the system, but it also results in difficult design challenges. The large holding force against the spring at the ends of the stroke suggests the use of a normal-force electromagnetic actuator, which, from a servomechanical point of view, is considerably inferior to a shear-force actuator. Furthermore, the large holding force generates a large jerk at the beginning and the end of a stroke and makes it difficult to achieve soft valve landing. An innovative electromechanical valve drive (EMVD) design is proposed, which incorporates a nonlinear mechanical transformer and a shear-force actuator. This allows not only fast but also smooth valve motion, almost zero seating velocity, zero holding power, and improved control with acceptable electric power. This proposed concept is modeled, analyzed, simulated, designed, and implemented. Experimental results show the beneficial features of the promising proposed concept."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8699",
    "title": "Principled computational methods for the validation discovery of genetic regulatory networks",
    "abstract": "As molecular biology continues to evolve in the direction of high-throughput collection of data, it has become increasingly necessary to develop computational methods for analyzing observed data that are at once both sophisticated enough to capture essential features of biological phenomena and at the same time approachable in terms of their application. We demonstrate how graphical models, and Bayesian networks in particular, can be used to model genetic regulatory networks. These methods are well-suited to this problem owing to their ability to model more than pair-wise relationships between variables, their ability to guard against over-fitting, and their robustness in the face of noisy data. Moreover, Bayesian network models can be scored in a principled manner in the presence of both genomic expression and location data. We develop methods for extending Bayesian network semantics to include edge annotations that allow us to model statistical dependencies between biological factors with greater refinement. We derive principled methods for scoring these annotated Bayesian networks. Using these models in the presence of genomic expression data requires suitable methods for the normalization and discretization of this data.",
    "advisors": ["David K. Gifford"],
    "text": "Principled computational methods for the validation discovery of genetic regulatory networks As molecular biology continues to evolve in the direction of high-throughput collection of data, it has become increasingly necessary to develop computational methods for analyzing observed data that are at once both sophisticated enough to capture essential features of biological phenomena and at the same time approachable in terms of their application. We demonstrate how graphical models, and Bayesian networks in particular, can be used to model genetic regulatory networks. These methods are well-suited to this problem owing to their ability to model more than pair-wise relationships between variables, their ability to guard against over-fitting, and their robustness in the face of noisy data. Moreover, Bayesian network models can be scored in a principled manner in the presence of both genomic expression and location data. We develop methods for extending Bayesian network semantics to include edge annotations that allow us to model statistical dependencies between biological factors with greater refinement. We derive principled methods for scoring these annotated Bayesian networks. Using these models in the presence of genomic expression data requires suitable methods for the normalization and discretization of this data."
}, {
    "id": "oai:dspace.mit.edu:1721.1/36185",
    "title": "Improving aggregate user utilities and providing fairness in multi-rate wireless LANs",
    "abstract": "A distributed medium access control (MAC) protocol is responsible for allocating the shared spectrum efficiently and fairly among competing devices using a wireless local area network. Unfortunately, existing MAC protocols, including 802.11's DCF, achieve neither efficiency nor fairness under many realistic conditions. In this dissertation, we show that both bit and frame-based fairness,the most widely used notions, lead to drastically reduced aggregate throughput and increased average delay in typical environments, in which competing nodes transmit at different data transmission rates. We demonstrate the advantages of time-based fairness, in which each competing node receives an equal share of the wireless channel occupancy time. Through analysis, experiments on a Linux test bed, and simulation, we demonstrate that time-based fairness can lead to significant improvements in aggregate throughput and average delay. Through a game theoretic analysis and simulation, we also show that existing MAC protocols encourage non-cooperative nodes to employ globally inefficient transmission strategies that lead to low aggregate throughput. We show that providing long-term time share guarantees among competing nodes leads rational nodes to employ efficient transmission strategies at equilibriums.",
    "advisors": ["John Guttag"],
    "text": "Improving aggregate user utilities and providing fairness in multi-rate wireless LANs A distributed medium access control (MAC) protocol is responsible for allocating the shared spectrum efficiently and fairly among competing devices using a wireless local area network. Unfortunately, existing MAC protocols, including 802.11's DCF, achieve neither efficiency nor fairness under many realistic conditions. In this dissertation, we show that both bit and frame-based fairness,the most widely used notions, lead to drastically reduced aggregate throughput and increased average delay in typical environments, in which competing nodes transmit at different data transmission rates. We demonstrate the advantages of time-based fairness, in which each competing node receives an equal share of the wireless channel occupancy time. Through analysis, experiments on a Linux test bed, and simulation, we demonstrate that time-based fairness can lead to significant improvements in aggregate throughput and average delay. Through a game theoretic analysis and simulation, we also show that existing MAC protocols encourage non-cooperative nodes to employ globally inefficient transmission strategies that lead to low aggregate throughput. We show that providing long-term time share guarantees among competing nodes leads rational nodes to employ efficient transmission strategies at equilibriums."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35596",
    "title": "Circuit-aware system design techniques for wireless communication",
    "abstract": "When designing wireless communication systems, many hardware details are hidden from the algorithm designer, especially with analog hardware. While it is difficult for a designer to understand all aspects of a complex system, some knowledge of circuit constraints can improve system performance by relaxing design constraints. The specifications of a circuit design are generally not equally difficult to meet, allowing excess margin in one area to be used to relax more difficult design constraints. We first propose an uplink/downlink architecture for a network with a multiple antenna central server. This design takes advantage of the central server to allow the nodes to achieve multiplexing gain by forming virtual arrays without coordination, or diversity gain to decrease SNR requirements. Computation and memory are offloaded from the nodes to the server, allowing less complex, inexpensive nodes to be used. We can further use this SNR margin to reduce circuit area and power consumption, sacrificing system capacity for circuit optimization. Besides the more common transmit power reduction, large passive analog components can be removed to reduce chip area, and bias currents lowered to save power at the expense of noise figure. Given the inevitable crosstalk coupling of circuits, we determine the minimum required crosstalk isolation in terms of circuit gain and signal range.",
    "advisors": ["Gregory W. Wornell"],
    "text": "Circuit-aware system design techniques for wireless communication When designing wireless communication systems, many hardware details are hidden from the algorithm designer, especially with analog hardware. While it is difficult for a designer to understand all aspects of a complex system, some knowledge of circuit constraints can improve system performance by relaxing design constraints. The specifications of a circuit design are generally not equally difficult to meet, allowing excess margin in one area to be used to relax more difficult design constraints. We first propose an uplink/downlink architecture for a network with a multiple antenna central server. This design takes advantage of the central server to allow the nodes to achieve multiplexing gain by forming virtual arrays without coordination, or diversity gain to decrease SNR requirements. Computation and memory are offloaded from the nodes to the server, allowing less complex, inexpensive nodes to be used. We can further use this SNR margin to reduce circuit area and power consumption, sacrificing system capacity for circuit optimization. Besides the more common transmit power reduction, large passive analog components can be removed to reduce chip area, and bias currents lowered to save power at the expense of noise figure. Given the inevitable crosstalk coupling of circuits, we determine the minimum required crosstalk isolation in terms of circuit gain and signal range."
}, {
    "id": "oai:dspace.mit.edu:1721.1/30155",
    "title": "Quantum wells on indium gallium arsenic compositionally graded buffers realized by molecular beam epitaxy",
    "abstract": "For a long time, there has been a desire to extend the emission wavelength of GaAs-based quantum well lasers, with the aim of eventually replacing InP with GaAs as the substrate of choice for communication applications. Using dilute nitride GaInAsN QWs or InAs quantum dots, emission wavelengths have successfully been extended to 1.3 m, but significant difficulties have been met going beyond 1.3 m. In this thesis, we present an alternative approach, namely, the molecular beam epitaxy (MBE) growth of quantum wells on top of indium gallium arsenic compositionally graded buffers, with the indium composition in the buffers linearly graded from 0% to 15% or 20%. We observed that one can obtain strong quantum emission on top of such graded buffers only under a very restricted range of growth conditions, detailed in this thesis, which are not compatible with the subsequent growth of the aluminum-containing barriers necessary for carrier confinement. Furthermore, upon proper ex-situ annealing, it was able to obtain QW emission as strong as, sometimes even stronger than, that from QWs pseudomorphically grown on GaAs.However, when even slight tensile or compressive strain was added to the QWs, severe degradation occurred, which was likely related with the amount of surface roughness induced by the crosshatches developed during and after the growth of the graded buffers. Temperature dependent photoluminescence was employed as a tool to investigate the relationship between the ex-situ annealing, strain and quantum well photoluminescence. It was found that there was a significant PL decay mechanism between 50K to about 250K for the aluminum containing unannealed quantum well samples. For the unstrained ones, this mechanism could be removed effectively by annealing. However, strain in quantum well was observed to retard this removal. The same observations were made in both the pseudomorphically and metamorphically grown samples, but the metamorphic ones seemed to suffer more from the retardation.Finally, the theoretical modeling of the photoluminescence temperature dependence was reformulated such that physical processes or band diagram features could be related to the measurement results. Only under restricted circumstances, our formulation was found to be identical to the existing, commonly used, description of the photoluminescence temperature dependence.",
    "advisors": ["Clifton G. Fonstad"],
    "text": "Quantum wells on indium gallium arsenic compositionally graded buffers realized by molecular beam epitaxy For a long time, there has been a desire to extend the emission wavelength of GaAs-based quantum well lasers, with the aim of eventually replacing InP with GaAs as the substrate of choice for communication applications. Using dilute nitride GaInAsN QWs or InAs quantum dots, emission wavelengths have successfully been extended to 1.3 m, but significant difficulties have been met going beyond 1.3 m. In this thesis, we present an alternative approach, namely, the molecular beam epitaxy (MBE) growth of quantum wells on top of indium gallium arsenic compositionally graded buffers, with the indium composition in the buffers linearly graded from 0% to 15% or 20%. We observed that one can obtain strong quantum emission on top of such graded buffers only under a very restricted range of growth conditions, detailed in this thesis, which are not compatible with the subsequent growth of the aluminum-containing barriers necessary for carrier confinement. Furthermore, upon proper ex-situ annealing, it was able to obtain QW emission as strong as, sometimes even stronger than, that from QWs pseudomorphically grown on GaAs.However, when even slight tensile or compressive strain was added to the QWs, severe degradation occurred, which was likely related with the amount of surface roughness induced by the crosshatches developed during and after the growth of the graded buffers. Temperature dependent photoluminescence was employed as a tool to investigate the relationship between the ex-situ annealing, strain and quantum well photoluminescence. It was found that there was a significant PL decay mechanism between 50K to about 250K for the aluminum containing unannealed quantum well samples. For the unstrained ones, this mechanism could be removed effectively by annealing. However, strain in quantum well was observed to retard this removal. The same observations were made in both the pseudomorphically and metamorphically grown samples, but the metamorphic ones seemed to suffer more from the retardation.Finally, the theoretical modeling of the photoluminescence temperature dependence was reformulated such that physical processes or band diagram features could be related to the measurement results. Only under restricted circumstances, our formulation was found to be identical to the existing, commonly used, description of the photoluminescence temperature dependence."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37913",
    "title": "Games, puzzles, and computation",
    "abstract": "There is a fundamental connection between the notions of game and of computation. At its most basic level, this is implied by any game complexity result, but the connection is deeper than this. One example is the concept of alternating nondeterminism, which is intimately connected with two-player games. In the first half of this thesis, I develop the idea of game as computation to a greater degree than has been done previously. I present a general family of games, called Constraint Logic, which is both mathematically simple and ideally suited for reductions to many actual board games. A deterministic version of Constraint Logic corresponds to a novel kind of logic circuit which is monotone and reversible. At the other end of the spectrum, I show that a multiplayer version of Constraint Logic is undecidable. That there are undecidable games using finite physical resources is philosophically important, and raises issues related to the Church-Turing thesis. In the second half of this thesis, I apply the Constraint Logic formalism to many actual games and puzzles, providing new hardness proofs. These applications include sliding-block puzzles, sliding-coin puzzles, plank puzzles, hinged polygon dissections, Amazons, Kohane, Cross Purposes, Tip over, and others.",
    "advisors": ["Erik D. Demaine", "Gerald J. Sussman"],
    "text": "Games, puzzles, and computation There is a fundamental connection between the notions of game and of computation. At its most basic level, this is implied by any game complexity result, but the connection is deeper than this. One example is the concept of alternating nondeterminism, which is intimately connected with two-player games. In the first half of this thesis, I develop the idea of game as computation to a greater degree than has been done previously. I present a general family of games, called Constraint Logic, which is both mathematically simple and ideally suited for reductions to many actual board games. A deterministic version of Constraint Logic corresponds to a novel kind of logic circuit which is monotone and reversible. At the other end of the spectrum, I show that a multiplayer version of Constraint Logic is undecidable. That there are undecidable games using finite physical resources is philosophically important, and raises issues related to the Church-Turing thesis. In the second half of this thesis, I apply the Constraint Logic formalism to many actual games and puzzles, providing new hardness proofs. These applications include sliding-block puzzles, sliding-coin puzzles, plank puzzles, hinged polygon dissections, Amazons, Kohane, Cross Purposes, Tip over, and others."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101565",
    "title": "Rethinking the application-database interface",
    "abstract": "Applications that interact with database management systems (DBMSs) are ubiquitous in our daily lives. Such database applications are usually hosted on an application server and perform many small accesses over the network to a DBMS hosted on a database server to retrieve data for processing. For decades, the database and programming systems research communities have worked on optimizing such applications from different perspectives: database researchers have built highly efficient DBMSs, and programming systems researchers have developed specialized compilers and runtime systems for hosting applications. However, there has been relative little work that examines the interface between these two software layers to improve application performance. In this thesis, I show how making use of application semantics and optimizing across these layers of the software stack can help us improve the performance of database applications. In particular, I describe three projects that optimize database applications by looking at both the programming system and the DBMS in tandem. By carefully revisiting the interface between the DBMS and the application, and by applying a mix of declarative database optimization and modern program analysis and synthesis techniques, we show that multiple orders of magnitude speedups are possible in real-world applications. I conclude by highlighting future work in the area, and propose a vision towards automatically generating application-specific data stores.",
    "text": "Rethinking the application-database interface Applications that interact with database management systems (DBMSs) are ubiquitous in our daily lives. Such database applications are usually hosted on an application server and perform many small accesses over the network to a DBMS hosted on a database server to retrieve data for processing. For decades, the database and programming systems research communities have worked on optimizing such applications from different perspectives: database researchers have built highly efficient DBMSs, and programming systems researchers have developed specialized compilers and runtime systems for hosting applications. However, there has been relative little work that examines the interface between these two software layers to improve application performance. In this thesis, I show how making use of application semantics and optimizing across these layers of the software stack can help us improve the performance of database applications. In particular, I describe three projects that optimize database applications by looking at both the programming system and the DBMS in tandem. By carefully revisiting the interface between the DBMS and the application, and by applying a mix of declarative database optimization and modern program analysis and synthesis techniques, we show that multiple orders of magnitude speedups are possible in real-world applications. I conclude by highlighting future work in the area, and propose a vision towards automatically generating application-specific data stores."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117838",
    "title": "Architecture design for highly flexible and energy-efficient deep neural network accelerators",
    "abstract": "Deep neural networks (DNNs) are the backbone of modern artificial intelligence (AI). However, due to their high computational complexity and diverse shapes and sizes, dedicated accelerators that can achieve high performance and energy efficiency across a wide range of DNNs are critical for enabling AI in real-world applications. To address this, we present Eyeriss, a co-design of software and hardware architecture for DNN processing that is optimized for performance, energy efficiency and flexibility. Eyeriss features a novel Row-Stationary (RS) dataflow to minimize data movement when processing a DNN, which is the bottleneck of both performance and energy efficiency. The RS dataflow supports highly-parallel processing while fully exploiting data reuse in a multi-level memory hierarchy to optimize for the overall system energy efficiency given any DNN shape and size. It achieves 1.4x to 2.5x higher energy efficiency than other existing dataflows. To support the RS dataflow, we present two versions of the Eyeriss architecture. Eyeriss v1 targets large DNNs that have plenty of data reuse. It features a flexible mapping strategy for high performance and a multicast on-chip network (NoC) for high data reuse, and further exploits data sparsity to reduce processing element (PE) power by 45% and off-chip bandwidth by up to 1.9x. Fabricated in a 65nm CMOS, Eyeriss v1 consumes 278 mW at 34.7 fps for the CONV layers of AlexNet, which is 10x more efficient than a mobile GPU. Eyeriss v2 addresses support for the emerging compact DNNs that introduce higher variation in data reuse. It features a RS+ dataflow that improves PE utilization, and a flexible and scalable NoC that adapts to the bandwidth requirement while also exploiting available data reuse. Together, they provide over 10x higher throughput than Eyeriss v1 at 256 PEs. Eyeriss v2 also exploits sparsity and SIMD for an additional 6x increase in throughput.",
    "advisors": ["Vivienne Sze", "Joel Emer"],
    "text": "Architecture design for highly flexible and energy-efficient deep neural network accelerators Deep neural networks (DNNs) are the backbone of modern artificial intelligence (AI). However, due to their high computational complexity and diverse shapes and sizes, dedicated accelerators that can achieve high performance and energy efficiency across a wide range of DNNs are critical for enabling AI in real-world applications. To address this, we present Eyeriss, a co-design of software and hardware architecture for DNN processing that is optimized for performance, energy efficiency and flexibility. Eyeriss features a novel Row-Stationary (RS) dataflow to minimize data movement when processing a DNN, which is the bottleneck of both performance and energy efficiency. The RS dataflow supports highly-parallel processing while fully exploiting data reuse in a multi-level memory hierarchy to optimize for the overall system energy efficiency given any DNN shape and size. It achieves 1.4x to 2.5x higher energy efficiency than other existing dataflows. To support the RS dataflow, we present two versions of the Eyeriss architecture. Eyeriss v1 targets large DNNs that have plenty of data reuse. It features a flexible mapping strategy for high performance and a multicast on-chip network (NoC) for high data reuse, and further exploits data sparsity to reduce processing element (PE) power by 45% and off-chip bandwidth by up to 1.9x. Fabricated in a 65nm CMOS, Eyeriss v1 consumes 278 mW at 34.7 fps for the CONV layers of AlexNet, which is 10x more efficient than a mobile GPU. Eyeriss v2 addresses support for the emerging compact DNNs that introduce higher variation in data reuse. It features a RS+ dataflow that improves PE utilization, and a flexible and scalable NoC that adapts to the bandwidth requirement while also exploiting available data reuse. Together, they provide over 10x higher throughput than Eyeriss v1 at 256 PEs. Eyeriss v2 also exploits sparsity and SIMD for an additional 6x increase in throughput."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62383",
    "title": "Using rigging and transfer to animate 3D characters",
    "abstract": "Transferring a mesh or skeletal animation onto a new mesh currently requires significant manual effort. For skeletal animations, this involves rigging the character, by specifying how the skeleton is positioned relative to the character and how posing the skeleton drives the character's shape. Currently, artists typically manually position the skeleton joints and paint skinning weights onto the character to associate points on the character surface with bones. For this problem, we present a fully automatic rigging algorithm based on the geometry of the target mesh. Given a generic skeleton, the method computes both joint placement and the character surface attachment automatically. For mesh animations, current techniques are limited to transferring the motion literally using a correspondence between the characters' surfaces. Instead, I propose an example-based method that can transfer motion between far more different characters and that gives the user more control over how to adapt the motion to the new character.",
    "advisors": ["Jovan Popović"],
    "text": "Using rigging and transfer to animate 3D characters Transferring a mesh or skeletal animation onto a new mesh currently requires significant manual effort. For skeletal animations, this involves rigging the character, by specifying how the skeleton is positioned relative to the character and how posing the skeleton drives the character's shape. Currently, artists typically manually position the skeleton joints and paint skinning weights onto the character to associate points on the character surface with bones. For this problem, we present a fully automatic rigging algorithm based on the geometry of the target mesh. Given a generic skeleton, the method computes both joint placement and the character surface attachment automatically. For mesh animations, current techniques are limited to transferring the motion literally using a correspondence between the characters' surfaces. Instead, I propose an example-based method that can transfer motion between far more different characters and that gives the user more control over how to adapt the motion to the new character."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37918",
    "title": "Thermophotovoltaics : shaping the flow of thermal radiation",
    "abstract": "This thesis explores the modeling, design, and optimization of photonic crystals as spectral control components for high-performance thermophotovoltaic (TPV) power conversion. In particular, we focus on the use of one-dimensional and two dimensional photonic crystals as optical filters and selective thermal emitters for thermophotovoltaic and micro-thermophotovoltaic (micro-TPV)) applications. In addition, we explore fundamental limitations of photonic crystal thermal emitters and provide new insights into the limiting power transfer mechanisms that are relevant for TPV, micro-TPV, lighting and sensor applications. Ideal thermodynamic models that capture dominant power transfer mechanism for TPV and micro-TPV case, are developed and used for the design, optimization and system performance estimation of TPV systems with photonic-crystals. Furthermore, we propose for the first time two new classes of narrow-band thermal emitters that use the resonant cavity effect. The first type of narrow-band thermal emitters rely on vertical-cavity to enhance the thermal emission of highly reflective materials (e.g metals). This class of emitters was named the vertical cavity enhanced resonant thermal emitter (VERTE).",
    "advisors": ["John G. Kassakian"],
    "text": "Thermophotovoltaics : shaping the flow of thermal radiation This thesis explores the modeling, design, and optimization of photonic crystals as spectral control components for high-performance thermophotovoltaic (TPV) power conversion. In particular, we focus on the use of one-dimensional and two dimensional photonic crystals as optical filters and selective thermal emitters for thermophotovoltaic and micro-thermophotovoltaic (micro-TPV)) applications. In addition, we explore fundamental limitations of photonic crystal thermal emitters and provide new insights into the limiting power transfer mechanisms that are relevant for TPV, micro-TPV, lighting and sensor applications. Ideal thermodynamic models that capture dominant power transfer mechanism for TPV and micro-TPV case, are developed and used for the design, optimization and system performance estimation of TPV systems with photonic-crystals. Furthermore, we propose for the first time two new classes of narrow-band thermal emitters that use the resonant cavity effect. The first type of narrow-band thermal emitters rely on vertical-cavity to enhance the thermal emission of highly reflective materials (e.g metals). This class of emitters was named the vertical cavity enhanced resonant thermal emitter (VERTE)."
}, {
    "id": "oai:dspace.mit.edu:1721.1/40497",
    "title": "Channel-adapted quantum error correction",
    "abstract": "Quantum error correction (QEC) is an essential concept for any quantum information processing device. Typically, QEC is designed with minimal assumptions about the noise process; this generic assumption exacts a high cost in efficiency and performance. We examine QEC methods that are adapted to the physical noise model. In physical systems, errors are not likely to be arbitrary; rather we will have reasonable models for the structure of quantum decoherence. We may choose quantum error correcting codes and recovery operations that specifically target the most likely errors. This can increase QEC performance and also reduce the required overhead. We present a convex optimization method to determine the optimal (in terms of average entanglement fidelity) recovery operation for a given channel, encoding, and information source. This is solvable via a semidefinite program (SDP). We derive an analytic solution to the optimal recovery for the case of stabilizer codes, the completely mixed input source, and channels characterized by Pauli group errors. We present computational algorithms to generate near-optimal recovery operations structured to begin with a projective syndrome measurement.",
    "advisors": ["Peter W. Shor", "Moe Z. Win"],
    "text": "Channel-adapted quantum error correction Quantum error correction (QEC) is an essential concept for any quantum information processing device. Typically, QEC is designed with minimal assumptions about the noise process; this generic assumption exacts a high cost in efficiency and performance. We examine QEC methods that are adapted to the physical noise model. In physical systems, errors are not likely to be arbitrary; rather we will have reasonable models for the structure of quantum decoherence. We may choose quantum error correcting codes and recovery operations that specifically target the most likely errors. This can increase QEC performance and also reduce the required overhead. We present a convex optimization method to determine the optimal (in terms of average entanglement fidelity) recovery operation for a given channel, encoding, and information source. This is solvable via a semidefinite program (SDP). We derive an analytic solution to the optimal recovery for the case of stabilizer codes, the completely mixed input source, and channels characterized by Pauli group errors. We present computational algorithms to generate near-optimal recovery operations structured to begin with a projective syndrome measurement."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8698",
    "title": "Exploring scatterer anisotrophy in synthetic aperture radar via sub-aperture analysis",
    "abstract": "Scattering from man-made objects in SAR imagery exhibits aspect and frequency dependencies which are not always well modeled by standard SAR imaging techniques based on the ideal point scattering model. This is particularly the case for highresolution wide-band and wide-aperture data where model deviations are even more pronounced. If ignored, these deviations will reduce recognition performance due to the model mismatch, but when appropriately accounted for, these deviations from the ideal point scattering model can be exploited as attributes to better distinguish scatterers and their respective targets. With this in mind, this thesis develops an efficient modeling framework based on a sub-aperture pyramid to utilize scatterer anisotropy for the purpose of target classification. Two approaches are presented to exploit scatterer anisotropy using the sub-aperture pyramid. The first is a nonparametric classifier that learns the azimuthal dependencies within an image and makes a classification decision based on the learned dependencies. The second approach is a parametric attribution of the observed anisotropy characterizing the azimuthal location and concentration of the scattering response. Working from the sub-aperture scattering model, we develop a hypothesis test to characterize anisotropy. We start with an isolated scatterer model which produces a test with an intuitive interpretation. We then address the problem of robustness to interfering scatterers by extending the model to account for neighboring scatterers which corrupt the anisotropy attribution.",
    "advisors": ["Alan S. Willsky"],
    "text": "Exploring scatterer anisotrophy in synthetic aperture radar via sub-aperture analysis Scattering from man-made objects in SAR imagery exhibits aspect and frequency dependencies which are not always well modeled by standard SAR imaging techniques based on the ideal point scattering model. This is particularly the case for highresolution wide-band and wide-aperture data where model deviations are even more pronounced. If ignored, these deviations will reduce recognition performance due to the model mismatch, but when appropriately accounted for, these deviations from the ideal point scattering model can be exploited as attributes to better distinguish scatterers and their respective targets. With this in mind, this thesis develops an efficient modeling framework based on a sub-aperture pyramid to utilize scatterer anisotropy for the purpose of target classification. Two approaches are presented to exploit scatterer anisotropy using the sub-aperture pyramid. The first is a nonparametric classifier that learns the azimuthal dependencies within an image and makes a classification decision based on the learned dependencies. The second approach is a parametric attribution of the observed anisotropy characterizing the azimuthal location and concentration of the scattering response. Working from the sub-aperture scattering model, we develop a hypothesis test to characterize anisotropy. We start with an isolated scatterer model which produces a test with an intuitive interpretation. We then address the problem of robustness to interfering scatterers by extending the model to account for neighboring scatterers which corrupt the anisotropy attribution."
}, {
    "id": "oai:dspace.mit.edu:1721.1/66004",
    "title": "Integration of photonic and passive microfluidic devices into lab-on-chip with femtosecond laser materials processing",
    "abstract": "Femtosecond laser materials processing is a powerful method for the integration of high resolution, 3D structures into Lab-On-Chip (LOC) systems. One major application of femtosecond laser materials processing is waveguide fabrication in glass via index modification. We demonstrate the ability to fabricate couplers and Mach-Zehnder Interferometers (MZI) with good repeatability and flexibility. An in-depth characterization of the spectral characteristics of symmetric directional couplers and MZI is presented. The spectral data from a series of unbalanced MZI is used to characterize changes in the waveguide propagation constant. Towards integrated sensing, we demonstrate the application of femtosecond laser waveguide fabrication to the integration of a MZI into a homemade and commercial LOC for label-free optical detection. The MZI has a unique tilted 3D geometry with one arm crossing a microfluidic channel and enables spatially resolved sensing of changes in the refractive index of the content inside the channel with a limit of detection as low as 1x10 4 RIU. Another major technique in femtosecond laser materials processing is femtosecond two-photon polymerization (TPP). TPP is used to integrate 3D porous filters into a commercial LOC and testing of the filter shows virtually 100% efficient separation of 3 tm polystyrene spheres from a liquid solution. The direct write and maskless nature of femtosecond materials processing makes it a powerful method to integrate 3D devices into LOC without altering existing elements or changing the microfluidic channel fabrication.",
    "advisors": ["James G. Fujimoto"],
    "text": "Integration of photonic and passive microfluidic devices into lab-on-chip with femtosecond laser materials processing Femtosecond laser materials processing is a powerful method for the integration of high resolution, 3D structures into Lab-On-Chip (LOC) systems. One major application of femtosecond laser materials processing is waveguide fabrication in glass via index modification. We demonstrate the ability to fabricate couplers and Mach-Zehnder Interferometers (MZI) with good repeatability and flexibility. An in-depth characterization of the spectral characteristics of symmetric directional couplers and MZI is presented. The spectral data from a series of unbalanced MZI is used to characterize changes in the waveguide propagation constant. Towards integrated sensing, we demonstrate the application of femtosecond laser waveguide fabrication to the integration of a MZI into a homemade and commercial LOC for label-free optical detection. The MZI has a unique tilted 3D geometry with one arm crossing a microfluidic channel and enables spatially resolved sensing of changes in the refractive index of the content inside the channel with a limit of detection as low as 1x10 4 RIU. Another major technique in femtosecond laser materials processing is femtosecond two-photon polymerization (TPP). TPP is used to integrate 3D porous filters into a commercial LOC and testing of the filter shows virtually 100% efficient separation of 3 tm polystyrene spheres from a liquid solution. The direct write and maskless nature of femtosecond materials processing makes it a powerful method to integrate 3D devices into LOC without altering existing elements or changing the microfluidic channel fabrication."
}, {
    "id": "oai:dspace.mit.edu:1721.1/105955",
    "title": "Scalable algorithms for semi-automatic segmentation of electron microscopy images of the brain tissue",
    "abstract": "I present a set of fast and scalable algorithms for segmenting very large 3D images of brain tissue. Currently, light and electron microscopy can now produce terascale 3D images within hours. Extracting the information about the shapes and connectivity of the neurons require fast and accurate image segmentation algorithms. Due to the sheer size of the problem, traditional approaches might be computationally infeasible. I focus on an segmentation pipeline that breaks up the segmentation problem into multiple stages, each of which can be improved independently. In the first step of the pipeline, convolutional neural networks are used to predict segment boundaries. Watershed transform is then used to obtain an over-segmentation, which is then reduced using agglomerative clustering algorithms. Finally, manual or computer-assisted proof reading is done by experts. In this thesis, I revisit the traditional approaches for training and applying convolutional neural networks, and propose: - A fast and scalable 3D convolutional network training algorithm suited for multi-core and many-core shared memory machines. The two main quantities of the algorithm are: (1) minimizing the required computation by using FFT-based convolution with memoization, and (2) parallelization approach that can utilize large number of CPUs while minimizing any required synchronization. - A high throughput inference algorithm that can utilize all available computational resources, CPUs and GPUs. I introduce a set of highly parallel algorithms for different layer types and architectures, and show how to combine them to achieve very high throughput. Additionally, I study the theoretical properties of the watershed transform of edge- weighed graphs and propose a liner-time algorithm. I propose a set of modification to the standard algorithm and a quasi-linear agglomerative clustering algorithm that can greatly reduce the over-segmentation produced by the standard watershed algorithm.",
    "advisors": ["H. Sebastian Seung", " Frédo Durand", "Nir Shavit"],
    "text": "Scalable algorithms for semi-automatic segmentation of electron microscopy images of the brain tissue I present a set of fast and scalable algorithms for segmenting very large 3D images of brain tissue. Currently, light and electron microscopy can now produce terascale 3D images within hours. Extracting the information about the shapes and connectivity of the neurons require fast and accurate image segmentation algorithms. Due to the sheer size of the problem, traditional approaches might be computationally infeasible. I focus on an segmentation pipeline that breaks up the segmentation problem into multiple stages, each of which can be improved independently. In the first step of the pipeline, convolutional neural networks are used to predict segment boundaries. Watershed transform is then used to obtain an over-segmentation, which is then reduced using agglomerative clustering algorithms. Finally, manual or computer-assisted proof reading is done by experts. In this thesis, I revisit the traditional approaches for training and applying convolutional neural networks, and propose: - A fast and scalable 3D convolutional network training algorithm suited for multi-core and many-core shared memory machines. The two main quantities of the algorithm are: (1) minimizing the required computation by using FFT-based convolution with memoization, and (2) parallelization approach that can utilize large number of CPUs while minimizing any required synchronization. - A high throughput inference algorithm that can utilize all available computational resources, CPUs and GPUs. I introduce a set of highly parallel algorithms for different layer types and architectures, and show how to combine them to achieve very high throughput. Additionally, I study the theoretical properties of the watershed transform of edge- weighed graphs and propose a liner-time algorithm. I propose a set of modification to the standard algorithm and a quasi-linear agglomerative clustering algorithm that can greatly reduce the over-segmentation produced by the standard watershed algorithm."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44903",
    "title": "Quantitative information-flow tracking for real systems",
    "abstract": "An information-flow security policy constrains a computer system's end-to-end use of information, even as it is transformed in computation. For instance, a policy would not just restrict what secret data could be revealed directly, but restrict any output that might allow inferences about the secret. Expressing such a policy quantitatively, in terms of a specific number of bits of information, is often an effective program independent way of distinguishing what scenarios should be allowed and disallowed. This thesis describes a family of new techniques for measuring how much information about a program's secret inputs is revealed by its public outputs on a particular execution, in order to check a quantitative policy on realistic systems. Our approach builds on dynamic tainting, tracking at runtime which bits might contain secret in formation, and also uses static control-flow regions to soundly account for implicit flows via branches and pointer operations. We introduce a new graph model that bounds information flow by the maximum flow between inputs and outputs in a flow network representation of an execution. The flow bounds obtained with maximum flow are much more precise than those based on tainting alone (which is equivalent to graph reachability). The bounds are a conservative estimate of channel capacity: the amount of information that could be transmitted by an adversary making an arbitrary choice of secret inputs. We describe an implementation named Flowcheck, built using the Valgrind framework for x86/Linux binaries, and use it to perform case studies on six real C, C++, and Objective C programs, three of which have more than 250,000 lines of code. We used the tool to check the confidentiality of a different kind of information appropriate to each program. Its results either verified that the information was appropriately kept secret on the examined executions, or revealed unacceptable leaks, in one case due to a previously unknown bug.",
    "advisors": ["Michael D. Ernst"],
    "text": "Quantitative information-flow tracking for real systems An information-flow security policy constrains a computer system's end-to-end use of information, even as it is transformed in computation. For instance, a policy would not just restrict what secret data could be revealed directly, but restrict any output that might allow inferences about the secret. Expressing such a policy quantitatively, in terms of a specific number of bits of information, is often an effective program independent way of distinguishing what scenarios should be allowed and disallowed. This thesis describes a family of new techniques for measuring how much information about a program's secret inputs is revealed by its public outputs on a particular execution, in order to check a quantitative policy on realistic systems. Our approach builds on dynamic tainting, tracking at runtime which bits might contain secret in formation, and also uses static control-flow regions to soundly account for implicit flows via branches and pointer operations. We introduce a new graph model that bounds information flow by the maximum flow between inputs and outputs in a flow network representation of an execution. The flow bounds obtained with maximum flow are much more precise than those based on tainting alone (which is equivalent to graph reachability). The bounds are a conservative estimate of channel capacity: the amount of information that could be transmitted by an adversary making an arbitrary choice of secret inputs. We describe an implementation named Flowcheck, built using the Valgrind framework for x86/Linux binaries, and use it to perform case studies on six real C, C++, and Objective C programs, three of which have more than 250,000 lines of code. We used the tool to check the confidentiality of a different kind of information appropriate to each program. Its results either verified that the information was appropriately kept secret on the examined executions, or revealed unacceptable leaks, in one case due to a previously unknown bug."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8117",
    "title": "Circuit design and technological limitations of silicon RFICs for wireless applications",
    "abstract": "Semiconductor technologies have been a key to the growth in wireless communication over the past decade, bringing added convenience and accessibility through advantages in cost, size, and power dissipation. A better understanding of how an IC technology affects critical RF signal chain components will greatly aid the design of wireless systems and the development of process technologies for the increasingly complex applications that lie on the horizon. Many of the evolving applications will embody the concept of adaptive performance to extract the maximum capability from the RF link in terms of bandwidth, dynamic range, and power consumption-further engaging the interplay of circuits and devices is this design space and making it even more difficult to discern a clear guide upon which to base technology decisions. Rooted in these observations, this research focuses on two key themes: 1) devising methods of implementing RF circuits which allow the performance to be dynamically tuned to match real-time conditions in a power-efficient manner, and 2) refining approaches for thinking about the optimization of RF circuits at the device level. Working toward a 5.8 GHz receiver consistent with 1 GBit/s operation, signal path topologies and adjustable biasing circuits are developed for low-noise amplifiers (LNAs) and voltage-controlled oscillators (VCOs) to provide a facility by which power can be conserved when the demand for sensitivity is low. As an integral component in this effort, tools for exploring device level issues are illustrated with both circuit types, helping to identify physical limitations and design techniques through which they can be mitigated.",
    "advisors": ["Charles G. Sodini"],
    "text": "Circuit design and technological limitations of silicon RFICs for wireless applications Semiconductor technologies have been a key to the growth in wireless communication over the past decade, bringing added convenience and accessibility through advantages in cost, size, and power dissipation. A better understanding of how an IC technology affects critical RF signal chain components will greatly aid the design of wireless systems and the development of process technologies for the increasingly complex applications that lie on the horizon. Many of the evolving applications will embody the concept of adaptive performance to extract the maximum capability from the RF link in terms of bandwidth, dynamic range, and power consumption-further engaging the interplay of circuits and devices is this design space and making it even more difficult to discern a clear guide upon which to base technology decisions. Rooted in these observations, this research focuses on two key themes: 1) devising methods of implementing RF circuits which allow the performance to be dynamically tuned to match real-time conditions in a power-efficient manner, and 2) refining approaches for thinking about the optimization of RF circuits at the device level. Working toward a 5.8 GHz receiver consistent with 1 GBit/s operation, signal path topologies and adjustable biasing circuits are developed for low-noise amplifiers (LNAs) and voltage-controlled oscillators (VCOs) to provide a facility by which power can be conserved when the demand for sensitivity is low. As an integral component in this effort, tools for exploring device level issues are illustrated with both circuit types, helping to identify physical limitations and design techniques through which they can be mitigated."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62424",
    "title": "Transforms for prediction residuals in video coding",
    "abstract": "Typically the same transform, the 2-D Discrete Cosine Transform (DCT), is used to compress both image intensities in image coding and prediction residuals in video coding. Major prediction residuals include the motion compensated prediction residual, the resolution enhancement residual in scalable video coding, and the intra prediction residual in intra-frame coding. The 2-D DCT is efficient at decorrelating images, but the spatial characteristics of prediction residuals can be significantly different from the spatial characteristics of images, and developing transforms that are adapted to the characteristics of prediction residuals can improve their compression efficiency. In this thesis, we explore the differences between the characteristics of images and prediction residuals by analyzing their local anisotropic characteristics and develop transforms adapted to the local anisotropic characteristics of some types of prediction residuals. The analysis shows that local regions in images have 2-D anisotropic characteristics and many regions in several types of prediction residuals have 1-D anisotropic characteristics. Based on this insight, we develop 1-D transforms for these residuals. We perform experiments to evaluate the potential gains achievable from using these transforms within the H.264 codec, and the experimental results indicate that these transforms can increase the compression efficiency of these residuals.",
    "advisors": ["Jae S. Lim"],
    "text": "Transforms for prediction residuals in video coding Typically the same transform, the 2-D Discrete Cosine Transform (DCT), is used to compress both image intensities in image coding and prediction residuals in video coding. Major prediction residuals include the motion compensated prediction residual, the resolution enhancement residual in scalable video coding, and the intra prediction residual in intra-frame coding. The 2-D DCT is efficient at decorrelating images, but the spatial characteristics of prediction residuals can be significantly different from the spatial characteristics of images, and developing transforms that are adapted to the characteristics of prediction residuals can improve their compression efficiency. In this thesis, we explore the differences between the characteristics of images and prediction residuals by analyzing their local anisotropic characteristics and develop transforms adapted to the local anisotropic characteristics of some types of prediction residuals. The analysis shows that local regions in images have 2-D anisotropic characteristics and many regions in several types of prediction residuals have 1-D anisotropic characteristics. Based on this insight, we develop 1-D transforms for these residuals. We perform experiments to evaluate the potential gains achievable from using these transforms within the H.264 codec, and the experimental results indicate that these transforms can increase the compression efficiency of these residuals."
}, {
    "id": "oai:dspace.mit.edu:1721.1/66469",
    "title": "Coincidence detection in the cochlear nucleus : implications for the coding of pitch",
    "abstract": "The spatio-temporal pattern in the auditory nerve (AN), i.e. the temporal pattern of AN fiber activity across the tonotopic axis, provides cues to important features in sounds such as pitch, loudness, and spatial location. These spatio-temporal cues may be extracted by central neurons in the cochlear nucleus (CN) that receive inputs from AN fibers innervating different cochlear regions and are sensitive to their relative timing. One possible mechanism for this extraction is cross-frequency coincidence detection (CD), in which a central neuron converts the degree of cross-frequency coincidence in the AN into a rate response by preferentially firing when its AN inputs across the tonotopic axis discharge in synchrony. We implemented a CD model receiving AN inputs from varying extents of the tonotopic axis, and compared responses of model CD cells with those of single units recorded in the CN of the anesthetized cat. We used Huffman stimuli, which have flat magnitude spectra and a single phase transition, to systematically manipulate the relative timing across AN fibers and to evaluate the sensitivity of model CD cells and CN units to the spatiotemporal pattern of AN discharges. Using a maximum likelihood approach, we found that certain unit types (primary-like-with-notch and some phase lockers) had responses consistent with cross-frequency CD cell. Some of these CN units provide input to neurons in a binaural circuit that process cues for sound localization and are sensitive to interaural level differences. A possible functional role of a cross-frequency CD mechanism in the CN is to increase the dynamic range of these binaural neurons. However, many other CN units had responses more consistent with AN fibers than with CD cells. We hypothesized that CN units resembling cross-frequency CD cells (as determined by their responses to Huffman stimuli) would convert spatio-temporal cues to pitch in the AN into rate cues that are robust with level. We found that, in response to harmonic complex tones, cross-frequency CD cells and some CN units (primary-like-with-notch and choppers) maintained robust rate cues at high levels compared to AN fibers, suggesting that at least some CN neurons extend the dynamic range of rate representations of pitch beyond that found in AN fibers. However, there was no obvious correlation between robust rate cues in individual CN units and similarity to cross-frequency CD cells as determined by responses to Huffman stimuli. It is likely that a model including more realistic inputs, membrane channels, and spiking mechanism, or other mechanisms such as lateral inhibition or spatial and temporal summation over spatially distributed inputs would provide insight into the neural mechanisms that give rise to the robust rate cues observed in some CN units.",
    "advisors": ["Bertrand Delgutte"],
    "text": "Coincidence detection in the cochlear nucleus : implications for the coding of pitch The spatio-temporal pattern in the auditory nerve (AN), i.e. the temporal pattern of AN fiber activity across the tonotopic axis, provides cues to important features in sounds such as pitch, loudness, and spatial location. These spatio-temporal cues may be extracted by central neurons in the cochlear nucleus (CN) that receive inputs from AN fibers innervating different cochlear regions and are sensitive to their relative timing. One possible mechanism for this extraction is cross-frequency coincidence detection (CD), in which a central neuron converts the degree of cross-frequency coincidence in the AN into a rate response by preferentially firing when its AN inputs across the tonotopic axis discharge in synchrony. We implemented a CD model receiving AN inputs from varying extents of the tonotopic axis, and compared responses of model CD cells with those of single units recorded in the CN of the anesthetized cat. We used Huffman stimuli, which have flat magnitude spectra and a single phase transition, to systematically manipulate the relative timing across AN fibers and to evaluate the sensitivity of model CD cells and CN units to the spatiotemporal pattern of AN discharges. Using a maximum likelihood approach, we found that certain unit types (primary-like-with-notch and some phase lockers) had responses consistent with cross-frequency CD cell. Some of these CN units provide input to neurons in a binaural circuit that process cues for sound localization and are sensitive to interaural level differences. A possible functional role of a cross-frequency CD mechanism in the CN is to increase the dynamic range of these binaural neurons. However, many other CN units had responses more consistent with AN fibers than with CD cells. We hypothesized that CN units resembling cross-frequency CD cells (as determined by their responses to Huffman stimuli) would convert spatio-temporal cues to pitch in the AN into rate cues that are robust with level. We found that, in response to harmonic complex tones, cross-frequency CD cells and some CN units (primary-like-with-notch and choppers) maintained robust rate cues at high levels compared to AN fibers, suggesting that at least some CN neurons extend the dynamic range of rate representations of pitch beyond that found in AN fibers. However, there was no obvious correlation between robust rate cues in individual CN units and similarity to cross-frequency CD cells as determined by responses to Huffman stimuli. It is likely that a model including more realistic inputs, membrane channels, and spiking mechanism, or other mechanisms such as lateral inhibition or spatial and temporal summation over spatially distributed inputs would provide insight into the neural mechanisms that give rise to the robust rate cues observed in some CN units."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35286",
    "title": "Low-complexity approaches to distributed data dissemination",
    "abstract": "In this thesis we consider practical ways of disseminating information from multiple senders to multiple receivers in an optimal or provably close-to-optimal fashion. The basis for our discussion of optimal transmission of information is mostly information theoretic - but the methods that we apply to do so in a low-complexity fashion draw from a number of different engineering disciplines. The three canonical multiple-input, multiple-output problems we focus our attention upon are: * The Slepian-Wolf problem where multiple correlated sources must be distributedly compressed and recovered with a common receiver. * The discrete memoryless multiple access problem where multiple senders communicate across a common channel to a single receiver. * The deterministic broadcast channel problem where multiple messages are sent from a common sender to multiple receivers through a deterministic medium. Chapter 1 serves as an introduction and provides models, definitions, and a discussion of barriers between theory and practice for the three canonical data dissemination problems we will discuss. Here we also discuss how these three problems are all in different senses 'dual' to each other, and use this as a motivating force to attack them with unifying themes.",
    "advisors": ["Muriel Médard"],
    "text": "Low-complexity approaches to distributed data dissemination In this thesis we consider practical ways of disseminating information from multiple senders to multiple receivers in an optimal or provably close-to-optimal fashion. The basis for our discussion of optimal transmission of information is mostly information theoretic - but the methods that we apply to do so in a low-complexity fashion draw from a number of different engineering disciplines. The three canonical multiple-input, multiple-output problems we focus our attention upon are: * The Slepian-Wolf problem where multiple correlated sources must be distributedly compressed and recovered with a common receiver. * The discrete memoryless multiple access problem where multiple senders communicate across a common channel to a single receiver. * The deterministic broadcast channel problem where multiple messages are sent from a common sender to multiple receivers through a deterministic medium. Chapter 1 serves as an introduction and provides models, definitions, and a discussion of barriers between theory and practice for the three canonical data dissemination problems we will discuss. Here we also discuss how these three problems are all in different senses 'dual' to each other, and use this as a motivating force to attack them with unifying themes."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45880",
    "title": "The paradigm of partial erasures",
    "abstract": "This thesis is a study of erasures in cryptographic protocols. Erasing old data and keys is an important capability of honest parties in cryptographic protocols. It is useful in many settings, including proactive security in the presence of a mobile adversary, adaptive security in the presence of an adaptive adversary, forward security, and intrusion resilience. Some of these settings, such as achieving proactive security, is provably impossible without some form of erasures. Other settings, such as designing protocols that are secure against adaptive adversaries, are much simpler to achieve when erasures are allowed. Protocols for all these contexts typically assume the ability to perfectly erase information. Unfortunately, as amply demonstrated in the systems literature, perfect erasures are hard to implement in practice. We propose a model of imperfect or partial erasures where erasure instructions are only partially effective and leave almost all the data intact, thus giving the honest parties only a limited capability to dispose old data. Nonetheless, we show how to design protocols for all of the above settings (including proactive security, adaptive security, forward security, and intrusion resilience) for which this weak form of erasures suffices. We do not have to invent entirely new protocols, but rather show how to automatically modify protocols relying on perfect erasures into ones for which partial erasures suffices. Stated most generally, we provide a compiler that transforms any protocol relying on perfect erasures for security into one with the same functionality that remains secure even if the erasures are only partial. The key idea is a new redundant representation of secret data which can still be computed on, and yet is rendered useless when partially erased. We prove that any such compiler must incur a cost in additional storage, and that our compiler is near optimal in terms of its storage overhead. We also give computationally more efficient compilers for a number of special cases: (1) when all the computations on secrets can be done in constant parallel time (NC⁰); (2) for a class of proactive secret sharing protocols where we leave the protocol intact except for changing the representation of the shares of the secret and the instructions that modify the shares (to correspondingly modify the new representation instead).",
    "advisors": ["Shafi Goldwasser", "Ran Canetti"],
    "text": "The paradigm of partial erasures This thesis is a study of erasures in cryptographic protocols. Erasing old data and keys is an important capability of honest parties in cryptographic protocols. It is useful in many settings, including proactive security in the presence of a mobile adversary, adaptive security in the presence of an adaptive adversary, forward security, and intrusion resilience. Some of these settings, such as achieving proactive security, is provably impossible without some form of erasures. Other settings, such as designing protocols that are secure against adaptive adversaries, are much simpler to achieve when erasures are allowed. Protocols for all these contexts typically assume the ability to perfectly erase information. Unfortunately, as amply demonstrated in the systems literature, perfect erasures are hard to implement in practice. We propose a model of imperfect or partial erasures where erasure instructions are only partially effective and leave almost all the data intact, thus giving the honest parties only a limited capability to dispose old data. Nonetheless, we show how to design protocols for all of the above settings (including proactive security, adaptive security, forward security, and intrusion resilience) for which this weak form of erasures suffices. We do not have to invent entirely new protocols, but rather show how to automatically modify protocols relying on perfect erasures into ones for which partial erasures suffices. Stated most generally, we provide a compiler that transforms any protocol relying on perfect erasures for security into one with the same functionality that remains secure even if the erasures are only partial. The key idea is a new redundant representation of secret data which can still be computed on, and yet is rendered useless when partially erased. We prove that any such compiler must incur a cost in additional storage, and that our compiler is near optimal in terms of its storage overhead. We also give computationally more efficient compilers for a number of special cases: (1) when all the computations on secrets can be done in constant parallel time (NC⁰); (2) for a class of proactive secret sharing protocols where we leave the protocol intact except for changing the representation of the shares of the secret and the instructions that modify the shares (to correspondingly modify the new representation instead)."
}, {
    "id": "oai:dspace.mit.edu:1721.1/38690",
    "title": "On the synthesis of switched output feedback controllers for linear, time-invariant systems",
    "abstract": "The theory of switching systems has seen many advances in the past decade. Its beginnings were founded primarily due to the physical limitations in devices to implement control such as relays, but today there exists a strong interest in the development of switching systems where switching is introduced as a means of increasing performance. With the newer set of problems that arise from this viewpoint comes the need for many new tools for analysis and design. Analysis tools which include, for instance, the celebrated work on multiple Lyapunov functions are extensive. Tools for the design of switched systems also exist, but, in many cases, the method of designing stabilizing switching laws is often a separate process from the method which is used to determine the set of vector fields between which switching takes place. For instance, one typical method of designing switching controllers for linear, time-invariant (LTI) systems is to first design a set of stabilizing LTI controllers using standard LTI methods, and then design a switching law to increase performance. While such design algorithms can lead to increases in performance, they often impose restrictions that do not allow the designer to take full advantage of the switching architecture being considered.",
    "advisors": ["Munther A. Dahleh"],
    "text": "On the synthesis of switched output feedback controllers for linear, time-invariant systems The theory of switching systems has seen many advances in the past decade. Its beginnings were founded primarily due to the physical limitations in devices to implement control such as relays, but today there exists a strong interest in the development of switching systems where switching is introduced as a means of increasing performance. With the newer set of problems that arise from this viewpoint comes the need for many new tools for analysis and design. Analysis tools which include, for instance, the celebrated work on multiple Lyapunov functions are extensive. Tools for the design of switched systems also exist, but, in many cases, the method of designing stabilizing switching laws is often a separate process from the method which is used to determine the set of vector fields between which switching takes place. For instance, one typical method of designing switching controllers for linear, time-invariant (LTI) systems is to first design a set of stabilizing LTI controllers using standard LTI methods, and then design a switching law to increase performance. While such design algorithms can lead to increases in performance, they often impose restrictions that do not allow the designer to take full advantage of the switching architecture being considered."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16896",
    "title": "Multiplexing, scheduling, and multicasting strategies for antenna arrays in wireless networks",
    "abstract": "A transmitter antenna array has the ability to direct data simultaneously to multiple receivers within a wireless network, creating potential for a more integrated view of algorithmic system components. In this thesis, such a perspective informs the design of two system tasks: the scheduling of packets from a number of data streams into groups; and the subsequent spatial multiplexing and encoding of these groups using array processing. We demonstrate how good system designs can help these two tasks reinforce one another, or alternatively enable tradeoffs in complexity between the two. Moreover, scheduling and array processing each benefit from a further awareness of both the fading channel state and certain properties of the data, providing information about key flexibilities, constraints and goals. Our development focuses on techniques that lead to high performance even with very low-complexity receivers. We first consider spatial precoding under simple scheduling and propose several extensions for implementation, such as a unified time-domain precoder that compensates for both cross-channel and intersymbol interfer- ence. We then show how more sophisticated, channel-aware scheduling can reduce the complexity requirements of the array processing. The scheduling algorithms presented are based on the receivers' fading channel realizations and the delay tolerances of the data streams. Finally, we address the multicasting of common data streams in terms of opportunities for reduced redundancy as well as the conflicting objectives inherent in sending to multiple receivers. Our channel-aware extensions of space-time codes for multicasting gain several dB over traditional versions that do not incorporate channel knowledge.",
    "advisors": ["George W. Wornell"],
    "text": "Multiplexing, scheduling, and multicasting strategies for antenna arrays in wireless networks A transmitter antenna array has the ability to direct data simultaneously to multiple receivers within a wireless network, creating potential for a more integrated view of algorithmic system components. In this thesis, such a perspective informs the design of two system tasks: the scheduling of packets from a number of data streams into groups; and the subsequent spatial multiplexing and encoding of these groups using array processing. We demonstrate how good system designs can help these two tasks reinforce one another, or alternatively enable tradeoffs in complexity between the two. Moreover, scheduling and array processing each benefit from a further awareness of both the fading channel state and certain properties of the data, providing information about key flexibilities, constraints and goals. Our development focuses on techniques that lead to high performance even with very low-complexity receivers. We first consider spatial precoding under simple scheduling and propose several extensions for implementation, such as a unified time-domain precoder that compensates for both cross-channel and intersymbol interfer- ence. We then show how more sophisticated, channel-aware scheduling can reduce the complexity requirements of the array processing. The scheduling algorithms presented are based on the receivers' fading channel realizations and the delay tolerances of the data streams. Finally, we address the multicasting of common data streams in terms of opportunities for reduced redundancy as well as the conflicting objectives inherent in sending to multiple receivers. Our channel-aware extensions of space-time codes for multicasting gain several dB over traditional versions that do not incorporate channel knowledge."
}, {
    "id": "oai:dspace.mit.edu:1721.1/17479",
    "title": "The design and testing of integrated circuits for submillimeter-wave spectroscopy",
    "abstract": "Optoelectronic techniques have extended the bandwidth of electronic spectroscopic systems to the submillimeter wavelengths. In a significant class of these systems the submillimeter-wave source, detector and device of interest are monolithically integrated. Such systems are attractive because of their reliability and small size and cost, because an integrated circuit is the highest-bandwidth environment for testing microelectronic devices, and because of their potential application to on-chip chemical and biological sensing. This thesis focuses on three separate topics in the field of submillimeter-wave spectroscopy with integrated circuits. The first topic is the decrease in bandwidth of photoconductive submillimeter wave emitters with increasing voltage bias, which limits the output power of these devices at frequencies near 1 THz. We performed measurements of a photoconductor made of low-temperature grown GaAs embedded in a coplanar waveguide with both static and dynamic illumination. We investigated the bandwidth decrease and an increase in de photocurrent that occurs at the same bias voltages. We attribute both phenomena to a reduction of the electron capture cross section of donor states due to electron heating and Coulomb-barrier lowering. The second topic is a novel circuit for ultrafast measurements with coplanar waveguide transmission lines. The circuit contains photoconductive switches that allow tunable generation and reception of a coplanar waveguide's two propagating modes. The circuit has fewer discontinuities than other circuits with similar capabilities and does not require air bridges. We show how the photoconductive switch can be biased to compensate for pump laser beam misalignment. The third topic is the first demonstration of an integrated circuit's use for submillimeter- wave frequency-domain spectroscopy. Such an application is attractive because of its inherently good frequency resolution, which is necessary for chemical and biological detection. The amplitude and phase of the measured spectrum of a circuit without a device under test agree with a model that takes into account circuit resonance, photoconductive-switch dynamics, and resistive loss. We discuss why photoconductive frequency-domain spectroscopy has an inherently lower output signal than similar time-domain spectroscopy, and how this drawback can be compensated for.",
    "advisors": ["Qing Hu"],
    "text": "The design and testing of integrated circuits for submillimeter-wave spectroscopy Optoelectronic techniques have extended the bandwidth of electronic spectroscopic systems to the submillimeter wavelengths. In a significant class of these systems the submillimeter-wave source, detector and device of interest are monolithically integrated. Such systems are attractive because of their reliability and small size and cost, because an integrated circuit is the highest-bandwidth environment for testing microelectronic devices, and because of their potential application to on-chip chemical and biological sensing. This thesis focuses on three separate topics in the field of submillimeter-wave spectroscopy with integrated circuits. The first topic is the decrease in bandwidth of photoconductive submillimeter wave emitters with increasing voltage bias, which limits the output power of these devices at frequencies near 1 THz. We performed measurements of a photoconductor made of low-temperature grown GaAs embedded in a coplanar waveguide with both static and dynamic illumination. We investigated the bandwidth decrease and an increase in de photocurrent that occurs at the same bias voltages. We attribute both phenomena to a reduction of the electron capture cross section of donor states due to electron heating and Coulomb-barrier lowering. The second topic is a novel circuit for ultrafast measurements with coplanar waveguide transmission lines. The circuit contains photoconductive switches that allow tunable generation and reception of a coplanar waveguide's two propagating modes. The circuit has fewer discontinuities than other circuits with similar capabilities and does not require air bridges. We show how the photoconductive switch can be biased to compensate for pump laser beam misalignment. The third topic is the first demonstration of an integrated circuit's use for submillimeter- wave frequency-domain spectroscopy. Such an application is attractive because of its inherently good frequency resolution, which is necessary for chemical and biological detection. The amplitude and phase of the measured spectrum of a circuit without a device under test agree with a model that takes into account circuit resonance, photoconductive-switch dynamics, and resistive loss. We discuss why photoconductive frequency-domain spectroscopy has an inherently lower output signal than similar time-domain spectroscopy, and how this drawback can be compensated for."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35530",
    "title": "Design, modeling, and simulation of a Compact Optoelectronic Neural Coprocessor",
    "abstract": "Microprocessors have substantially increased in speed and computational power over the past two decades. However, they still are unable to solve certain classes of problems efficiently, particularly those which involve the analysis of large noisy data sets such as the case of image processing, feature extraction, and pattern recognition. Substantial research has focused on using neural network algorithms to process this type of data with much success. Most of this effort, however, has resulted in sophisticated neural network-based software algorithms rather than physical neural network hardware. Consequently, most neural network-type processing systems today consist of neural algorithms running on traditional sequential (i.e. Intel-based) microprocessors rather than on actual neurocomputers, and thus achieve less than optimal performance. The objective of the Compact Optoelectronic Neural Coprocessor (CONCOP) project is to build a compact, pixilated, parallel optoelectronic processor capable of running neural network-type algorithms in native hardware.",
    "advisors": ["Cardinal Warde"],
    "text": "Design, modeling, and simulation of a Compact Optoelectronic Neural Coprocessor Microprocessors have substantially increased in speed and computational power over the past two decades. However, they still are unable to solve certain classes of problems efficiently, particularly those which involve the analysis of large noisy data sets such as the case of image processing, feature extraction, and pattern recognition. Substantial research has focused on using neural network algorithms to process this type of data with much success. Most of this effort, however, has resulted in sophisticated neural network-based software algorithms rather than physical neural network hardware. Consequently, most neural network-type processing systems today consist of neural algorithms running on traditional sequential (i.e. Intel-based) microprocessors rather than on actual neurocomputers, and thus achieve less than optimal performance. The objective of the Compact Optoelectronic Neural Coprocessor (CONCOP) project is to build a compact, pixilated, parallel optoelectronic processor capable of running neural network-type algorithms in native hardware."
}, {
    "id": "oai:dspace.mit.edu:1721.1/30158",
    "title": "Experimental study of a 1.5-MW, 110-GHz gyrotron oscillator",
    "abstract": "This thesis reports the design, construction and testing of a 1.5 MW, 110 GHz gyrotron oscillator. This high power microwave tube has been proposed as the next evolutionary step for gyrotrons used to provide electron cyclotron heating required in fusion devices. A short pulse gyrotron based on the industrial tube design was built at MIT for experimental studies. The experiments are the first demonstration of such high powers at 110 GHz. Using a 96 kV, 40 A electron beam, over 1.4 MW was axially extracted in the design (TE22,6) mode in 3 us pulses, corresponding to a microwave efficiency of 37 %. The beam alpha, the ratio of transverse to axial velocity in the electron beam, was measured with a probe. At the high efficiency operating point the beam alpha was measured as 1.33. This value of alpha is less than the design value of 1.4, possibly accounting for the slightly reduced experimental efficiency. The output power and efficiency, as a function of magnetic field, beam voltage, and beam current, are in good agreement with nonlinear theory and simulations with the MAGY code. In another phase of the experiment, a second tube was built and tested. This tube used the same gun and cavity but also incorporated an internal mode converter to transform the generated waveguide mode into a free-space propagating beam. The gun was tested to full power and current in the experiment. Preliminary results were obtained. A mode map was generated to locate the region of operating parameters for the design mode, as well as for neighboring modes. Scans of the output microwave beam were also taken using a power-detecting diode. Future work will focus on generating high power, as well as operating the collector at a depressed voltage for even higher efficiency. A study is also presented of the 96 kV, 40 A magnetron injection gun.",
    "advisors": ["Richard J. Temkin"],
    "text": "Experimental study of a 1.5-MW, 110-GHz gyrotron oscillator This thesis reports the design, construction and testing of a 1.5 MW, 110 GHz gyrotron oscillator. This high power microwave tube has been proposed as the next evolutionary step for gyrotrons used to provide electron cyclotron heating required in fusion devices. A short pulse gyrotron based on the industrial tube design was built at MIT for experimental studies. The experiments are the first demonstration of such high powers at 110 GHz. Using a 96 kV, 40 A electron beam, over 1.4 MW was axially extracted in the design (TE22,6) mode in 3 us pulses, corresponding to a microwave efficiency of 37 %. The beam alpha, the ratio of transverse to axial velocity in the electron beam, was measured with a probe. At the high efficiency operating point the beam alpha was measured as 1.33. This value of alpha is less than the design value of 1.4, possibly accounting for the slightly reduced experimental efficiency. The output power and efficiency, as a function of magnetic field, beam voltage, and beam current, are in good agreement with nonlinear theory and simulations with the MAGY code. In another phase of the experiment, a second tube was built and tested. This tube used the same gun and cavity but also incorporated an internal mode converter to transform the generated waveguide mode into a free-space propagating beam. The gun was tested to full power and current in the experiment. Preliminary results were obtained. A mode map was generated to locate the region of operating parameters for the design mode, as well as for neighboring modes. Scans of the output microwave beam were also taken using a power-detecting diode. Future work will focus on generating high power, as well as operating the collector at a depressed voltage for even higher efficiency. A study is also presented of the 96 kV, 40 A magnetron injection gun."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117841",
    "title": "Derivation, experimental verification, and applications of a new color image model",
    "abstract": "Image modeling is an important area of image processing. Good image models are useful, for example, in image restoration problems because they provide constraints that can be imposed on degraded images to retrieve better approximations of the original image. Many physical models of images are separable functions of position and wavelength, which means that images can be written as a color independent local average multiplied by a color dependent residual. We will present experimental results showing that this is the case in practice, and discuss the limitations of this model. We will also show that several commonly used observations in image processing follow from this model. Finally, we will demonstrate the results of imposing the model constraints in several image denoising problems and show that degraded images can be improved by imposing the model constraints.",
    "advisors": ["Jae S. Lim"],
    "text": "Derivation, experimental verification, and applications of a new color image model Image modeling is an important area of image processing. Good image models are useful, for example, in image restoration problems because they provide constraints that can be imposed on degraded images to retrieve better approximations of the original image. Many physical models of images are separable functions of position and wavelength, which means that images can be written as a color independent local average multiplied by a color dependent residual. We will present experimental results showing that this is the case in practice, and discuss the limitations of this model. We will also show that several commonly used observations in image processing follow from this model. Finally, we will demonstrate the results of imposing the model constraints in several image denoising problems and show that degraded images can be improved by imposing the model constraints."
}, {
    "id": "oai:dspace.mit.edu:1721.1/38924",
    "title": "Tiled microprocessors",
    "abstract": "Current-day microprocessors have reached the point of diminishing returns due to inherent scalability limitations. This thesis examines the tiled microprocessor, a class of microprocessor which is physically scalable but inherits many of the desirable properties of conventional microprocessors. Tiled microprocessors are composed of an array of replicated tiles connected by a special class of network, the Scalar Operand Network (SON), which is optimized for low-latency, low-occupancy communication between remote ALUs on different tiles. Tiled microprocessors can be constructed to scale to 100's or 1000's of functional units. This thesis identifies seven key criteria for achieving physical scalability in tiled microprocessors. It employs an archetypal tiled microprocessor to examine the challenges in achieving these criteria and to explore the properties of Scalar Operand Networks. The thesis develops the field of SONs in three major ways: it introduces the 5-tuple performance metric, it describes a complete, high-frequency <0,0,1,2,0> SON implementation, and it proposes a taxonomy, called AsTrO, for categorizing them.",
    "advisors": ["Anant Agarwal"],
    "text": "Tiled microprocessors Current-day microprocessors have reached the point of diminishing returns due to inherent scalability limitations. This thesis examines the tiled microprocessor, a class of microprocessor which is physically scalable but inherits many of the desirable properties of conventional microprocessors. Tiled microprocessors are composed of an array of replicated tiles connected by a special class of network, the Scalar Operand Network (SON), which is optimized for low-latency, low-occupancy communication between remote ALUs on different tiles. Tiled microprocessors can be constructed to scale to 100's or 1000's of functional units. This thesis identifies seven key criteria for achieving physical scalability in tiled microprocessors. It employs an archetypal tiled microprocessor to examine the challenges in achieving these criteria and to explore the properties of Scalar Operand Networks. The thesis develops the field of SONs in three major ways: it introduces the 5-tuple performance metric, it describes a complete, high-frequency <0,0,1,2,0> SON implementation, and it proposes a taxonomy, called AsTrO, for categorizing them."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8228",
    "title": "Cellular computation and communications using engineered genetic regulatory networks",
    "abstract": "In this thesis, I present an engineering discipline for obtaining complex, predictable, and reliable cell behaviors by embedding biochemical logic circuits and programmed intercellular communications into cells. To accomplish this goal, I provide a well-characterized component library, a biocircuit design methodology, and software design tools. I have built and characterized an initial cellular gate library with biochemical gates that implement the NOT, IMPLIES, and AND logic functions in E. coli cells. The logic gates perform computation using DNA-binding proteins, small molecules that interact with these proteins, and segments of DNA that regulate the expression of the proteins. I introduce genetic process engineering, a methodology for modifying the DNA encoding of existing genetic elements to achieve the desired input/output behavior for constructing reliable circuits of significant complexity. I demonstrate the feasibility of digital computation in cells by building several operational in-vivo digital logic circuits, each composed of three gates that have been optimized by genetic process engineering.",
    "advisors": ["Thomas F. Knight", " Jr., Gerald Jay Sussman", "Harold Abelson"],
    "text": "Cellular computation and communications using engineered genetic regulatory networks In this thesis, I present an engineering discipline for obtaining complex, predictable, and reliable cell behaviors by embedding biochemical logic circuits and programmed intercellular communications into cells. To accomplish this goal, I provide a well-characterized component library, a biocircuit design methodology, and software design tools. I have built and characterized an initial cellular gate library with biochemical gates that implement the NOT, IMPLIES, and AND logic functions in E. coli cells. The logic gates perform computation using DNA-binding proteins, small molecules that interact with these proteins, and segments of DNA that regulate the expression of the proteins. I introduce genetic process engineering, a methodology for modifying the DNA encoding of existing genetic elements to achieve the desired input/output behavior for constructing reliable circuits of significant complexity. I demonstrate the feasibility of digital computation in cells by building several operational in-vivo digital logic circuits, each composed of three gates that have been optimized by genetic process engineering."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113998",
    "title": "Extreme imaging via physical model inversion : seeing around corners and imaging black holes",
    "abstract": "Imaging often plays a critical role in advancing fundamental science. However, as science continues to push the boundaries of knowledge, imaging systems are reaching the limits of what can be measured using traditional-direct approaches. By designing systems that tightly integrate novel sensor and algorithm design, it may be possible to develop imaging systems that exceed fundamental theoretical limitations to observe things previously impossible to see. However, these non-traditional imaging systems generally come with a trade-off; they produce increasingly sparse and/or noisy measurements that require incorporating additional structure to extract anything meaningful. The focus of this thesis is on using computational methods that exploit structure in our universe to move past these obstacles and reveal the invisible. In this thesis, we focus on two imaging problems that explicitly leverage structure in our universe: reconstructing images and video from a computational telescope the size of the Earth, and seeing around corners. For the first imaging problem, this thesis investigates ways to reconstruct images and video from a sparse telescope array distributed around the globe. Additionally, it presents a number of evaluation techniques developed to rigorously evaluate imaging methods in order to establish confidence in reconstructions done with real scientific data. The methods and evaluation techniques developed in this thesis will hopefully aid in ongoing work to take the first picture of a black hole. Next, this thesis presents methods developed for using the subtle spatio-temporal radiance variations that arise on the ground at the base of an edge to construct a one-dimensional video of a hidden scene. These methods may be especially valuable in remotely sensing occupants in a room during search and rescue operations, or in detecting hidden, oncoming vehicles and/or pedestrians for collision avoidance systems.",
    "advisors": ["William T. Freeman"],
    "text": "Extreme imaging via physical model inversion : seeing around corners and imaging black holes Imaging often plays a critical role in advancing fundamental science. However, as science continues to push the boundaries of knowledge, imaging systems are reaching the limits of what can be measured using traditional-direct approaches. By designing systems that tightly integrate novel sensor and algorithm design, it may be possible to develop imaging systems that exceed fundamental theoretical limitations to observe things previously impossible to see. However, these non-traditional imaging systems generally come with a trade-off; they produce increasingly sparse and/or noisy measurements that require incorporating additional structure to extract anything meaningful. The focus of this thesis is on using computational methods that exploit structure in our universe to move past these obstacles and reveal the invisible. In this thesis, we focus on two imaging problems that explicitly leverage structure in our universe: reconstructing images and video from a computational telescope the size of the Earth, and seeing around corners. For the first imaging problem, this thesis investigates ways to reconstruct images and video from a sparse telescope array distributed around the globe. Additionally, it presents a number of evaluation techniques developed to rigorously evaluate imaging methods in order to establish confidence in reconstructions done with real scientific data. The methods and evaluation techniques developed in this thesis will hopefully aid in ongoing work to take the first picture of a black hole. Next, this thesis presents methods developed for using the subtle spatio-temporal radiance variations that arise on the ground at the base of an edge to construct a one-dimensional video of a hidden scene. These methods may be especially valuable in remotely sensing occupants in a room during search and rescue operations, or in detecting hidden, oncoming vehicles and/or pedestrians for collision avoidance systems."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34021",
    "title": "Dynamic resource allocation in WDM networks with optical bypass and waveband switching",
    "abstract": "In this thesis, we investigate network architecture from the twin perspectives of link resource allocation and node complexity in WDM optical networks Chapter 2 considers networks where the nodes have full wavelength accessibility, and investigates link resource allocation in ring networks in the form of the routing and wavelength assignment problem. In a ring network with N nodes and P calls allowed per node, we show that a necessary and sufficient lower bound on the number of wavelengths required for rearrangeably non-blocking traffic is PN/41 wavelengths. Two novel algorithms are presented: one that achieves this lower bound using at most two converters per wavelength, and a second requiring 2PN/71 wavelengths that requires significantly fewer wavelength converters. Chapter 3 begins our investigation of the role of reduced-complexity nodes in WDM networks by considering networks with optical bypass. The ring, torus, and tree architectures are considered. For the ring, an optical bypass architecture is constructed that requires the minimum number of locally-accessible wavelengths, with the remaining wavelengths bypassing all but a small number of hub nodes. The routing and wavelength assignment for all non-hub nodes is statically assigned, and these nodes do not require dynamic switching capability.",
    "advisors": ["Eytan Modiano"],
    "text": "Dynamic resource allocation in WDM networks with optical bypass and waveband switching In this thesis, we investigate network architecture from the twin perspectives of link resource allocation and node complexity in WDM optical networks Chapter 2 considers networks where the nodes have full wavelength accessibility, and investigates link resource allocation in ring networks in the form of the routing and wavelength assignment problem. In a ring network with N nodes and P calls allowed per node, we show that a necessary and sufficient lower bound on the number of wavelengths required for rearrangeably non-blocking traffic is PN/41 wavelengths. Two novel algorithms are presented: one that achieves this lower bound using at most two converters per wavelength, and a second requiring 2PN/71 wavelengths that requires significantly fewer wavelength converters. Chapter 3 begins our investigation of the role of reduced-complexity nodes in WDM networks by considering networks with optical bypass. The ring, torus, and tree architectures are considered. For the ring, an optical bypass architecture is constructed that requires the minimum number of locally-accessible wavelengths, with the remaining wavelengths bypassing all but a small number of hub nodes. The routing and wavelength assignment for all non-hub nodes is statically assigned, and these nodes do not require dynamic switching capability."
}, {
    "id": "oai:dspace.mit.edu:1721.1/111904",
    "title": "Energy scalable systems for 2D and 3D low-power ultrasound beamforming",
    "abstract": "In traditional ultrasound imaging systems, bulky and power-intensive mainframes are used to process the high number of waveforms acquired in parallel from a large transducer array. The computational power of these systems scales linearly with transducer count. However, there exist applications where basic functionality in low-power conditions may be favorable to an \"all-or-nothing\" system that only produces a high resolution image when enough power is supplied. This thesis presents systems designed to support energy-scalability at run-time, enabling the user to make the tradeoff between power and performance. First, a system-level energy model for a receive-side digital beamforming system is presented. Power-performance tradeoffs for the analog front-end, analog-to-digital converter, and digital beamformer are analyzed individually and then combined to account for the performance dependency between the functional components. These considerations inform a recommendation on design choices for the end-to-end system. Second, this thesis describes an energy-scalable 2-D beamformer that provides user-controlled run-time tradeoff between image quality and energy consumption. Architectural design choices that enable three operating modes are discussed. A test chip was fabricated in 65-nm low power CMOS technology. It can operate with functional correctness at 0.49 V, with a measured power of 185 [mu]W in real-time operation at 0.52 V. Finally, a software-based energy-scalable 3-D ultrasound beamformer is implemented on an embedded supercomputer. The energy consumption and corresponding imaging quality are measured and compared.",
    "advisors": ["Anantha P. Chandrakasan", "Gerald J. Sussman"],
    "text": "Energy scalable systems for 2D and 3D low-power ultrasound beamforming In traditional ultrasound imaging systems, bulky and power-intensive mainframes are used to process the high number of waveforms acquired in parallel from a large transducer array. The computational power of these systems scales linearly with transducer count. However, there exist applications where basic functionality in low-power conditions may be favorable to an \"all-or-nothing\" system that only produces a high resolution image when enough power is supplied. This thesis presents systems designed to support energy-scalability at run-time, enabling the user to make the tradeoff between power and performance. First, a system-level energy model for a receive-side digital beamforming system is presented. Power-performance tradeoffs for the analog front-end, analog-to-digital converter, and digital beamformer are analyzed individually and then combined to account for the performance dependency between the functional components. These considerations inform a recommendation on design choices for the end-to-end system. Second, this thesis describes an energy-scalable 2-D beamformer that provides user-controlled run-time tradeoff between image quality and energy consumption. Architectural design choices that enable three operating modes are discussed. A test chip was fabricated in 65-nm low power CMOS technology. It can operate with functional correctness at 0.49 V, with a measured power of 185 [mu]W in real-time operation at 0.52 V. Finally, a software-based energy-scalable 3-D ultrasound beamformer is implemented on an embedded supercomputer. The energy consumption and corresponding imaging quality are measured and compared."
}, {
    "id": "oai:dspace.mit.edu:1721.1/40514",
    "title": "Supramolecular architectures for neural prostheses",
    "abstract": "Neural prosthetic devices offer a means of restoring function that have been lost due to neural damage. The first part of this thesis investigates the design of a 15-channel, low-power, fully implantable stimulator chip. The chip is powered wirelessly and receives wireless commands. The chip features a CMOS only ASK detector, a single-differential converter based on a novel feedback loop, a low-power adaptive bandwidth DLL and 15 programmable current sources that can be controlled via four commands. Though it is feasible to build an implantable stimulator chip, the amount of power required to stimulate more than 16 channels is prohibitively large. Clearly, there is a need for a fundamentally different approach. The ultimate challenge is to design a self-sufficient neural interface. The ideal device will lend itself to seamless integration with the existing neural architecture. This necessitates that communication with the neural tissue should be performed via chemical rather than electrical messages. However, catastrophic destruction of neural tissue due to the release of large quantities of a neuroactive species, like neurotransmitters, precludes the storage of quantities large enough to suffice for the lifetime of the device. The ideal device then should actively sequester the chemical species from the body and release it upon receiving appropriate triggers in a power efficient manner. This thesis proposes the use of ionic gradients, specifically K+ ions as an alternative chemical stimulation method. The required ions can readily be sequestered from the background extracellular fluid. The parameters of using such a stimulation technique are first established by performing in-vitro experiments on rabbit retinas. The results show that modest increases (~~10mM) of K+ ions are sufficient to elicit a neural response.",
    "advisors": ["Marc A. Baldo"],
    "text": "Supramolecular architectures for neural prostheses Neural prosthetic devices offer a means of restoring function that have been lost due to neural damage. The first part of this thesis investigates the design of a 15-channel, low-power, fully implantable stimulator chip. The chip is powered wirelessly and receives wireless commands. The chip features a CMOS only ASK detector, a single-differential converter based on a novel feedback loop, a low-power adaptive bandwidth DLL and 15 programmable current sources that can be controlled via four commands. Though it is feasible to build an implantable stimulator chip, the amount of power required to stimulate more than 16 channels is prohibitively large. Clearly, there is a need for a fundamentally different approach. The ultimate challenge is to design a self-sufficient neural interface. The ideal device will lend itself to seamless integration with the existing neural architecture. This necessitates that communication with the neural tissue should be performed via chemical rather than electrical messages. However, catastrophic destruction of neural tissue due to the release of large quantities of a neuroactive species, like neurotransmitters, precludes the storage of quantities large enough to suffice for the lifetime of the device. The ideal device then should actively sequester the chemical species from the body and release it upon receiving appropriate triggers in a power efficient manner. This thesis proposes the use of ionic gradients, specifically K+ ions as an alternative chemical stimulation method. The required ions can readily be sequestered from the background extracellular fluid. The parameters of using such a stimulation technique are first established by performing in-vitro experiments on rabbit retinas. The results show that modest increases (~~10mM) of K+ ions are sufficient to elicit a neural response."
}, {
    "id": "oai:dspace.mit.edu:1721.1/89995",
    "title": "Linguistically motivated models for lightly-supervised dependency parsing",
    "abstract": "Today, the top performing parsing algorithms rely on the availability of annotated data for learning the syntactic structure of a language. Unfortunately, syntactically annotated texts are available only for a handful of languages. The research presented in this thesis aims at developing parsing models that can effectively perform in a lightly-supervised training regime. In particular we focus on formulating linguistically aware models of dependency parsing that can exploit readily available sources of linguistic knowledge such as language universals and typological features. This type of linguistic knowledge can be used to motivate model design and/or to guide inference procedure. We propose three alternative approaches for incorporating linguistic information into a lightly-supervised training setup: First, we show that linguistic information can be used in the form of rules on top of standard unsupervised parsing models to guide inference procedure. This method consistently outperforms existing monolingual and multilingual unsupervised parsers when tested on a set of 6 Indo-European languages. Next, we show that a linguistically aware model design greatly facilitates crosslingual parser transfer by leveraging syntactic connections between languages. Our transfer approach outperforms the state-of-the-art multilingual transfer parser across a set of 19 languages, achieving an average gain of 5.9%. The gains are even more pronounced - 14.4% - on non-Indo-European languages where existing transfer methods fail to perform. Finally, we propose a corpus-level Bayesian framework that allows multiple views of data in a single model. We use this framework to combine a dependency model with constituency view and universal rules, achieving a performance gain of 1.9% compared to the top-performing unsupervised parsing model.",
    "advisors": ["Regina Barzilay"],
    "text": "Linguistically motivated models for lightly-supervised dependency parsing Today, the top performing parsing algorithms rely on the availability of annotated data for learning the syntactic structure of a language. Unfortunately, syntactically annotated texts are available only for a handful of languages. The research presented in this thesis aims at developing parsing models that can effectively perform in a lightly-supervised training regime. In particular we focus on formulating linguistically aware models of dependency parsing that can exploit readily available sources of linguistic knowledge such as language universals and typological features. This type of linguistic knowledge can be used to motivate model design and/or to guide inference procedure. We propose three alternative approaches for incorporating linguistic information into a lightly-supervised training setup: First, we show that linguistic information can be used in the form of rules on top of standard unsupervised parsing models to guide inference procedure. This method consistently outperforms existing monolingual and multilingual unsupervised parsers when tested on a set of 6 Indo-European languages. Next, we show that a linguistically aware model design greatly facilitates crosslingual parser transfer by leveraging syntactic connections between languages. Our transfer approach outperforms the state-of-the-art multilingual transfer parser across a set of 19 languages, achieving an average gain of 5.9%. The gains are even more pronounced - 14.4% - on non-Indo-European languages where existing transfer methods fail to perform. Finally, we propose a corpus-level Bayesian framework that allows multiple views of data in a single model. We use this framework to combine a dependency model with constituency view and universal rules, achieving a performance gain of 1.9% compared to the top-performing unsupervised parsing model."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44412",
    "title": "A comparison of parallel Gaussian elimination solvers for the computation of electrochemical battery models on the cell processor",
    "abstract": "The rising cost of fossil fuels, together with a push for more eco-friendly methods of transportation, has increased interest in and demand for electrically powered or assisted vehicles. The majority of these electric or hybrid electric vehicles will be, for the foreseeable future, powered by batteries. One of the major problems with batteries is their aging. For batteries, aging means that the maximum charge they can store decreases as number of charge/discharge cycles increases. Aging also means that after a certain number of charge/discharge cycles, the battery will fail. In lead-acid batteries, one of the major phenomenon that promotes battery failure is the development of a non-uniform concentration gradient of electrolyte along the electrodes' height. This phenomenon is known as electrolyte stratification. This thesis develops a simple two-level circuit model that can be used to model electrolyte stratification. The two-level circuit model is justified experimentally using digital Mach-Zehnder interferometry and is explained theoretically by means of two different electrochemical battery models. The experiments show how the usage of the electrode varies along its height while the simulations indicate that the high resistivity of the lead dioxide electrode plays a major role in the development of a stratified electrolyte. Finally, computational issues associated with the computation of a sophisticated two dimensional electrochemical battery model on the multicore Cell Broadband Engine processor are addressed in detail. In particular, three different banded parallel Gaussian elimination solvers are developed and compared. These three solvers vividly illustrate how performance achieved on the new multicore processors is strongly dependent on the algorithm used.",
    "advisors": ["John L. Wyatt", "Thomas A. Keim"],
    "text": "A comparison of parallel Gaussian elimination solvers for the computation of electrochemical battery models on the cell processor The rising cost of fossil fuels, together with a push for more eco-friendly methods of transportation, has increased interest in and demand for electrically powered or assisted vehicles. The majority of these electric or hybrid electric vehicles will be, for the foreseeable future, powered by batteries. One of the major problems with batteries is their aging. For batteries, aging means that the maximum charge they can store decreases as number of charge/discharge cycles increases. Aging also means that after a certain number of charge/discharge cycles, the battery will fail. In lead-acid batteries, one of the major phenomenon that promotes battery failure is the development of a non-uniform concentration gradient of electrolyte along the electrodes' height. This phenomenon is known as electrolyte stratification. This thesis develops a simple two-level circuit model that can be used to model electrolyte stratification. The two-level circuit model is justified experimentally using digital Mach-Zehnder interferometry and is explained theoretically by means of two different electrochemical battery models. The experiments show how the usage of the electrode varies along its height while the simulations indicate that the high resistivity of the lead dioxide electrode plays a major role in the development of a stratified electrolyte. Finally, computational issues associated with the computation of a sophisticated two dimensional electrochemical battery model on the multicore Cell Broadband Engine processor are addressed in detail. In particular, three different banded parallel Gaussian elimination solvers are developed and compared. These three solvers vividly illustrate how performance achieved on the new multicore processors is strongly dependent on the algorithm used."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16612",
    "title": "Predictive multiple sampling algorithm with overlapping integration intervals for linear wide dynamic range integrating image sensors",
    "abstract": "Machine vision systems are used in a wide range of applications such as security, automated quality control and intelligent transportation systems. Several of these systems need to extract information from natural scenes in the section of the electromagnetic spectrum visible to humans. These scenes can easily have intra-frame illumination ratios in excess of 10⁶ : 1. Solid-state image sensors that can correctly process wide illumination dynamic range scenes are therefore required to ensure correct reliability and performance. This thesis describes a new algorithm to linearly increase the illumination dynamic range of integrating-type image sensors. A user-defined integration time is taken as a reference to create a potentially large set of integration intervals of different duration (the selected integration time being the longest) but with a common end. The light intensity received by each pixel in the sensing array is used to choose the optimal integration interval from the set, while a pixel saturation predictive decision is used to overlap the integration intervals within the given integration time such that only one frame using the optimal integration interval for each pixel is produced. The total integration time is never exceeded. Benefits from this approach are motion minimization, real-time operation, reduced memory requirements, programmable light intensity dynamic range increase and access to incremental light intensity information during the integration time.",
    "advisors": ["Charles G. Sodini"],
    "text": "Predictive multiple sampling algorithm with overlapping integration intervals for linear wide dynamic range integrating image sensors Machine vision systems are used in a wide range of applications such as security, automated quality control and intelligent transportation systems. Several of these systems need to extract information from natural scenes in the section of the electromagnetic spectrum visible to humans. These scenes can easily have intra-frame illumination ratios in excess of 10⁶ : 1. Solid-state image sensors that can correctly process wide illumination dynamic range scenes are therefore required to ensure correct reliability and performance. This thesis describes a new algorithm to linearly increase the illumination dynamic range of integrating-type image sensors. A user-defined integration time is taken as a reference to create a potentially large set of integration intervals of different duration (the selected integration time being the longest) but with a common end. The light intensity received by each pixel in the sensing array is used to choose the optimal integration interval from the set, while a pixel saturation predictive decision is used to overlap the integration intervals within the given integration time such that only one frame using the optimal integration interval for each pixel is produced. The total integration time is never exceeded. Benefits from this approach are motion minimization, real-time operation, reduced memory requirements, programmable light intensity dynamic range increase and access to incremental light intensity information during the integration time."
}, {
    "id": "oai:dspace.mit.edu:1721.1/66009",
    "title": "Subspace and graph methods to leverage auxiliary data for limited target data multi-class classification, applied to speaker verification",
    "abstract": "Multi-class classification can be adversely affected by the absence of sufficient target (in-class) instances for training. Such cases arise in face recognition, speaker verification, and document classification, among others. Auxiliary data-sets, which contain a diverse sampling of non-target instances, are leveraged in this thesis using subspace and graph methods to improve classification where target data is limited. The auxiliary data is used to define a compact representation that maps instances into a vector space where inner products quantify class similarity. Within this space, an estimate of the subspace that constitutes within-class variability (e.g. the recording channel in speaker verification or the illumination conditions in face recognition) can be obtained using class-labeled auxiliary data. This thesis proposes a way to incorporate this estimate into the SVM framework to perform nuisance compensation, thus improving classification performance. Another contribution is a framework that combines mapping and compensation into a single linear comparison, which motivates computationally inexpensive and accurate comparison functions. A key aspect of the work takes advantage of efficient pairwise comparisons between the training, test, and auxiliary instances to characterize their interaction within the vector space, and exploits it for improved classification in three ways. The first uses the local variability around the train and test instances to reduce false-alarms. The second assumes the instances lie on a low-dimensional manifold and uses the distances along the manifold. The third extracts relational features from a similarity graph where nodes correspond to the training, test and auxiliary instances. To quantify the merit of the proposed techniques, results of experiments in speaker verification are presented where only a single target recording is provided to train the classifier. Experiments are preformed on standard NIST corpora and methods are compared using standard evalutation metrics: detection error trade-off curves, minimum decision costs, and equal error rates.",
    "advisors": ["William M. Campbell", "Alan V. Oppenheim"],
    "text": "Subspace and graph methods to leverage auxiliary data for limited target data multi-class classification, applied to speaker verification Multi-class classification can be adversely affected by the absence of sufficient target (in-class) instances for training. Such cases arise in face recognition, speaker verification, and document classification, among others. Auxiliary data-sets, which contain a diverse sampling of non-target instances, are leveraged in this thesis using subspace and graph methods to improve classification where target data is limited. The auxiliary data is used to define a compact representation that maps instances into a vector space where inner products quantify class similarity. Within this space, an estimate of the subspace that constitutes within-class variability (e.g. the recording channel in speaker verification or the illumination conditions in face recognition) can be obtained using class-labeled auxiliary data. This thesis proposes a way to incorporate this estimate into the SVM framework to perform nuisance compensation, thus improving classification performance. Another contribution is a framework that combines mapping and compensation into a single linear comparison, which motivates computationally inexpensive and accurate comparison functions. A key aspect of the work takes advantage of efficient pairwise comparisons between the training, test, and auxiliary instances to characterize their interaction within the vector space, and exploits it for improved classification in three ways. The first uses the local variability around the train and test instances to reduce false-alarms. The second assumes the instances lie on a low-dimensional manifold and uses the distances along the manifold. The third extracts relational features from a similarity graph where nodes correspond to the training, test and auxiliary instances. To quantify the merit of the proposed techniques, results of experiments in speaker verification are presented where only a single target recording is provided to train the classifier. Experiments are preformed on standard NIST corpora and methods are compared using standard evalutation metrics: detection error trade-off curves, minimum decision costs, and equal error rates."
}, {
    "id": "oai:dspace.mit.edu:1721.1/47777",
    "title": "Building dependability arguments for software intensive systems",
    "abstract": "A method is introduced for structuring and guiding the development of end-to-end dependability arguments. The goal is to establish high-level requirements of complex software-intensive systems, especially properties that cross-cut normal functional decomposition. The resulting argument documents and validates the justification of system-level claims by tracing them down to component-level substantiation, such as automatic code analysis or cryptographic proofs. The method is evaluated on case studies drawn from the Burr Proton Therapy Center, operating at Massachusetts General Hospital, and on the Pret a Voter cryptographic voting system, developed at the University of Newcastle.",
    "advisors": ["Daniel Jackson"],
    "text": "Building dependability arguments for software intensive systems A method is introduced for structuring and guiding the development of end-to-end dependability arguments. The goal is to establish high-level requirements of complex software-intensive systems, especially properties that cross-cut normal functional decomposition. The resulting argument documents and validates the justification of system-level claims by tracing them down to component-level substantiation, such as automatic code analysis or cryptographic proofs. The method is evaluated on case studies drawn from the Burr Proton Therapy Center, operating at Massachusetts General Hospital, and on the Pret a Voter cryptographic voting system, developed at the University of Newcastle."
}, {
    "id": "oai:dspace.mit.edu:1721.1/82367",
    "title": "Stochastic methods for large-scale linear problems, variational inequalities, and convex optimization",
    "abstract": "This thesis considers stochastic methods for large-scale linear systems, variational inequalities, and convex optimization problems. I focus on special structures that lend themselves to sampling, such as when the linear/nonlinear mapping or the objective function is an expected value or is the sum of a large number of terms, and/or the constraint is the intersection of a large number of simpler sets. For linear systems, I propose modifications to deterministic methods to allow the use of random samples and maintain the stochastic convergence, which is particularly challenging when the unknown system is singular or nearly singular. For variational inequalities and optimization problems, I propose a class of methods that combine elements of incremental constraint projection, stochastic gradient/ subgradient descent, and proximal algorithm. These methods can be applied with various sampling schemes that are suitable for applications involving distributed implementation, large data set, or online learning. I use a unified framework to analyze the convergence and the rate of convergence of these methods. This framework is based on a pair of supermartingale bounds, which control the convergence to feasibility and the convergence to optimality, respectively, and are coupled at different time scales.",
    "advisors": ["Dimitri P. Bertsekas"],
    "text": "Stochastic methods for large-scale linear problems, variational inequalities, and convex optimization This thesis considers stochastic methods for large-scale linear systems, variational inequalities, and convex optimization problems. I focus on special structures that lend themselves to sampling, such as when the linear/nonlinear mapping or the objective function is an expected value or is the sum of a large number of terms, and/or the constraint is the intersection of a large number of simpler sets. For linear systems, I propose modifications to deterministic methods to allow the use of random samples and maintain the stochastic convergence, which is particularly challenging when the unknown system is singular or nearly singular. For variational inequalities and optimization problems, I propose a class of methods that combine elements of incremental constraint projection, stochastic gradient/ subgradient descent, and proximal algorithm. These methods can be applied with various sampling schemes that are suitable for applications involving distributed implementation, large data set, or online learning. I use a unified framework to analyze the convergence and the rate of convergence of these methods. This framework is based on a pair of supermartingale bounds, which control the convergence to feasibility and the convergence to optimality, respectively, and are coupled at different time scales."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8230",
    "title": "Intelligence by design : principles of modularity and coordination for engineering complex adaptive agents",
    "abstract": "All intelligence relies on search - for example, the search for an intelligent agent's next action. Search is only likely to succeed in resource-bounded agents if they have already been biased towards finding the right answer. In artificial agents, the primary source of bias is engineering. This dissertation describes an approach, Behavior-Oriented Design (BOD) for engineering complex agents. A complex agent is one that must arbitrate between potentially conflicting goals or behaviors. Behavior-oriented design builds on work in behavior-based and hybrid architectures for agents, and the object oriented approach to software engineering. The primary contributions of this dissertation are: 1. The BOD architecture: a modular architecture with each module providing specialized representations to facilitate learning. This includes one pre-specified module and representation for action selection or behavior arbitration. The specialized representation underlying BOD action selection is Parallel-rooted, Ordered, Slip-stack Hierarchical (POSH) reactive plans. 2. The BOD development process: an iterative process that alternately scales the agent's capabilities then optimizes the agent for simplicity, exploiting tradeoffs between the component representations. This ongoing process for controlling complexity not only provides bias for the behaving agent, but also facilitates its maintenance and extendibility.",
    "advisors": ["Lynn Andrea Stein"],
    "text": "Intelligence by design : principles of modularity and coordination for engineering complex adaptive agents All intelligence relies on search - for example, the search for an intelligent agent's next action. Search is only likely to succeed in resource-bounded agents if they have already been biased towards finding the right answer. In artificial agents, the primary source of bias is engineering. This dissertation describes an approach, Behavior-Oriented Design (BOD) for engineering complex agents. A complex agent is one that must arbitrate between potentially conflicting goals or behaviors. Behavior-oriented design builds on work in behavior-based and hybrid architectures for agents, and the object oriented approach to software engineering. The primary contributions of this dissertation are: 1. The BOD architecture: a modular architecture with each module providing specialized representations to facilitate learning. This includes one pre-specified module and representation for action selection or behavior arbitration. The specialized representation underlying BOD action selection is Parallel-rooted, Ordered, Slip-stack Hierarchical (POSH) reactive plans. 2. The BOD development process: an iterative process that alternately scales the agent's capabilities then optimizes the agent for simplicity, exploiting tradeoffs between the component representations. This ongoing process for controlling complexity not only provides bias for the behaving agent, but also facilitates its maintenance and extendibility."
}]